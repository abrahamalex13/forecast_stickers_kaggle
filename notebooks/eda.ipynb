{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import requests\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import optuna\n",
    "import xgboost as xgb\n",
    "from dateutil.easter import easter\n",
    "from datetime import timedelta\n",
    "import plotnine as p9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily = (\n",
    "    pd.read_csv(\"./data/external/train.csv\")\n",
    "    .assign(\n",
    "\n",
    "        date = lambda df_: pd.to_datetime(df_['date']),\n",
    "\n",
    "        country_store = lambda df_: df_['country'].str.cat(df_['store'], sep='|'),\n",
    "        country_product = lambda df_: df_['country'].str.cat(df_['product'], sep='|'),\n",
    "        store_product = lambda df_: df_['store'].str.cat(df_['product'], sep='|'),\n",
    "        country_store_product = lambda df_: df_['country'].str.cat([df_['store'], df_['product']], sep='|')\n",
    "\n",
    "    )\n",
    "    .assign(series_id = lambda df_: df_['country_store_product'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_gdp_per_capita(country_code, year):\n",
    "    \"\"\"\n",
    "    Adapted from https://www.kaggle.com/competitions/playground-series-s5e1/discussion/554349.\n",
    "    TODO: what about GDP level, which accounts also for population?\n",
    "\n",
    "    Source reference: https://datahelpdesk.worldbank.org/knowledgebase/articles/889392-about-the-indicators-api-documentation.\n",
    "    \"\"\"\n",
    "\n",
    "    url='https://api.worldbank.org/v2/country/{0}/indicator/NY.GDP.PCAP.CD?date={1}&format=json'\n",
    "    response = requests.get(url.format(country_code, year)).json()\n",
    "\n",
    "    return response[1][0]['value']\n",
    "\n",
    "# per CountryCode-year: request GDP per capita.\n",
    "# concatenate dataframe of CountryCode | Country | Year | GDP, for integration to Kaggle source\n",
    "\n",
    "countries_code_map = {\n",
    "    'Canada': 'CAN', \n",
    "    'Finland': 'FIN',\n",
    "    'Italy': 'ITA',\n",
    "    'Kenya': 'KEN',\n",
    "    'Norway': 'NOR',\n",
    "    'Singapore': 'SGP'\n",
    "    }\n",
    "\n",
    "countries_gdp_yearly = []\n",
    "for country_title, country_code in countries_code_map.items():\n",
    "    \n",
    "    values_yearly = [\n",
    "        {'year': i, 'gdp_per_capita': extract_gdp_per_capita(country_code, i)}\n",
    "        for i in range(2010, 2019+1)\n",
    "        ]\n",
    "    values_yearly = [pd.DataFrame(x, index=[0]) for x in values_yearly]\n",
    "    values_yearly = pd.concat(values_yearly, axis=0)\n",
    "    \n",
    "    values_yearly = (\n",
    "        values_yearly\n",
    "        .assign(\n",
    "            country = country_title,\n",
    "            country_code = country_code\n",
    "            )\n",
    "        .sort_values('year')\n",
    "        .assign(\n",
    "            gdp_per_capita_lag1 = lambda df_: df_['gdp_per_capita'].shift(1),\n",
    "            gdp_per_capita_lag2 = lambda df_: df_['gdp_per_capita'].shift(2),\n",
    "            gdp_per_capita_lag3 = lambda df_: df_['gdp_per_capita'].shift(3)\n",
    "            )\n",
    "        .assign(\n",
    "            # first observation won't have a preceding lag\n",
    "            is_recession = lambda df_: (df_['gdp_per_capita'] < df_['gdp_per_capita_lag1']).astype(int).fillna(0),\n",
    "            gdp_per_capita_growth1 = lambda df_: np.log(df_['gdp_per_capita']) - np.log(df_['gdp_per_capita_lag1']),\n",
    "            gdp_per_capita_growth2 = lambda df_: np.log(df_['gdp_per_capita']) - np.log(df_['gdp_per_capita_lag2']),\n",
    "            gdp_per_capita_growth3 = lambda df_: np.log(df_['gdp_per_capita']) - np.log(df_['gdp_per_capita_lag3'])\n",
    "            )\n",
    "        .assign(\n",
    "            # asymmetric effects of growth & contraction\n",
    "            gdp_per_capita_growth1_is_positive = lambda df_: np.where(df_['gdp_per_capita_growth1'] > 0, df_['gdp_per_capita_growth1'], 0),\n",
    "            gdp_per_capita_growth1_is_negative = lambda df_: np.where(df_['gdp_per_capita_growth1'] < 0, df_['gdp_per_capita_growth1'], 0),\n",
    "\n",
    "            gdp_per_capita_growth2_is_positive = lambda df_: np.where(df_['gdp_per_capita_growth2'] > 0, df_['gdp_per_capita_growth2'], 0),\n",
    "            gdp_per_capita_growth2_is_negative = lambda df_: np.where(df_['gdp_per_capita_growth2'] < 0, df_['gdp_per_capita_growth2'], 0),\n",
    "\n",
    "            gdp_per_capita_growth3_is_positive = lambda df_: np.where(df_['gdp_per_capita_growth3'] > 0, df_['gdp_per_capita_growth3'], 0),\n",
    "            gdp_per_capita_growth3_is_negative = lambda df_: np.where(df_['gdp_per_capita_growth3'] < 0, df_['gdp_per_capita_growth3'], 0),\n",
    "        )\n",
    "        )\n",
    "    \n",
    "    countries_gdp_yearly.append(values_yearly)\n",
    "\n",
    "    print(f\"{country_title} ({country_code}) GDP Per Capita extraction complete.\")\n",
    "\n",
    "countries_gdp_yearly = pd.concat(countries_gdp_yearly, axis=0)\n",
    "\n",
    "countries_gdp_yearly = (\n",
    "    countries_gdp_yearly\n",
    "    .assign(gdp_per_capita_log = lambda df_: np.log(df_['gdp_per_capita']))\n",
    "    )\n",
    "countries_gdp_yearly['gdp_per_capita_log^2'] = countries_gdp_yearly['gdp_per_capita_log']**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_gdp_yearly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    countries_gdp_yearly\n",
    "    [['year', 'country', 'is_recession', 'gdp_per_capita']]\n",
    "    .set_index(['country', 'year'])\n",
    "    .unstack(-1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    countries_gdp_yearly\n",
    "    .set_index('year')\n",
    "    .groupby('country')\n",
    "    ['gdp_per_capita']\n",
    "    .plot(legend=True, marker='o')\n",
    ")\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    countries_gdp_yearly\n",
    "    .query(\"country == 'Kenya'\")\n",
    "    .set_index('year')\n",
    "    .groupby('country')\n",
    "    ['gdp_per_capita']\n",
    "    .plot(legend=True)\n",
    ")\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_population(country_code, year):\n",
    "\n",
    "    url='https://api.worldbank.org/v2/country/{0}/indicator/SP.POP.TOTL?date={1}&format=json'\n",
    "    response = requests.get(url.format(country_code, year)).json()\n",
    "\n",
    "    return response[1][0]['value']\n",
    "\n",
    "# concatenate dataframe of CountryCode | Country | Year | KPI, for integration to Kaggle source\n",
    "\n",
    "countries_population_yearly = []\n",
    "for country_title, country_code in countries_code_map.items():\n",
    "    \n",
    "    values_yearly = [\n",
    "        {'year': i, 'population': extract_population(country_code, i)}\n",
    "        for i in range(2010, 2019+1)\n",
    "        ]\n",
    "    values_yearly = [pd.DataFrame(x, index=[0]) for x in values_yearly]\n",
    "    values_yearly = pd.concat(values_yearly, axis=0)\n",
    "    \n",
    "    values_yearly = (\n",
    "        values_yearly\n",
    "        .assign(\n",
    "            country = country_title,\n",
    "            country_code = country_code\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    countries_population_yearly.append(values_yearly)\n",
    "\n",
    "    print(f\"{country_title} ({country_code}) population extraction complete.\")\n",
    "\n",
    "countries_population_yearly = pd.concat(countries_population_yearly, axis=0)\n",
    "assert countries_population_yearly.notnull().all().all()\n",
    "\n",
    "countries_population_yearly = (\n",
    "    countries_population_yearly\n",
    "    .assign(population_log = lambda df_: np.log(df_['population']))\n",
    "    )\n",
    "countries_population_yearly['population_log^2'] = countries_population_yearly['population_log']**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    countries_population_yearly\n",
    "    .set_index('year')\n",
    "    .groupby('country')\n",
    "    ['population_log']\n",
    "    .plot(legend=True, marker='o')\n",
    ")\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: is there a better practice for holidays feature representation?\n",
    "    # in Hyndman's Electricity Load Forecasting Kaggle (https://robjhyndman.com/papers/kaggle-competition.pdf),\n",
    "    # \"holiday effect modelled with a factor variable, taking value zero on a non-work day,\n",
    "    # some non-zero value day before a non-work day, and a different value the day after a non-work day.\"\n",
    "    # meaning -- holiday days pooled, before & after estimated separately?\n",
    "\n",
    "def onehot_encode_dates_delta(dates_base_feature: pd.DataFrame, deltas_day: list):\n",
    "    \"\"\"\n",
    "    Given dates where `is_easter` (dates_base_feature): \n",
    "    - create the Easter-1 days, title `is_easter_sub1day`.\n",
    "    - create the Easter+1 days, title `is_easter_add1day`.\n",
    "    ... So on, for deltas_day beyond [-1, 1].\n",
    "\n",
    "    Motivated by analysis of model errors.\n",
    "    \"\"\"\n",
    "\n",
    "    base_feature_stem = list(dates_base_feature.columns)\n",
    "    base_feature_stem.remove('date')\n",
    "    base_feature_stem = base_feature_stem[0]\n",
    "\n",
    "    frames_date_deltas = [dates_base_feature]\n",
    "    for delta in deltas_day: \n",
    "\n",
    "        if delta < 0:\n",
    "            sgn = 'sub'\n",
    "        else:\n",
    "            sgn = 'add'\n",
    "\n",
    "        df_delta = (\n",
    "            dates_base_feature\n",
    "            .copy()\n",
    "            .assign(date = lambda df_: df_['date'] + timedelta(days=delta))\n",
    "            .rename(columns={base_feature_stem: f\"{base_feature_stem}_{sgn}{abs(delta)}\"})\n",
    "            )\n",
    "        \n",
    "        frames_date_deltas.append(df_delta)\n",
    "\n",
    "    dates_features = (\n",
    "        pd.concat(frames_date_deltas, axis=0)\n",
    "        .fillna(0)\n",
    "        .assign(date = lambda df_: pd.to_datetime(df_['date']))\n",
    "        )\n",
    "\n",
    "    assert dates_features['date'].is_unique\n",
    "\n",
    "    return dates_features\n",
    "\n",
    "\n",
    "days_easter0 = [easter(x) for x in range(2010, 2019+1)]\n",
    "days_easter = pd.DataFrame({'date': days_easter0}).assign(is_easter = 1)\n",
    "days_special_relative_easter = onehot_encode_dates_delta(days_easter, [i for i in range(2, 7+1)])\n",
    "FEATURES_EASTER = list(days_special_relative_easter.columns)\n",
    "FEATURES_EASTER.remove('date')\n",
    "\n",
    "days_new_years_eve = [pd.to_datetime(f\"{yr}-12-31\") for yr in range(2010, 2019+1)]\n",
    "days_new_years_eve = pd.DataFrame({'date': days_new_years_eve}).assign(is_new_years_eve = 1)\n",
    "days_special_relative_new_years_eve = onehot_encode_dates_delta(\n",
    "    days_new_years_eve, \n",
    "    [-3, -2, -1, 1, 2, 3]\n",
    "    )\n",
    "FEATURES_NEW_YEARS_EVE = list(days_special_relative_new_years_eve.columns)\n",
    "FEATURES_NEW_YEARS_EVE.remove('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_calendar_features(df):\n",
    "\n",
    "    df = (\n",
    "        df\n",
    "        .assign(\n",
    "            year = lambda df_: df_['date'].dt.year,\n",
    "            month = lambda df_: df_['date'].dt.month,\n",
    "            week_of_year = lambda df_: df_['date'].dt.isocalendar().week,\n",
    "            day_of_week = lambda df_: df_['date'].dt.day_name(),\n",
    "            # President's Day is the 'third Monday in February'\n",
    "            day_of_month = lambda df_: df_['date'].dt.day,\n",
    "            day_of_year = lambda df_: df_['date'].dt.dayofyear,\n",
    "            # week of month would be ambiguous because, one week may span 2 months,\n",
    "            days_since_start = lambda df_: (df['date'] - pd.to_datetime(\"2010-01-01\")).dt.days\n",
    "            )\n",
    "        .assign(\n",
    "            # TODO: are periodic feature transforms too rigid?\n",
    "\n",
    "            # as day_of_year rises, don't expect monotonic relationship with outcome.\n",
    "            # rather, expect periodic (sinusoidal) relationship.\n",
    "            # as sin(x) rises, so too does outcome ...\n",
    "            # ensure one cycle over one year.\n",
    "            # at baseline, one sinusoidal cycle occurs per 2π\n",
    "            day_of_year_sin = lambda df_: np.sin(df_['day_of_year'] * 2 * np.pi / 365),\n",
    "            day_of_year_cos = lambda df_: np.cos(df_['day_of_year'] * 2 * np.pi / 365),\n",
    "\n",
    "            day_of_month_sin = lambda df_: np.sin(df_['day_of_month'] * 2 * np.pi / 30),\n",
    "            day_of_month_cos = lambda df_: np.cos(df_['day_of_month'] * 2 * np.pi / 30),\n",
    "\n",
    "            # exploratory visuals suggest multi-year cycles.\n",
    "            # moreover, for out-of-sample predictions to follow a periodic dynamic,\n",
    "            # need a strong parameterized form like this.\n",
    "            days_since_start_2year_sin = lambda df_: np.sin(df_['days_since_start'] * 2 * np.pi / (365*2)),\n",
    "            days_since_start_3year_sin = lambda df_: np.sin(df_['days_since_start'] * 2 * np.pi / (365*3)),\n",
    "            days_since_start_4year_sin = lambda df_: np.sin(df_['days_since_start'] * 2 * np.pi / (365*4)),\n",
    "            days_since_start_5year_sin = lambda df_: np.sin(df_['days_since_start'] * 2 * np.pi / (365*5)),\n",
    "\n",
    "            days_since_start_2year_cos = lambda df_: np.cos(df_['days_since_start'] * 2 * np.pi / (365*2)),\n",
    "            days_since_start_3year_cos = lambda df_: np.cos(df_['days_since_start'] * 2 * np.pi / (365*3)),\n",
    "            days_since_start_4year_cos = lambda df_: np.cos(df_['days_since_start'] * 2 * np.pi / (365*4)),\n",
    "            days_since_start_5year_cos = lambda df_: np.cos(df_['days_since_start'] * 2 * np.pi / (365*5)),\n",
    "            )\n",
    "\n",
    "        )\n",
    "    \n",
    "    df = pd.merge(df, days_special_relative_easter, how='left')\n",
    "    assert df['is_easter'].notnull().any()\n",
    "    df[FEATURES_EASTER] = df[FEATURES_EASTER].fillna(0)\n",
    "\n",
    "    df = pd.merge(df, days_special_relative_new_years_eve, how='left')\n",
    "    assert df['is_new_years_eve'].notnull().any()\n",
    "    df[FEATURES_NEW_YEARS_EVE] = df[FEATURES_NEW_YEARS_EVE].fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def integrate_external_features(df):\n",
    "\n",
    "    df = pd.merge(df, countries_gdp_yearly, how='left')\n",
    "    assert df['gdp_per_capita'].notnull().all().all()\n",
    "\n",
    "    df = pd.merge(df, countries_population_yearly, how='left')\n",
    "    assert df['population'].notnull().all().all()\n",
    "\n",
    "    return df\n",
    "\n",
    "def transform_lagged_predictors(df):\n",
    "\n",
    "    # to ensure proper within-series outcome lags\n",
    "    # TODO: with lagged features coming into play, how to enforce proper order via indexes?\n",
    "    df = (\n",
    "        df\n",
    "        .sort_values(['series_id', 'date'])\n",
    "        .assign(\n",
    "            num_sold_lag1 = lambda df_: df_.groupby('series_id')['num_sold'].shift(1),\n",
    "            num_sold_lag7 = lambda df_: df_.groupby('series_id')['num_sold'].shift(7)\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    return df\n",
    "\n",
    "def transform_logs(df):\n",
    "\n",
    "    df = (\n",
    "        df\n",
    "        .assign(\n",
    "            num_sold_log = lambda df_: np.log(df_['num_sold']),\n",
    "            num_sold_lag1_log = lambda df_: np.log(df_['num_sold_lag1']),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "sales_daily = transform_calendar_features(sales_daily)\n",
    "sales_daily = integrate_external_features(sales_daily)\n",
    "sales_daily = transform_lagged_predictors(sales_daily)\n",
    "sales_daily = transform_logs(sales_daily)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description Report: \"Surface Properties\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily['id'].is_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Volumetric Analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily['series_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily['date'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily['series_id'].value_counts().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily['product'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily['country'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily['store'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fields' Types and Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily['date'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily['num_sold'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily.isnull().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# are null sales events concentrated on a particular date?\n",
    "# doesn't appear so\n",
    "sales_daily.query(\"num_sold.isnull()\")['date'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily.query(\"num_sold.isnull()\")['series_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily.groupby('series_id')['num_sold'].sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a naive model: series' historical average sales, daily\n",
    "\n",
    "is_training = (\n",
    "    (sales_daily['date'] >= pd.to_datetime('2010-01-01'))\n",
    "    & (sales_daily['date'] < pd.to_datetime(\"2014-01-01\"))\n",
    ")\n",
    "\n",
    "is_validation = sales_daily['date'] >= pd.to_datetime(\"2014-01-01\")\n",
    "\n",
    "# is_training.sum(), is_validation.sum()\n",
    "\n",
    "# with original train data: using strictly train segment, groupby average\n",
    "predictions_naive = (\n",
    "    sales_daily\n",
    "    .loc[is_training]\n",
    "    .groupby('series_id')\n",
    "    [['num_sold']]\n",
    "    .agg('mean')\n",
    "    .reset_index(drop=False)\n",
    ")\n",
    "\n",
    "predictions_naive_evaluate = pd.merge(\n",
    "    sales_daily.loc[is_validation],\n",
    "    predictions_naive.rename(columns={'num_sold': 'yhat'}),\n",
    "    how='left'\n",
    "    )\n",
    "\n",
    "# expect a couple series with all null\n",
    "predictions_naive.isnull().sum()\n",
    "# recommended in this discussion: https://www.kaggle.com/competitions/playground-series-s5e1/discussion/554553 \n",
    "predictions_naive_evaluate = predictions_naive_evaluate.dropna()\n",
    "\n",
    "mean_absolute_percentage_error(\n",
    "    predictions_naive_evaluate['num_sold'],\n",
    "    predictions_naive_evaluate['yhat']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_naive_evaluate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a naive model: series' historical average sales, daily\n",
    "\n",
    "is_training = (\n",
    "    (sales_daily['date'] >= pd.to_datetime('2013-01-01'))\n",
    "    & (sales_daily['date'] < pd.to_datetime(\"2014-01-01\"))\n",
    ")\n",
    "\n",
    "is_validation = sales_daily['date'] >= pd.to_datetime(\"2014-01-01\")\n",
    "\n",
    "# is_training.sum(), is_validation.sum()\n",
    "\n",
    "# with original train data: using strictly train segment, groupby average\n",
    "predictions_naive = (\n",
    "    sales_daily\n",
    "    .loc[is_training]\n",
    "    .groupby('series_id')\n",
    "    [['num_sold']]\n",
    "    .agg('mean')\n",
    "    .reset_index(drop=False)\n",
    ")\n",
    "\n",
    "predictions_naive_evaluate = pd.merge(\n",
    "    sales_daily.loc[is_validation],\n",
    "    predictions_naive.rename(columns={'num_sold': 'yhat'}),\n",
    "    how='left'\n",
    "    )\n",
    "\n",
    "# expect a couple series with all null\n",
    "predictions_naive.isnull().sum()\n",
    "# recommended in this discussion: https://www.kaggle.com/competitions/playground-series-s5e1/discussion/554553 \n",
    "predictions_naive_evaluate = predictions_naive_evaluate.dropna()\n",
    "\n",
    "mean_absolute_percentage_error(\n",
    "    predictions_naive_evaluate['num_sold'],\n",
    "    predictions_naive_evaluate['yhat']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_sample_daily = sales_daily.query(\"series_id == 'Norway|Premium Sticker Mart|Kaggle'\")\n",
    "\n",
    "(\n",
    "    sales_sample_daily\n",
    "    .loc[is_training]\n",
    "    [['date', 'num_sold']]\n",
    "    .set_index('date')\n",
    "    .plot\n",
    "    .line()\n",
    ")\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# descend level of abstraction -- \n",
    "\n",
    "    # across-year patterns (multi-year business cycles)\n",
    "    # within-year,\n",
    "        # month-of-year seasonality\n",
    "        # week-of-year seasonality\n",
    "        # day-of-month seasonality\n",
    "        # day-of-week seasonality\n",
    "\n",
    "# outcome vs predictors\n",
    "    # lagged outcome\n",
    "    # gdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    p9.ggplot(sales_daily.groupby(['year'])[['num_sold']].agg('sum').reset_index(drop=False)) + \n",
    "    p9.theme_bw() + \n",
    "    p9.geom_col(p9.aes('year', 'num_sold'), alpha=0.25)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    p9.ggplot(sales_daily.groupby(['country', 'year'])[['num_sold']].agg('sum').reset_index(drop=False)) + \n",
    "    p9.theme_bw() + \n",
    "    p9.geom_line(p9.aes('year', 'num_sold', group='country', color='country')) + \n",
    "    p9.theme(figure_size=(8,4))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    p9.ggplot(sales_daily.groupby(['date'])[['num_sold']].agg('sum').reset_index(drop=False)) + \n",
    "    p9.theme_bw() + \n",
    "    p9.geom_point(p9.aes('date', 'num_sold'), alpha=0.25)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    p9.ggplot(sales_daily) + \n",
    "    p9.theme_bw() + \n",
    "    p9.geom_boxplot(p9.aes('month', 'num_sold', group='month'), alpha=0.1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    p9.ggplot(sales_daily.query(\"series_id == 'Norway|Premium Sticker Mart|Kaggle'\")) + \n",
    "    p9.theme_bw() + \n",
    "    p9.geom_boxplot(p9.aes('month', 'num_sold', group='month'), alpha=0.1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    p9.ggplot((\n",
    "        sales_daily\n",
    "        .assign(day_of_week = lambda df_: pd.Categorical(\n",
    "            df_['day_of_week'], \n",
    "            ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']\n",
    "            ))\n",
    "    )) + \n",
    "    p9.theme_bw() + \n",
    "    p9.geom_boxplot(p9.aes('day_of_week', 'num_sold', group='day_of_week'), alpha=0.1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    p9.ggplot(sales_daily.query(\"series_id == 'Norway|Premium Sticker Mart|Kaggle'\")) + \n",
    "    p9.theme_bw() + \n",
    "    p9.geom_point(p9.aes('num_sold_lag1', 'num_sold'), alpha=0.1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    p9.ggplot(sales_daily.query(\"series_id == 'Norway|Premium Sticker Mart|Kaggle'\")) + \n",
    "    p9.theme_bw() + \n",
    "    p9.geom_point(p9.aes('num_sold_lag7', 'num_sold'), alpha=0.1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    p9.ggplot((\n",
    "        sales_daily\n",
    "        .assign(num_sold_log_mean = lambda df_: df_.groupby('series_id')['num_sold_log'].transform('mean'))\n",
    "        .assign(num_sold_log_demean = lambda df_: df_['num_sold_log'] - df_['num_sold_log_mean'])\n",
    "        )) + \n",
    "    p9.theme_bw() + \n",
    "    p9.geom_point(p9.aes('gdp_per_capita_log', 'num_sold_log_demean'), alpha=0.5) + \n",
    "    p9.geom_smooth(p9.aes('gdp_per_capita_log', 'num_sold_log_demean', group='country', color='country'), method='lowess')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features set varies by model -- _global_ and _local_. \n",
    "\n",
    "\"Universe\" implies, all features that could be used. A subset enters into each \"building block\" of the overall modeling system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_UNIVERSE_TO_ONEHOT = [\n",
    "    'country', \n",
    "    'store',\n",
    "    'product',\n",
    "    'country_store',\n",
    "    'country_product',\n",
    "    'store_product',\n",
    "    # omit country-store-product, that's perfect collinearity with above.\n",
    "    # year attempted, but then omitted. Because \n",
    "    # exogenous factors should help explain year-to-year shifts,\n",
    "    # so that out-of-sample years' forecasts aren't flat\n",
    "    'month', \n",
    "    'week_of_year', \n",
    "    'day_of_week'\n",
    "    ]\n",
    "\n",
    "# local model fits by country-store-product segment;\n",
    "# therefore, those onehots don't vary\n",
    "FEATURES_LOCAL_MODEL_TO_ONEHOT = [\n",
    "    x for x in FEATURES_UNIVERSE_TO_ONEHOT \n",
    "    if not any(stem in x for stem in ['country', 'store', 'product'])\n",
    "    ]\n",
    "\n",
    "FEATURES_UNIVERSE_NUMERIC_CONTINUOUS = [\n",
    "    'gdp_per_capita_log',\n",
    "    'gdp_per_capita_log^2',\n",
    "    'population_log',\n",
    "    'population_log^2',\n",
    "\n",
    "    # continuous year allows evolving patterns over time\n",
    "    'year',\n",
    "    # TODO: better treated as onehot?\n",
    "    'day_of_month', \n",
    "    'day_of_month_sin',\n",
    "    'day_of_month_cos',\n",
    "\n",
    "    'day_of_year',\n",
    "    'day_of_year_sin',\n",
    "    'day_of_year_cos',\n",
    "\n",
    "    'days_since_start',\n",
    "    'days_since_start_2year_sin',\n",
    "    'days_since_start_3year_sin',\n",
    "    'days_since_start_4year_sin',\n",
    "    'days_since_start_5year_sin',\n",
    "    'days_since_start_2year_cos',\n",
    "    'days_since_start_3year_cos',\n",
    "    'days_since_start_4year_cos',\n",
    "    'days_since_start_5year_cos'\n",
    "    ]\n",
    "# from experimentation, tree-based models' out-of-sample forecasts flat\n",
    "# when incorporating days_since_start based feature. (or onehot-encoded year)\n",
    "\n",
    "FEATURES_UNIVERSE_ALREADY_ONEHOT = FEATURES_EASTER + FEATURES_NEW_YEARS_EVE\n",
    "\n",
    "FEATURES_AUTOREGRESSIVE = ['num_sold_lag1_log']\n",
    "\n",
    "FEATURES_GLOBAL_MODEL = (\n",
    "    FEATURES_UNIVERSE_TO_ONEHOT + \n",
    "    FEATURES_UNIVERSE_NUMERIC_CONTINUOUS + \n",
    "    FEATURES_UNIVERSE_ALREADY_ONEHOT\n",
    "    )\n",
    "\n",
    "FEATURES_LOCAL_MODEL = (\n",
    "    FEATURES_LOCAL_MODEL_TO_ONEHOT + \n",
    "    FEATURES_UNIVERSE_NUMERIC_CONTINUOUS + \n",
    "    FEATURES_UNIVERSE_ALREADY_ONEHOT\n",
    "    )\n",
    "\n",
    "ATTRIBUTES = ['series_id', 'date', 'id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily_complete = sales_daily.dropna(subset=['num_sold', 'num_sold_lag1_log'])\n",
    "\n",
    "# shorter alias\n",
    "XY = sales_daily_complete\n",
    "\n",
    "# from previous retail forecasting competitions' leaders,\n",
    "# plus theoretically expected heterogeneity between series: \n",
    "# one model per segment\n",
    "\n",
    "segments_XY = {grp: df for grp, df in XY.groupby('series_id')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have gone back-and-forth about the validation set decision.\n",
    "# validation set 2014-16 ensures, more consistently accurate model (trend-cycle component, especially).\n",
    "# validation 2016 ensures, higher accuracy on data very recent to 2017-19 test.\n",
    "    # *if relationships are time-varying*, and sample size insufficient to explicitly estimate time-varying parameters -- \n",
    "    # then prefer to upweight data more recent to test period. \n",
    "KFOLDS_ALTERNATIVES = {\n",
    "\n",
    "    'validate_2014_2016': ( \n",
    "        ( (XY['date'] >= pd.to_datetime('2010-01-01')) & (XY['date'] < pd.to_datetime(\"2014-01-01\")) ),\n",
    "        XY['date'] >= pd.to_datetime(\"2014-01-01\")\n",
    "        ),\n",
    "\n",
    "    'validate_2015_2016': ( \n",
    "        ( (XY['date'] >= pd.to_datetime('2010-01-01')) & (XY['date'] < pd.to_datetime(\"2015-01-01\")) ),\n",
    "        XY['date'] >= pd.to_datetime(\"2015-01-01\")\n",
    "        ),\n",
    "\n",
    "    'validate_2016': ( \n",
    "        ( (XY['date'] >= pd.to_datetime('2010-01-01')) & (XY['date'] < pd.to_datetime(\"2016-01-01\")) ),\n",
    "        XY['date'] >= pd.to_datetime(\"2016-01-01\")\n",
    "        )\n",
    "\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to execute finer-grained feature selection -- for example, which categorical levels to onehot-encode --\n",
    "# start with pre-transformed, wide-as-possible features set\n",
    "\n",
    "# must not leak train/test, and also expect model-building by segment.\n",
    "\n",
    "# alternatives:\n",
    "    # (1) pre-build train/test splits for every segment\n",
    "        # drawbacks: data management complexity. many more objects to keep track of.\n",
    "    # (2) pre-build one \"universe features\" dataset, to collect all those colnames.\n",
    "    # then, on the fly, apply transformer and subset challenger cols.\n",
    "        # drawbacks: repeating a calculation many times, when it could calculate upfront.\n",
    "\n",
    "is_training, is_validation = KFOLDS_ALTERNATIVES['validate_2016']\n",
    "\n",
    "segments_XY_transform_features_universe = {}\n",
    "for grp, XY_grp in segments_XY.items():\n",
    "\n",
    "    feature_transform_pipeline_universe = ColumnTransformer(\n",
    "        [\n",
    "            ('transformer_onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), FEATURES_LOCAL_MODEL_TO_ONEHOT),\n",
    "            ('standardizer', StandardScaler(), FEATURES_UNIVERSE_NUMERIC_CONTINUOUS),\n",
    "            ('select_others', 'passthrough', FEATURES_UNIVERSE_ALREADY_ONEHOT)\n",
    "        ],\n",
    "        verbose_feature_names_out=False,\n",
    "        remainder='drop'\n",
    "        ).set_output(transform='pandas')\n",
    "\n",
    "    X_train_transform = feature_transform_pipeline_universe.fit_transform(XY_grp.loc[is_training])\n",
    "    y_train = XY_grp.loc[is_training, ['num_sold', 'num_sold_log']]\n",
    "    X_test_transform = feature_transform_pipeline_universe.transform(XY_grp.loc[is_validation])\n",
    "    y_test = XY_grp.loc[is_validation, ['num_sold', 'num_sold_log']]\n",
    "\n",
    "    XY_grp_train = pd.concat([X_train_transform, y_train], axis=1)\n",
    "    XY_grp_test = pd.concat([X_test_transform, y_test], axis=1)\n",
    "    XY_grp = pd.concat([XY_grp_train, XY_grp_test], axis=0)\n",
    "\n",
    "    segments_XY_transform_features_universe[grp] = {\n",
    "        'X_train': X_train_transform,\n",
    "        'y_train': y_train,\n",
    "        'X_test': X_test_transform,\n",
    "        'y_test': y_test,\n",
    "        'XY_grp': XY_grp\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "class TrendRemainderModelPipeline:\n",
    "    \"\"\"\n",
    "    Y = TrendComponent + RemainderComponent\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        trend_level_feature_transformer: ColumnTransformer,\n",
    "        trend_model_ridge_alpha,\n",
    "        remainder_feature_transformer: ColumnTransformer,\n",
    "        remainder_model_kind: Literal[\"random_forest\", \"xgboost\"],\n",
    "        remainder_model_xgboost_rounds_count=1_000,\n",
    "        remainder_model_xgboost_param=None\n",
    "        ):\n",
    "\n",
    "        self.trend_level_feature_transformer = trend_level_feature_transformer\n",
    "        self.trend_level_model_ridge_alpha = trend_model_ridge_alpha\n",
    "\n",
    "        self.remainder_feature_transformer = remainder_feature_transformer\n",
    "        self.remainder_model_kind = remainder_model_kind\n",
    "        \n",
    "        self.remainder_model_xgboost_rounds_count = remainder_model_xgboost_rounds_count\n",
    "        self.remainder_model_xgboost_param = remainder_model_xgboost_param\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        self.fit_trend_level_model(X, y)\n",
    "        yhat_trend_level = self.predict_trend_level_model(X)\n",
    "        y_detrended = y - yhat_trend_level\n",
    "\n",
    "        self.fit_remainder_model(X, y_detrended)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        yhat_trend_level = self.predict_trend_level_model(X)\n",
    "        yhat_remainder = self.predict_remainder_model(X)\n",
    "        preds = yhat_trend_level + yhat_remainder\n",
    "\n",
    "        return preds\n",
    "\n",
    "    def fit_trend_level_model(self, X, y):\n",
    "        \"\"\"\n",
    "        'Weak learner', strictly intended to explain trend (level) shifts between years.\n",
    "        \"\"\"\n",
    "\n",
    "        X = self.trend_level_feature_transformer.fit_transform(X)\n",
    "\n",
    "        model_trend_level = Ridge(self.trend_level_model_ridge_alpha)\n",
    "        model_trend_level.fit(X, y)\n",
    "\n",
    "        self.trend_level_model = model_trend_level\n",
    "\n",
    "    def _fit_xgboost_remainder_model(self, X, y):\n",
    "        \"\"\"\n",
    "        Use xgboost-optimized data structures.\n",
    "        Precondition: transformed X\n",
    "        \"\"\"\n",
    "\n",
    "        dtrain = xgb.DMatrix(X, label=y)\n",
    "\n",
    "        self.remainder_model = xgb.train(\n",
    "            self.remainder_model_xgboost_param, \n",
    "            dtrain, \n",
    "            self.remainder_model_xgboost_rounds_count\n",
    "            )\n",
    "\n",
    "    def _fit_random_forest_remainder_model(self, X, y):\n",
    "        \"\"\"Precondition: transformed X\"\"\"\n",
    "\n",
    "        self.remainder_model = RandomForestRegressor(n_estimators=100, n_jobs=-1)\n",
    "        self.remainder_model.fit(X, y)\n",
    "\n",
    "    def fit_remainder_model(self, X, y):\n",
    "\n",
    "        X = self.remainder_feature_transformer.fit_transform(X)\n",
    "\n",
    "        if self.remainder_model_kind == 'xgboost':\n",
    "            self._fit_xgboost_remainder_model(X, y)\n",
    "\n",
    "        elif self.remainder_model_kind == 'random_forest':\n",
    "            self._fit_random_forest_remainder_model(X, y)\n",
    "\n",
    "    def predict_trend_level_model(self, X):\n",
    "        X = self.trend_level_feature_transformer.transform(X)\n",
    "        preds = self.trend_level_model.predict(X) \n",
    "        return preds\n",
    "    \n",
    "    def predict_remainder_model(self, X):\n",
    "\n",
    "        X = self.remainder_feature_transformer.transform(X)\n",
    "        if self.remainder_model_kind == 'xgboost':\n",
    "            X = xgb.DMatrix(X)\n",
    "\n",
    "        preds = self.remainder_model.predict(X)\n",
    "\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trend-Cycle Component: Strongly Parametric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for model to use gdp_per_capita_log (annual measurement),\n",
    "# cannot include categorical year. otherwise, year partials out gdp\n",
    "\n",
    "# experimented with gdp_per_capita_growth lags 1-3,\n",
    "# but these *severely* overfit! \n",
    "\n",
    "# experimented with polynomial transforms of gdp and/or population,\n",
    "# but those hardly improved result\n",
    "\n",
    "# experimented with days_since_start_2year_sin, cos; but \n",
    "# result seems weakly improved versus additional model complexity\n",
    "\n",
    "# TODO: some feature for expansionary vs contractionary periods?\n",
    "\n",
    "FEATURES_TREND_CYCLE_NUMERIC_CONTINUOUS = [\n",
    "    'gdp_per_capita_log',\n",
    "    ]\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    parameters = {'alpha': trial.suggest_float(\"alpha\", 1e-1, 1_000, log=True)}\n",
    "\n",
    "    kfolds_evaluation = []\n",
    "    for is_training, is_validation in [KFOLDS_ALTERNATIVES['validate_2016']]:\n",
    "\n",
    "        segments_models = {}\n",
    "        segments_predictions = []\n",
    "\n",
    "        for grp, XY_grp in segments_XY.items():\n",
    "\n",
    "            feature_transform_pipeline_local_model = ColumnTransformer([\n",
    "                # ('transformer_onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), ['year']),\n",
    "                ('transformer_std', StandardScaler(), FEATURES_TREND_CYCLE_NUMERIC_CONTINUOUS)\n",
    "                ],\n",
    "                verbose_feature_names_out=False,\n",
    "                remainder='drop'\n",
    "                )\n",
    "            feature_transform_pipeline_local_model.set_output(transform='pandas')\n",
    "\n",
    "            pipeline_e2e = Pipeline([\n",
    "                ('transform_features', feature_transform_pipeline_local_model), \n",
    "                ('model', Ridge(**parameters))\n",
    "                ])\n",
    "            pipeline_e2e.fit(XY_grp.loc[is_training], XY_grp.loc[is_training, 'num_sold_log'])\n",
    "            \n",
    "            segments_models[grp] = pipeline_e2e\n",
    "\n",
    "            predictions = (\n",
    "                XY_grp\n",
    "                .copy()\n",
    "                .assign(yhat = lambda df_: np.exp(pipeline_e2e.predict(df_)))\n",
    "                )\n",
    "            \n",
    "            segments_predictions.append(predictions)\n",
    "\n",
    "\n",
    "        predictions = pd.concat(segments_predictions, axis=0)\n",
    "        scores = {\n",
    "            'validation': mean_absolute_percentage_error( \n",
    "                predictions.loc[is_validation, 'num_sold'],\n",
    "                predictions.loc[is_validation, 'yhat']\n",
    "                ),\n",
    "            'train': mean_absolute_percentage_error( \n",
    "                predictions.loc[is_training, 'num_sold'],\n",
    "                predictions.loc[is_training, 'yhat']\n",
    "                )\n",
    "            }\n",
    "        \n",
    "        kfolds_evaluation.append(scores)\n",
    "\n",
    "    score_overall = np.mean([ \n",
    "        kfolds_evaluation[0]['validation'], \n",
    "        # kfolds_evaluation[1]['validation'] \n",
    "        ])\n",
    "    \n",
    "    return score_overall\n",
    "\n",
    "study = optuna.create_study()\n",
    "\n",
    "study.optimize(\n",
    "    objective,\n",
    "    n_trials=50,\n",
    "    catch=(ValueError,),\n",
    "    n_jobs=-1,\n",
    "    timeout=12 * 60 * 60,\n",
    ")\n",
    "\n",
    "RIDGE_ALPHA_TREND_CYCLE_LOCAL_MODELS = study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_trial, study.best_params, study.best_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trend-Cycle Component: Periodicity Feature Selection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfit, likely due to feature selection based on single validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# previously, overfit with boosting pipeline of:  \n",
    "#   1. parametric trend level shifts\n",
    "#   2. nonparametric periodic trend\n",
    "#   3. nonparametric seasonality & remainder\n",
    "# try (2) with a parametric model.\n",
    "\n",
    "HAS_TREND_PERIODICITY_FEATURE_SELECTION = True\n",
    "FEATURES_TREND_PERIODICITY_NUMERIC_CONTINUOUS = [\n",
    "    # don't force equal magnitude across years.\n",
    "    'days_since_start_2year_sin',\n",
    "    'days_since_start_3year_sin',\n",
    "    'days_since_start_4year_sin',\n",
    "    'days_since_start_5year_sin',\n",
    "    'days_since_start_2year_cos',\n",
    "    'days_since_start_3year_cos',\n",
    "    'days_since_start_4year_cos',\n",
    "    'days_since_start_5year_cos'\n",
    "    ]\n",
    "features_champion = FEATURES_TREND_CYCLE_NUMERIC_CONTINUOUS\n",
    "challengers_marginal_features_roots = [2, 3, 4, 5]\n",
    "selections_model_alternatives = {-1: 0, 0: 0, 1: 0, 2:0, 3:0}\n",
    "\n",
    "kfolds_evaluation = []\n",
    "for is_training, is_validation in [KFOLDS_ALTERNATIVES['validate_2016']]:\n",
    "\n",
    "    SEGMENTS_TREND_MODELS_TRAINSET = {}\n",
    "    segments_predictions = []\n",
    "\n",
    "    i = 0\n",
    "    segments_total = len(segments_XY)\n",
    "    for grp, XY_grp in segments_XY.items():\n",
    "\n",
    "        feature_transform_pipeline_champion = ColumnTransformer(\n",
    "            [('transformer_std', StandardScaler(), FEATURES_TREND_CYCLE_NUMERIC_CONTINUOUS)],\n",
    "            verbose_feature_names_out=False,\n",
    "            remainder='drop'\n",
    "            ).set_output(transform='pandas')\n",
    "\n",
    "        pipeline_e2e = Pipeline([\n",
    "            ('transform_features', feature_transform_pipeline_champion), \n",
    "            ('model', Ridge(**RIDGE_ALPHA_TREND_CYCLE_LOCAL_MODELS))\n",
    "            ])\n",
    "        pipeline_e2e.fit(XY_grp.loc[is_training], XY_grp.loc[is_training, 'num_sold_log'])\n",
    "        \n",
    "        pipeline_champion_e2e = pipeline_e2e\n",
    "        predictions_champion = XY_grp.copy().assign(yhat = lambda df_: np.exp(pipeline_champion_e2e.predict(df_)))\n",
    "\n",
    "        score_champion = mean_absolute_percentage_error(\n",
    "            predictions_champion.loc[is_validation, 'num_sold'],\n",
    "            predictions_champion.loc[is_validation, 'yhat']\n",
    "            )\n",
    "        \n",
    "        if HAS_TREND_PERIODICITY_FEATURE_SELECTION:\n",
    "\n",
    "            models_challengers = []\n",
    "            predictions_challengers = []\n",
    "            scores_challengers = []\n",
    "            for root in challengers_marginal_features_roots:\n",
    "\n",
    "                features_marginal = [f\"days_since_start_{root}year_sin\", f\"days_since_start_{root}year_cos\"]\n",
    "                features_challenger = features_champion + features_marginal\n",
    "\n",
    "                feature_transform_pipeline_challenger = ColumnTransformer(\n",
    "                    [('transformer_std', StandardScaler(), features_challenger)],\n",
    "                    verbose_feature_names_out=False,\n",
    "                    remainder='drop'\n",
    "                    ).set_output(transform='pandas')\n",
    "                pipeline_challenger_e2e = Pipeline([\n",
    "                    ('transform_features', feature_transform_pipeline_challenger), \n",
    "                    ('model', Ridge(**RIDGE_ALPHA_TREND_CYCLE_LOCAL_MODELS))\n",
    "                    ])\n",
    "                pipeline_challenger_e2e.fit(XY_grp.loc[is_training], XY_grp.loc[is_training, 'num_sold_log'])\n",
    "                models_challengers.append(pipeline_challenger_e2e)\n",
    "\n",
    "                predictions_challenger = (\n",
    "                    XY_grp\n",
    "                    .copy()\n",
    "                    .assign(yhat = lambda df_: np.exp(pipeline_challenger_e2e.predict(df_)))\n",
    "                    )\n",
    "                predictions_challengers.append(predictions_challenger)\n",
    "\n",
    "                score_challenger = mean_absolute_percentage_error(\n",
    "                    predictions_challenger.loc[is_validation, 'num_sold'],\n",
    "                    predictions_challenger.loc[is_validation, 'yhat']\n",
    "                    )\n",
    "                scores_challengers.append(score_challenger)\n",
    "\n",
    "            score_best_challenger = min(scores_challengers)\n",
    "            is_best_challenger_better_vs_champion = score_best_challenger < score_champion\n",
    "            if is_best_challenger_better_vs_champion:\n",
    "                index_best = scores_challengers.index(score_best_challenger)\n",
    "                model_final = models_challengers[index_best]            \n",
    "                predictions_final = predictions_challengers[index_best]\n",
    "                selections_model_alternatives[index_best] += 1\n",
    "            else:\n",
    "                model_final = pipeline_champion_e2e\n",
    "                predictions_final = predictions_champion\n",
    "                selections_model_alternatives[-1] += 1\n",
    "\n",
    "        else:\n",
    "            model_final = pipeline_champion_e2e\n",
    "            predictions_final = predictions_champion\n",
    "\n",
    "        SEGMENTS_TREND_MODELS_TRAINSET[grp] = model_final\n",
    "\n",
    "        segments_predictions.append(predictions_final)\n",
    "        \n",
    "        i += 1\n",
    "        print(f\"{i}/{segments_total} models fit.\")\n",
    "\n",
    "    predictions = pd.concat(segments_predictions, axis=0)\n",
    "\n",
    "    scores = {\n",
    "        'validation': {\n",
    "            'nobs': predictions.loc[is_validation].shape[0],\n",
    "            'score': mean_absolute_percentage_error( \n",
    "                predictions.loc[is_validation, 'num_sold'],\n",
    "                predictions.loc[is_validation, 'yhat']\n",
    "                )\n",
    "            },\n",
    "        'train': {\n",
    "            'nobs': predictions.loc[is_training].shape[0],\n",
    "            'score': mean_absolute_percentage_error( \n",
    "                predictions.loc[is_training, 'num_sold'],\n",
    "                predictions.loc[is_training, 'yhat']\n",
    "                )\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    kfolds_evaluation.append(scores)\n",
    "\n",
    "kfolds_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selections_model_alternatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = (\n",
    "    predictions\n",
    "    .assign(ape = lambda df_: abs(100 * (df_['num_sold'] / df_['yhat'] - 1)))\n",
    "    .assign(pe = lambda df_: (100 * (df_['num_sold'] / df_['yhat'] - 1)))\n",
    "    .assign(residual = lambda df_: df_['num_sold'] - df_['yhat'])\n",
    "    .assign(residual_log = lambda df_: df_['num_sold_log'] - np.log(df_['yhat']))\n",
    "    .assign(residual_abs = lambda df_: np.abs(df_['residual']))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    predictions\n",
    "    .groupby('country')\n",
    "    ['ape']\n",
    "    .agg(['mean', 'size'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    predictions\n",
    "    .groupby('year')\n",
    "    ['ape']\n",
    "    .agg(['mean', 'size'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    predictions\n",
    "    .groupby(['country', 'year'])\n",
    "    ['ape']\n",
    "    .agg(['mean', 'size'])\n",
    "    .unstack(-1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looked previously at AVG(residual) by year,\n",
    "# but that allows large errors to wash out -- not the right metric for this contest!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_sample = predictions.query(\"series_id == 'Norway|Premium Sticker Mart|Kaggle'\")\n",
    "\n",
    "predictions_sample.set_index('date')[['num_sold', 'yhat']].plot.line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_sample = predictions.query(\"series_id == 'Kenya|Stickers for Less|Holographic Goose'\")\n",
    "\n",
    "predictions_sample.set_index('date')[['num_sold', 'yhat']].plot.line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    predictions_sample\n",
    "    .groupby('year')\n",
    "    [['residual']]\n",
    "    .agg([np.std, 'mean'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seasonal & Remainder Component: Nonparametric, Explore Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_SEASONALITY_TO_ONEHOT = [\n",
    "    'month', \n",
    "    'week_of_year',\n",
    "    'day_of_week',\n",
    "    ]\n",
    "\n",
    "FEATURES_SEASONALITY_NUMERIC_CONTINUOUS = [\n",
    "    'year', # allow seasonality to evolve over time\n",
    "    'day_of_month', \n",
    "    'day_of_month_sin',\n",
    "    'day_of_month_cos',\n",
    "    'day_of_year_sin',\n",
    "    'day_of_year_cos',\n",
    "    'day_of_year'\n",
    "    # previously tested days_since_start_{X}year_{sin_or_cos} -- forecasts worsened\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def objective(trial):\n",
    "\n",
    "#     ROUNDS_COUNT = 1_000\n",
    "\n",
    "#     param = {\n",
    "#         \"objective\": \"reg:squarederror\",\n",
    "#         \"booster\": \"gbtree\",\n",
    "#         \"max_depth\": trial.suggest_int(\"max_depth\", 1, 9, step=2),\n",
    "#         \"subsample\": trial.suggest_float(\"subsample\", 0.4, 1.0),\n",
    "#         \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n",
    "#         }\n",
    "\n",
    "#     kfolds_evaluation = []\n",
    "#     for is_training, is_validation in [KFOLDS_ALTERNATIVES['validate_2016']]:\n",
    "\n",
    "#         segments_models = {}\n",
    "#         segments_predictions = []\n",
    "\n",
    "#         for grp, XY_grp in segments_XY.items():\n",
    "\n",
    "#             predictions = (\n",
    "#                 XY_grp\n",
    "#                 .copy()\n",
    "#                 .assign(yhat_trend_log = lambda df_: SEGMENTS_TREND_MODELS_TRAINSET[grp].predict(df_))\n",
    "#                 .assign(num_sold_log_detrend = lambda df_: df_['num_sold_log'] - df_['yhat_trend_log'])\n",
    "#                 )\n",
    "\n",
    "#             remainder_feature_transformer = ColumnTransformer([\n",
    "#                 ('transformer_onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), FEATURES_SEASONALITY_TO_ONEHOT),\n",
    "#                 # no transforms required\n",
    "#                 (\n",
    "#                     'select_others', \n",
    "#                     'passthrough', \n",
    "#                     FEATURES_SEASONALITY_NUMERIC_CONTINUOUS + FEATURES_UNIVERSE_ALREADY_ONEHOT\n",
    "#                 )\n",
    "#                 ],\n",
    "#                 verbose_feature_names_out=False,\n",
    "#                 remainder='drop'\n",
    "#                 ).set_output(transform='pandas')\n",
    "            \n",
    "#             X_remainder_train = remainder_feature_transformer.fit_transform(predictions.loc[is_training])        \n",
    "#             dtrain = xgb.DMatrix(X_remainder_train, label=predictions.loc[is_training, 'num_sold_log_detrend'])\n",
    "#             model_remainder = xgb.train(param, dtrain, ROUNDS_COUNT)\n",
    "\n",
    "#             dtest = xgb.DMatrix(remainder_feature_transformer.transform(predictions))\n",
    "#             yhat_remainder_log = model_remainder.predict(dtest)\n",
    "#             predictions = (\n",
    "#                 predictions\n",
    "#                 .assign(yhat_remainder_log = yhat_remainder_log)\n",
    "#                 .assign(yhat = lambda df_: np.exp(df_['yhat_trend_log'] + df_['yhat_remainder_log']))\n",
    "#                 )\n",
    "\n",
    "#             segments_predictions.append(predictions)\n",
    "\n",
    "#         predictions = pd.concat(segments_predictions, axis=0)\n",
    "\n",
    "#         scores = {\n",
    "#             'validation': {\n",
    "#                 'nobs': predictions.loc[is_validation].shape[0],\n",
    "#                 'score': mean_absolute_percentage_error( \n",
    "#                     predictions.loc[is_validation, 'num_sold'],\n",
    "#                     predictions.loc[is_validation, 'yhat']\n",
    "#                     )\n",
    "#                 },\n",
    "#             'train': {\n",
    "#                 'nobs': predictions.loc[is_training].shape[0],\n",
    "#                 'score': mean_absolute_percentage_error( \n",
    "#                     predictions.loc[is_training, 'num_sold'],\n",
    "#                     predictions.loc[is_training, 'yhat']\n",
    "#                     )\n",
    "#                 }\n",
    "#             }\n",
    "        \n",
    "#         kfolds_evaluation.append(scores)\n",
    "\n",
    "#     score_overall = kfolds_evaluation[0]['validation']['score']\n",
    "\n",
    "#     return score_overall\n",
    "\n",
    "\n",
    "# study = optuna.create_study()\n",
    "# # if optuna returns nulls in y_pred, don't fail the entire study\n",
    "# study.optimize(\n",
    "#     objective,\n",
    "#     n_trials=10,\n",
    "#     catch=(ValueError,),\n",
    "#     n_jobs=-1,\n",
    "#     timeout=12 * 60 * 60,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'max_depth': 1, 'subsample': 0.9499378306625286, 'colsample_bytree': 0.8295502726981714}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Model: Parametric Stepwise Features Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEGMENTS_MODELS_TRAINSET = {}\n",
    "\n",
    "# kfolds_evaluation = []\n",
    "# for is_training, is_validation in [KFOLDS_ALTERNATIVES['validate_2016']]:\n",
    "\n",
    "#     segments_predictions = []\n",
    "\n",
    "#     i = 0\n",
    "#     segments_total = len(segments_XY_transform_features_universe)\n",
    "#     for grp in segments_XY_transform_features_universe.keys():\n",
    "\n",
    "#         print(f\"{grp} modeling begun.\")\n",
    "\n",
    "#         XY_grp = segments_XY_transform_features_universe[grp]['XY_grp']\n",
    "#         # when a particular series has null outcomes, time categoricals may be missing.\n",
    "#         features_universe = list(segments_XY_transform_features_universe[grp]['X_train'].columns)\n",
    "        \n",
    "#         model_champion = Ridge(**RIDGE_ALPHA_TREND_CYCLE_LOCAL_MODELS)\n",
    "#         model_champion.fit(\n",
    "#             XY_grp.loc[is_training, ['gdp_per_capita_log']],\n",
    "#             XY_grp.loc[is_training, 'num_sold_log']\n",
    "#             )\n",
    "#         features_champion = model_champion.feature_names_in_\n",
    "\n",
    "#         predictions_champion = (\n",
    "#             XY_grp\n",
    "#             .copy()\n",
    "#             .assign(yhat = lambda df_: np.exp(model_champion.predict(df_[features_champion])))\n",
    "#             )\n",
    "#         score_champion = mean_absolute_percentage_error(\n",
    "#             predictions_champion.loc[is_validation, 'num_sold'],\n",
    "#             predictions_champion.loc[is_validation, 'yhat']\n",
    "#             )\n",
    "        \n",
    "#         is_champion_beatable = True\n",
    "\n",
    "#         while is_champion_beatable:\n",
    "\n",
    "#             models_challengers = []\n",
    "#             predictions_challengers = []\n",
    "#             scores_challengers = []\n",
    "#             features_champion = model_champion.feature_names_in_\n",
    "#             features_marginal = list( set(features_universe).difference(set(features_champion)) )\n",
    "#             for feature_mrgn in features_marginal:\n",
    "\n",
    "#                 features_challenger = list(features_champion) + [feature_mrgn]\n",
    "#                 model_challenger = Ridge(**RIDGE_ALPHA_TREND_CYCLE_LOCAL_MODELS)\n",
    "#                 model_challenger.fit(\n",
    "#                     XY_grp.loc[is_training, features_challenger],\n",
    "#                     XY_grp.loc[is_training, 'num_sold_log']\n",
    "#                     )\n",
    "\n",
    "#                 models_challengers.append(model_challenger)\n",
    "\n",
    "#                 predictions_challenger = (\n",
    "#                     XY_grp\n",
    "#                     .copy()\n",
    "#                     .assign(yhat = lambda df_: np.exp(model_challenger.predict(df_[features_challenger])))\n",
    "#                     )\n",
    "#                 predictions_challengers.append(predictions_challenger)\n",
    "\n",
    "#                 score_challenger = mean_absolute_percentage_error(\n",
    "#                     predictions_challenger.loc[is_validation, 'num_sold'],\n",
    "#                     predictions_challenger.loc[is_validation, 'yhat']\n",
    "#                     )\n",
    "#                 scores_challengers.append(score_challenger)\n",
    "\n",
    "#             score_best_challenger = min(scores_challengers)\n",
    "#             is_best_challenger_better_vs_champion = score_best_challenger < score_champion\n",
    "#             if is_best_challenger_better_vs_champion:\n",
    "#                 index_best = scores_challengers.index(score_best_challenger)\n",
    "#                 model_champion = models_challengers[index_best]\n",
    "#                 score_champion = scores_challengers[index_best]            \n",
    "#                 predictions_champion = predictions_challengers[index_best]\n",
    "#             else:\n",
    "#                 is_champion_beatable = False\n",
    "            \n",
    "\n",
    "#         SEGMENTS_MODELS_TRAINSET[grp] = model_champion\n",
    "\n",
    "#         segments_predictions.append(predictions_champion)\n",
    "        \n",
    "#         i += 1\n",
    "#         print(f\"{i}/{segments_total} models fit.\")\n",
    "\n",
    "#     predictions = pd.concat(segments_predictions, axis=0)\n",
    "\n",
    "#     scores = {\n",
    "#         'validation': {\n",
    "#             'nobs': predictions.loc[is_validation].shape[0],\n",
    "#             'score': mean_absolute_percentage_error( \n",
    "#                 predictions.loc[is_validation, 'num_sold'],\n",
    "#                 predictions.loc[is_validation, 'yhat']\n",
    "#                 )\n",
    "#             },\n",
    "#         'train': {\n",
    "#             'nobs': predictions.loc[is_training].shape[0],\n",
    "#             'score': mean_absolute_percentage_error( \n",
    "#                 predictions.loc[is_training, 'num_sold'],\n",
    "#                 predictions.loc[is_training, 'yhat']\n",
    "#                 )\n",
    "#             }\n",
    "#         }\n",
    "    \n",
    "#     kfolds_evaluation.append(scores)\n",
    "\n",
    "# kfolds_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_scores = []\n",
    "SEGMENTS_FEATURES_SELECTED = {}\n",
    "\n",
    "segments_XY_sample = {\n",
    "    # 'Canada|Discount Stickers|Kaggle': segments_XY['Canada|Discount Stickers|Kaggle'],\n",
    "    # 'Canada|Discount Stickers|Kaggle Tiers': segments_XY['Canada|Discount Stickers|Kaggle Tiers']\n",
    "    'Kenya|Stickers for Less|Holographic Goose': segments_XY['Kenya|Stickers for Less|Holographic Goose']\n",
    "    }\n",
    "\n",
    "kfold_alternative_latest = {'validate_2016': KFOLDS_ALTERNATIVES['validate_2016']}\n",
    "\n",
    "i = 0\n",
    "segments_total = len(segments_XY)\n",
    "for grp, XY_grp in segments_XY.items():\n",
    "\n",
    "    print(f\"{grp} modeling begun.\")\n",
    "\n",
    "    # for a summary score robust to any one set's idiosyncrasies -- \n",
    "    # score multiple test sets\n",
    "    scores_champion = []\n",
    "    for kfold, (is_training, is_validation) in kfold_alternative_latest.items():\n",
    "\n",
    "        features_champion = ['gdp_per_capita_log']\n",
    "        feature_transform_pipeline = ColumnTransformer(\n",
    "            [('standardizer', StandardScaler(), ['gdp_per_capita_log'])],\n",
    "            verbose_feature_names_out=False,\n",
    "            remainder='drop'\n",
    "            ).set_output(transform='pandas')\n",
    "        \n",
    "        pipeline_e2e = Pipeline([\n",
    "            ('transform_features', feature_transform_pipeline), \n",
    "            ('model', Ridge(**RIDGE_ALPHA_TREND_CYCLE_LOCAL_MODELS))\n",
    "            ])\n",
    "        pipeline_e2e.fit(XY_grp.loc[is_training], XY_grp.loc[is_training, 'num_sold_log'])\n",
    "        model_champion = pipeline_e2e.named_steps.model\n",
    "\n",
    "        predictions_champion = (\n",
    "            XY_grp\n",
    "            .copy()\n",
    "            .assign(yhat = lambda df_: np.exp(pipeline_e2e.predict(df_)))\n",
    "            )\n",
    "        score_champion = mean_absolute_percentage_error( \n",
    "            predictions_champion.loc[is_validation, 'num_sold'],\n",
    "            predictions_champion.loc[is_validation, 'yhat']\n",
    "            )\n",
    "        \n",
    "        scores_champion.append(score_champion)\n",
    "        \n",
    "    score_champion = np.mean(scores_champion)\n",
    "\n",
    "    # ultimate goal: evaluate challenger models, feature-by-feature (marginally).\n",
    "    # expect features universe varies slightly by time series segment.\n",
    "    # example, select weeks may have null outcomes, thus excluding those weeks as categorical levels\n",
    "    feature_transform_pipeline_universe = ColumnTransformer(\n",
    "        [\n",
    "            ('transformer_onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), FEATURES_LOCAL_MODEL_TO_ONEHOT),\n",
    "            ('standardizer', StandardScaler(), FEATURES_UNIVERSE_NUMERIC_CONTINUOUS),\n",
    "            ('select_others', 'passthrough', FEATURES_UNIVERSE_ALREADY_ONEHOT)\n",
    "        ],\n",
    "        verbose_feature_names_out=False,\n",
    "        remainder='drop'\n",
    "        ).set_output(transform='pandas')\n",
    "    # most restrictive training set\n",
    "    is_training_min = XY_grp['date'] < pd.to_datetime(\"2016-01-01\")\n",
    "    XY_grp_features_universe = feature_transform_pipeline_universe.fit_transform(XY_grp.loc[is_training_min])\n",
    "    features_universe = list(XY_grp_features_universe.columns)\n",
    "\n",
    "    is_champion_beatable = True\n",
    "\n",
    "    while is_champion_beatable:\n",
    "\n",
    "        # one \"challenger\" model defined by one features set.\n",
    "        # compare several challengers (feature-sets); then, proceed with the best.\n",
    "        scores_challengers = []\n",
    "        features_marginal = list( set(features_universe).difference(set(features_champion)) )\n",
    "        for feature_mrgn in features_marginal:\n",
    "                \n",
    "            features_challenger = list(features_champion) + [feature_mrgn]\n",
    "            # for challenger's robust evaluation, summarize multiple test set evaluations -- \n",
    "            # wash out any one set's idiosyncrasies\n",
    "            scores_kfolds_challenger = []\n",
    "            for kfold, (is_training, is_validation) in kfold_alternative_latest.items():\n",
    "\n",
    "                # operationally, easier to (1) transform features universe, then (2) select subset\n",
    "                feature_transform_pipeline_universe = ColumnTransformer(\n",
    "                    [\n",
    "                        ('transformer_onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), FEATURES_LOCAL_MODEL_TO_ONEHOT),\n",
    "                        ('standardizer', StandardScaler(), FEATURES_UNIVERSE_NUMERIC_CONTINUOUS),\n",
    "                        ('select_others', 'passthrough', FEATURES_UNIVERSE_ALREADY_ONEHOT)\n",
    "                    ],\n",
    "                    verbose_feature_names_out=False,\n",
    "                    remainder='drop'\n",
    "                    ).set_output(transform='pandas')\n",
    "                \n",
    "                features_selector = ColumnTransformer(\n",
    "                    [('select_relevant', 'passthrough', features_challenger)],\n",
    "                    verbose_feature_names_out=False,\n",
    "                    remainder='drop'\n",
    "                    ).set_output(transform='pandas')\n",
    "                \n",
    "                pipeline_e2e = Pipeline([\n",
    "                    ('transform_features', feature_transform_pipeline_universe), \n",
    "                    ('select_relevant_features', features_selector),\n",
    "                    ('model', Ridge(**RIDGE_ALPHA_TREND_CYCLE_LOCAL_MODELS))\n",
    "                    ])\n",
    "                pipeline_e2e.fit(XY_grp.loc[is_training], XY_grp.loc[is_training, 'num_sold_log'])\n",
    "\n",
    "                predictions = XY_grp.copy().assign(yhat = lambda df_: np.exp(pipeline_e2e.predict(df_)))\n",
    "                score_kfold = mean_absolute_percentage_error( \n",
    "                    predictions.loc[is_validation, 'num_sold'],\n",
    "                    predictions.loc[is_validation, 'yhat']\n",
    "                    )\n",
    "                scores_kfolds_challenger.append(score_kfold)\n",
    "\n",
    "            score_challenger = np.mean(scores_kfolds_challenger)\n",
    "            scores_challengers.append(score_challenger)\n",
    "\n",
    "        # the new champion is the best challenger\n",
    "        score_best_challenger = min(scores_challengers)\n",
    "        is_best_challenger_better_vs_champion = score_best_challenger < score_champion\n",
    "        if is_best_challenger_better_vs_champion:\n",
    "            index_best = scores_challengers.index(score_best_challenger)\n",
    "            features_champion += [features_marginal[index_best]]\n",
    "            score_champion = scores_challengers[index_best]\n",
    "        else:\n",
    "            is_champion_beatable = False\n",
    "\n",
    "    SEGMENTS_FEATURES_SELECTED[grp] = features_champion\n",
    "    segments_scores.append(score_champion)\n",
    "\n",
    "    i += 1\n",
    "    print(f\"Champion features set has size {len(features_champion)}\")\n",
    "    print(f\"{i}/{segments_total} models fit.\")\n",
    "\n",
    "np.mean(segments_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./models/segments_features_select.txt\", 'w') as file:\n",
    "    file.write(str(SEGMENTS_FEATURES_SELECTED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ast\n",
    "\n",
    "# with open(\"./models/segments_features_select.txt\", 'r') as f:\n",
    "#     SEGMENTS_FEATURES_SELECTED = ast.literal_eval(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_no_gdp = ['gdp_per_capita_log' not in x for x in SEGMENTS_FEATURES_SELECTED.values()]\n",
    "assert sum(has_no_gdp) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfolds_evaluation = []\n",
    "for is_training, is_validation in [KFOLDS_ALTERNATIVES['validate_2016']]:\n",
    "\n",
    "    segments_predictions = []\n",
    "\n",
    "    for grp, XY_grp in segments_XY.items():\n",
    "\n",
    "        feature_transform_pipeline_universe = ColumnTransformer(\n",
    "            [\n",
    "                ('transformer_onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), FEATURES_LOCAL_MODEL_TO_ONEHOT),\n",
    "                ('standardizer', StandardScaler(), FEATURES_UNIVERSE_NUMERIC_CONTINUOUS),\n",
    "                ('select_others', 'passthrough', FEATURES_UNIVERSE_ALREADY_ONEHOT)\n",
    "            ],\n",
    "            verbose_feature_names_out=False,\n",
    "            remainder='drop'\n",
    "            ).set_output(transform='pandas')\n",
    "        \n",
    "        features_selected = SEGMENTS_FEATURES_SELECTED[grp]\n",
    "        features_selector = ColumnTransformer(\n",
    "            [('select_relevant', 'passthrough', features_selected)],\n",
    "            verbose_feature_names_out=False,\n",
    "            remainder='drop'\n",
    "            ).set_output(transform='pandas')\n",
    "        \n",
    "        pipeline_e2e = Pipeline([\n",
    "            ('transform_features', feature_transform_pipeline_universe), \n",
    "            ('select_relevant_features', features_selector),\n",
    "            ('model', Ridge(**RIDGE_ALPHA_TREND_CYCLE_LOCAL_MODELS))\n",
    "            ])\n",
    "        pipeline_e2e.fit(XY_grp.loc[is_training], XY_grp.loc[is_training, 'num_sold_log'])\n",
    "\n",
    "        predictions = XY_grp.copy().assign(yhat = lambda df_: np.exp(pipeline_e2e.predict(df_)))\n",
    "\n",
    "        segments_predictions.append(predictions)\n",
    "\n",
    "    predictions = pd.concat(segments_predictions, axis=0)\n",
    "\n",
    "    scores = {\n",
    "        'validation': {\n",
    "            'nobs': predictions.loc[is_validation].shape[0],\n",
    "            'score': mean_absolute_percentage_error( \n",
    "                predictions.loc[is_validation, 'num_sold'],\n",
    "                predictions.loc[is_validation, 'yhat']\n",
    "                )\n",
    "            },\n",
    "        'train': {\n",
    "            'nobs': predictions.loc[is_training].shape[0],\n",
    "            'score': mean_absolute_percentage_error( \n",
    "                predictions.loc[is_training, 'num_sold'],\n",
    "                predictions.loc[is_training, 'yhat']\n",
    "                )\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    kfolds_evaluation.append(scores)\n",
    "\n",
    "kfolds_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = (\n",
    "    predictions\n",
    "    .assign(ape = lambda df_: abs(100 * (df_['num_sold'] / df_['yhat'] - 1)))\n",
    "    .assign(pe = lambda df_: (100 * (df_['num_sold'] / df_['yhat'] - 1)))\n",
    "    .assign(residual = lambda df_: df_['num_sold'] - df_['yhat'])\n",
    "    .assign(residual_log = lambda df_: df_['num_sold_log'] - np.log(df_['yhat']))\n",
    "    .assign(residual_abs = lambda df_: np.abs(df_['residual']))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    predictions\n",
    "    .groupby('country')\n",
    "    ['ape']\n",
    "    .agg(['mean', 'size'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    predictions\n",
    "    .groupby('year')\n",
    "    ['ape']\n",
    "    .agg(['mean', 'size'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    predictions\n",
    "    .groupby(['country', 'year'])\n",
    "    ['ape']\n",
    "    .agg(['mean', 'size'])\n",
    "    .unstack(-1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_sample = predictions.query(\"series_id == 'Norway|Premium Sticker Mart|Kaggle'\")\n",
    "\n",
    "predictions_sample.set_index('date')[['num_sold', 'yhat']].plot.line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    ROUNDS_COUNT = 1_000\n",
    "\n",
    "    param = {\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"booster\": \"gbtree\",\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 9, step=2),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.4, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n",
    "        }\n",
    "\n",
    "    kfolds_evaluation = []\n",
    "    for is_training, is_validation in [KFOLDS_ALTERNATIVES['validate_2016']]:\n",
    "\n",
    "        segments_models = {}\n",
    "        segments_predictions = []\n",
    "\n",
    "        for grp, XY_grp in segments_XY.items():\n",
    "\n",
    "            trend_level_feature_transformer = ColumnTransformer(\n",
    "                [('transformer_std', StandardScaler(), ['gdp_per_capita_log'])],\n",
    "                verbose_feature_names_out=False,\n",
    "                remainder='drop'\n",
    "                ).set_output(transform='pandas')\n",
    "            trend_pipeline = Pipeline([\n",
    "                ('feature_transform_pipeline', trend_level_feature_transformer),\n",
    "                ('model', Ridge(**RIDGE_ALPHA_TREND_CYCLE_LOCAL_MODELS))\n",
    "                ])\n",
    "            trend_pipeline.fit(XY_grp.loc[is_training], XY_grp.loc[is_training, 'num_sold_log'])\n",
    "\n",
    "            predictions = (\n",
    "                XY_grp\n",
    "                .copy()\n",
    "                .assign(yhat_trend_log = lambda df_: trend_pipeline.predict(df_))\n",
    "                .assign(num_sold_log_detrend = lambda df_: df_['num_sold_log'] - df_['yhat_trend_log'])\n",
    "                )\n",
    "            \n",
    "\n",
    "            feature_universe_transformer = ColumnTransformer(\n",
    "                [\n",
    "                    ('transformer_onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), FEATURES_LOCAL_MODEL_TO_ONEHOT),\n",
    "                    ('standardizer', StandardScaler(), FEATURES_UNIVERSE_NUMERIC_CONTINUOUS),\n",
    "                    ('select_others', 'passthrough', FEATURES_UNIVERSE_ALREADY_ONEHOT)\n",
    "                ],\n",
    "                verbose_feature_names_out=False,\n",
    "                remainder='drop'\n",
    "                ).set_output(transform='pandas')\n",
    "            features_selected = SEGMENTS_FEATURES_SELECTED[grp].copy()\n",
    "            # TODO: delete could fail due to data copies across parallel threads?\n",
    "            if 'gdp_per_capita_log' in features_selected:\n",
    "                features_selected.remove('gdp_per_capita_log')\n",
    "            features_selector = ColumnTransformer(\n",
    "                [('select_relevant', 'passthrough', features_selected)],\n",
    "                verbose_feature_names_out=False,\n",
    "                remainder='drop'\n",
    "                ).set_output(transform='pandas')\n",
    "            remainder_feature_transformer = Pipeline([\n",
    "                ('transform_features', feature_universe_transformer), \n",
    "                ('select_relevant_features', features_selector)\n",
    "                ])\n",
    "            \n",
    "            X_remainder_train = remainder_feature_transformer.fit_transform(predictions.loc[is_training])        \n",
    "            dtrain = xgb.DMatrix(X_remainder_train, label=predictions.loc[is_training, 'num_sold_log_detrend'])\n",
    "            model_remainder = xgb.train(param, dtrain, ROUNDS_COUNT)\n",
    "\n",
    "            dtest = xgb.DMatrix(remainder_feature_transformer.transform(predictions))\n",
    "            yhat_remainder_log = model_remainder.predict(dtest)\n",
    "            predictions = (\n",
    "                predictions\n",
    "                .assign(yhat_remainder_log = yhat_remainder_log)\n",
    "                .assign(yhat = lambda df_: np.exp(df_['yhat_trend_log'] + df_['yhat_remainder_log']))\n",
    "                )\n",
    "\n",
    "            segments_predictions.append(predictions)\n",
    "\n",
    "        predictions = pd.concat(segments_predictions, axis=0)\n",
    "\n",
    "        scores = {\n",
    "            'validation': {\n",
    "                'nobs': predictions.loc[is_validation].shape[0],\n",
    "                'score': mean_absolute_percentage_error( \n",
    "                    predictions.loc[is_validation, 'num_sold'],\n",
    "                    predictions.loc[is_validation, 'yhat']\n",
    "                    )\n",
    "                },\n",
    "            'train': {\n",
    "                'nobs': predictions.loc[is_training].shape[0],\n",
    "                'score': mean_absolute_percentage_error( \n",
    "                    predictions.loc[is_training, 'num_sold'],\n",
    "                    predictions.loc[is_training, 'yhat']\n",
    "                    )\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        kfolds_evaluation.append(scores)\n",
    "\n",
    "    score_overall = kfolds_evaluation[0]['validation']['score']\n",
    "\n",
    "    return score_overall\n",
    "\n",
    "\n",
    "study = optuna.create_study()\n",
    "# if optuna returns nulls in y_pred, don't fail the entire study\n",
    "study.optimize(\n",
    "    objective,\n",
    "    n_trials=10,\n",
    "    catch=(ValueError,),\n",
    "    n_jobs=-1,\n",
    "    timeout=12 * 60 * 60,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seasonal & Remainder Component: Nonparametric, Fix Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from preceding optuna run\n",
    "# REMAINDER_MODEL_XGBOOST_ROUNDS_COUNT = 1_000\n",
    "# # TODO: could deeper optimization help?\n",
    "# REMAINDER_XGBOOST_MODEL_BEST_PARAMS = {\n",
    "#     \"objective\": \"reg:squarederror\",\n",
    "#     \"booster\": \"gbtree\",\n",
    "#     'max_depth': 1,\n",
    "#     'subsample': 0.9,\n",
    "#     'colsample_bytree': 0.5\n",
    "#     }\n",
    "\n",
    "# kfolds_evaluation = []\n",
    "# for is_training, is_validation in [KFOLDS_ALTERNATIVES['validate_2016']]:\n",
    "\n",
    "#     segments_models = {}\n",
    "#     segments_predictions = []\n",
    "\n",
    "#     for grp, XY_grp in segments_XY.items():\n",
    "\n",
    "#         trend_level_model_features_selected = SEGMENTS_TREND_MODELS_TRAINSET[grp].named_steps.model\n",
    "#         trend_level_feature_transformer = ColumnTransformer(\n",
    "#             [('transformer_std', StandardScaler(), trend_level_model_features_selected.feature_names_in_)],\n",
    "#             verbose_feature_names_out=False,\n",
    "#             remainder='drop'\n",
    "#             ).set_output(transform='pandas')\n",
    "\n",
    "#         remainder_feature_transformer = ColumnTransformer([\n",
    "#             ('transformer_onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), FEATURES_SEASONALITY_TO_ONEHOT),\n",
    "#             # no transforms required\n",
    "#             (\n",
    "#                 'select_others', \n",
    "#                 'passthrough', \n",
    "#                 FEATURES_SEASONALITY_NUMERIC_CONTINUOUS + FEATURES_UNIVERSE_ALREADY_ONEHOT\n",
    "#             )\n",
    "#             ],\n",
    "#             verbose_feature_names_out=False,\n",
    "#             remainder='drop'\n",
    "#             ).set_output(transform='pandas')\n",
    "\n",
    "#         pipeline_e2e = TrendRemainderModelPipeline(\n",
    "#             trend_level_feature_transformer,\n",
    "#             RIDGE_ALPHA_TREND_CYCLE_LOCAL_MODELS['alpha'],\n",
    "#             remainder_feature_transformer, \n",
    "#             'xgboost',\n",
    "#             REMAINDER_MODEL_XGBOOST_ROUNDS_COUNT, \n",
    "#             REMAINDER_XGBOOST_MODEL_BEST_PARAMS\n",
    "#             )\n",
    "\n",
    "#         pipeline_e2e.fit(XY_grp.loc[is_training], XY_grp.loc[is_training, 'num_sold_log'])\n",
    "        \n",
    "#         segments_models[grp] = pipeline_e2e\n",
    "\n",
    "#         predictions = (\n",
    "#             XY_grp\n",
    "#             .copy()\n",
    "#             .assign(yhat = lambda df_: np.exp(pipeline_e2e.predict(df_)))\n",
    "#             )\n",
    "#         segments_predictions.append(predictions)\n",
    "\n",
    "#     predictions = pd.concat(segments_predictions, axis=0)\n",
    "\n",
    "#     scores = {\n",
    "#         'validation': {\n",
    "#             'nobs': predictions.loc[is_validation].shape[0],\n",
    "#             'score': mean_absolute_percentage_error( \n",
    "#                 predictions.loc[is_validation, 'num_sold'],\n",
    "#                 predictions.loc[is_validation, 'yhat']\n",
    "#                 )\n",
    "#             },\n",
    "#         'train': {\n",
    "#             'nobs': predictions.loc[is_training].shape[0],\n",
    "#             'score': mean_absolute_percentage_error( \n",
    "#                 predictions.loc[is_training, 'num_sold'],\n",
    "#                 predictions.loc[is_training, 'yhat']\n",
    "#                 )\n",
    "#             }\n",
    "#         }\n",
    "    \n",
    "#     kfolds_evaluation.append(scores)\n",
    "\n",
    "# kfolds_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = (\n",
    "#     predictions\n",
    "#     .assign(ape = lambda df_: abs(100 * (df_['num_sold'] / df_['yhat'] - 1)))\n",
    "#     .assign(pe = lambda df_: (100 * (df_['num_sold'] / df_['yhat'] - 1)))\n",
    "#     .assign(residual = lambda df_: df_['num_sold'] - df_['yhat'])\n",
    "#     .assign(residual_log = lambda df_: df_['num_sold_log'] - np.log(df_['yhat']))\n",
    "#     .assign(residual_abs = lambda df_: np.abs(df_['residual']))\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (\n",
    "#     predictions\n",
    "#     .groupby('country')\n",
    "#     ['ape']\n",
    "#     .agg(['mean', 'size'])\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (\n",
    "#     predictions\n",
    "#     .groupby('year')\n",
    "#     ['ape']\n",
    "#     .agg(['mean', 'size'])\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (\n",
    "#     predictions\n",
    "#     .groupby(['country', 'year'])\n",
    "#     ['ape']\n",
    "#     .agg(['mean', 'size'])\n",
    "#     .unstack(-1)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_sample = segments_models['Kenya|Stickers for Less|Holographic Goose']\n",
    "# importances = pd.DataFrame.from_dict(model_sample.remainder_model.get_score(importance_type='gain'), orient='index')\n",
    "# importances.sort_values(0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions_sample = predictions.query(\"series_id == 'Norway|Premium Sticker Mart|Kaggle'\")\n",
    "\n",
    "# predictions_sample.set_index('date')[['num_sold', 'yhat']].plot.line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions_sample = (\n",
    "#     predictions_sample\n",
    "#     .assign(residual = lambda df_: df_['num_sold'] - df_['yhat'])\n",
    "#     .sort_values('date')\n",
    "#     .assign(\n",
    "#         residual_lag1 = lambda df_: df_['residual'].shift(1),\n",
    "#         residual_lag2 = lambda df_: df_['residual'].shift(2),\n",
    "#         residual_lag3 = lambda df_: df_['residual'].shift(3),\n",
    "#         residual_lag5 = lambda df_: df_['residual'].shift(5),\n",
    "#         residual_lag7 = lambda df_: df_['residual'].shift(7),\n",
    "#         residual_lag14 = lambda df_: df_['residual'].shift(14),\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "# (\n",
    "#     p9.ggplot((\n",
    "#         predictions_sample\n",
    "#         .loc[predictions_sample['date'] <= pd.to_datetime(\"2014-01-01\")]\n",
    "#         )) + \n",
    "#     p9.theme_bw() + \n",
    "#     p9.geom_point(p9.aes('residual_lag1', 'residual')) + \n",
    "#     p9.geom_smooth(p9.aes('residual_lag1', 'residual'), method='lm', se=False, color='lightblue')\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (\n",
    "#     predictions\n",
    "#     .assign(mape = lambda df_: np.abs(100 * (df_['num_sold']/df_['yhat'] - 1)) )\n",
    "#     .sort_values('mape', ascending=False)\n",
    "#     .head(100)\n",
    "#     .to_csv(\"./data/processed/predictions_errors_local_model.csv\", index=False)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (\n",
    "#     predictions\n",
    "#     .assign(mape = lambda df_: np.abs(100 * (df_['num_sold']/df_['yhat'] - 1)) )\n",
    "#     .sort_values('mape', ascending=False)\n",
    "#     .head(50)\n",
    "#     .to_csv(\"./data/processed/predictions_local_model.csv\")\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions_sample = predictions.query(\"series_id == 'Kenya|Stickers for Less|Holographic Goose'\")\n",
    "\n",
    "# predictions_sample.set_index('date')[['num_sold', 'yhat']].plot.line()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ultimate Models Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEGMENTS_MODELS = {}\n",
    "\n",
    "# for grp, XY_grp in segments_XY.items():\n",
    "\n",
    "#     feature_transform_pipeline_universe = ColumnTransformer(\n",
    "#         [\n",
    "#             ('transformer_onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), FEATURES_LOCAL_MODEL_TO_ONEHOT),\n",
    "#             ('standardizer', StandardScaler(), FEATURES_UNIVERSE_NUMERIC_CONTINUOUS),\n",
    "#             ('select_others', 'passthrough', FEATURES_UNIVERSE_ALREADY_ONEHOT)\n",
    "#         ],\n",
    "#         verbose_feature_names_out=False,\n",
    "#         remainder='drop'\n",
    "#         ).set_output(transform='pandas')\n",
    "    \n",
    "#     features_selected = SEGMENTS_FEATURES_SELECTED[grp]\n",
    "#     features_selector = ColumnTransformer(\n",
    "#         [('select_relevant', 'passthrough', features_selected)],\n",
    "#         verbose_feature_names_out=False,\n",
    "#         remainder='drop'\n",
    "#         ).set_output(transform='pandas')\n",
    "    \n",
    "#     pipeline_e2e = Pipeline([\n",
    "#         ('transform_features', feature_transform_pipeline_universe), \n",
    "#         ('select_relevant_features', features_selector),\n",
    "#         ('model', Ridge(**RIDGE_ALPHA_TREND_CYCLE_LOCAL_MODELS))\n",
    "#         ])\n",
    "#     pipeline_e2e.fit(XY_grp, XY_grp['num_sold_log'])\n",
    "    \n",
    "#     SEGMENTS_MODELS[grp] = pipeline_e2e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEGMENTS_MODELS = {}\n",
    "\n",
    "# for grp, XY_grp in segments_XY.items():\n",
    "\n",
    "#     trend_level_model_features_selected = SEGMENTS_TREND_MODELS_TRAINSET[grp].named_steps.model\n",
    "#     trend_level_feature_transformer = ColumnTransformer(\n",
    "#         [('transformer_std', StandardScaler(), trend_level_model_features_selected.feature_names_in_)],\n",
    "#         verbose_feature_names_out=False,\n",
    "#         remainder='drop'\n",
    "#         ).set_output(transform='pandas')\n",
    "\n",
    "#     remainder_feature_transformer = ColumnTransformer([\n",
    "#         ('transformer_onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), FEATURES_SEASONALITY_TO_ONEHOT),\n",
    "#         (\n",
    "#             'select_others', \n",
    "#             'passthrough', \n",
    "#             FEATURES_SEASONALITY_NUMERIC_CONTINUOUS + FEATURES_UNIVERSE_ALREADY_ONEHOT\n",
    "#         )\n",
    "#         ],\n",
    "#         verbose_feature_names_out=False,\n",
    "#         remainder='drop'\n",
    "#         ).set_output(transform='pandas')\n",
    "\n",
    "#     pipeline_e2e = TrendRemainderModelPipeline(\n",
    "#         trend_level_feature_transformer,\n",
    "#         RIDGE_ALPHA_TREND_CYCLE_LOCAL_MODELS['alpha'],\n",
    "#         remainder_feature_transformer, \n",
    "#         'xgboost',\n",
    "#         REMAINDER_MODEL_XGBOOST_ROUNDS_COUNT, \n",
    "#         REMAINDER_XGBOOST_MODEL_BEST_PARAMS\n",
    "#         )\n",
    "#     pipeline_e2e.fit(XY_grp, XY_grp['num_sold_log'])\n",
    "    \n",
    "#     SEGMENTS_MODELS[grp] = pipeline_e2e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonparametric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfolds_evaluation = []\n",
    "for is_training, is_validation in [KFOLDS_ALTERNATIVES['validate_2016']]:\n",
    "\n",
    "    feature_transform_pipeline_global_model = ColumnTransformer([\n",
    "        ('transformer_onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), FEATURES_UNIVERSE_TO_ONEHOT),\n",
    "        ('transformer_std', StandardScaler(), FEATURES_UNIVERSE_NUMERIC_CONTINUOUS)\n",
    "        ],\n",
    "        verbose_feature_names_out=False,\n",
    "        # caller expected to provide some features that require no transforms.\n",
    "        # don't drop those just because they received no special operations.\n",
    "        # but do be sure to drop non-features in advance!\n",
    "        remainder='passthrough'\n",
    "        )\n",
    "    feature_transform_pipeline_global_model.set_output(transform='pandas')\n",
    "    \n",
    "    pipeline_e2e = Pipeline([\n",
    "        ('transform_features', feature_transform_pipeline_global_model), \n",
    "        ('model', RandomForestRegressor(n_estimators=100, n_jobs=-1))\n",
    "        ])\n",
    "\n",
    "    pipeline_e2e.fit(\n",
    "        XY.loc[is_training, FEATURES_GLOBAL_MODEL],\n",
    "        XY.loc[is_training, 'num_sold_log']\n",
    "        )\n",
    "    \n",
    "    predictions = (\n",
    "        XY\n",
    "        .copy()\n",
    "        .assign(yhat = lambda df_: np.exp(pipeline_e2e.predict(df_[FEATURES_GLOBAL_MODEL])))\n",
    "        )\n",
    "    \n",
    "    scores = {\n",
    "        'validation': {\n",
    "            'nobs': predictions.loc[is_validation].shape[0],\n",
    "            'score': mean_absolute_percentage_error( \n",
    "                predictions.loc[is_validation, 'num_sold'],\n",
    "                predictions.loc[is_validation, 'yhat']\n",
    "                )\n",
    "            },\n",
    "        'train': {\n",
    "            'nobs': predictions.loc[is_training].shape[0],\n",
    "            'score': mean_absolute_percentage_error( \n",
    "                predictions.loc[is_training, 'num_sold'],\n",
    "                predictions.loc[is_training, 'yhat']\n",
    "                )\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    kfolds_evaluation.append(scores)\n",
    "\n",
    "kfolds_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_sample = predictions.query(\"series_id == 'Norway|Stickers for Less|Kaggle'\")\n",
    "\n",
    "predictions_sample.set_index('date')[['num_sold', 'yhat']].plot.line(alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfolds_evaluation = []\n",
    "for is_training, is_validation in [KFOLDS_ALTERNATIVES['validate_2016']]:\n",
    "\n",
    "    feature_transform_pipeline_global_model = ColumnTransformer([\n",
    "        ('transformer_onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), FEATURES_UNIVERSE_TO_ONEHOT),\n",
    "        ('transformer_std', StandardScaler(), FEATURES_UNIVERSE_NUMERIC_CONTINUOUS)\n",
    "        ],\n",
    "        verbose_feature_names_out=False,\n",
    "        remainder='passthrough'\n",
    "        )\n",
    "    feature_transform_pipeline_global_model.set_output(transform='pandas')\n",
    "\n",
    "    pipeline_e2e = Pipeline([\n",
    "        ('transform_features', feature_transform_pipeline_global_model), \n",
    "        ('model', Ridge(1e-2))\n",
    "        ])\n",
    "\n",
    "    pipeline_e2e.fit(\n",
    "        XY.loc[is_training, FEATURES_GLOBAL_MODEL],\n",
    "        XY.loc[is_training, 'num_sold_log']\n",
    "        )\n",
    "    \n",
    "    predictions = (\n",
    "        XY\n",
    "        .copy()\n",
    "        .assign(yhat = lambda df_: np.exp(pipeline_e2e.predict(df_[FEATURES_GLOBAL_MODEL])))\n",
    "        )\n",
    "    \n",
    "    scores = {\n",
    "        'validation': {\n",
    "            'nobs': predictions.loc[is_validation].shape[0],\n",
    "            'score': mean_absolute_percentage_error( \n",
    "                predictions.loc[is_validation, 'num_sold'],\n",
    "                predictions.loc[is_validation, 'yhat']\n",
    "                )\n",
    "            },\n",
    "        'train': {\n",
    "            'nobs': predictions.loc[is_training].shape[0],\n",
    "            'score': mean_absolute_percentage_error( \n",
    "                predictions.loc[is_training, 'num_sold'],\n",
    "                predictions.loc[is_training, 'yhat']\n",
    "                )\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    kfolds_evaluation.append(scores)\n",
    "\n",
    "kfolds_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_sample = predictions.query(\"series_id == 'Norway|Stickers for Less|Kaggle'\")\n",
    "# predictions_sample = predictions.query(\"series_id == 'Norway|Premium Sticker Mart|Kaggle'\")\n",
    "\n",
    "predictions_sample.set_index('date')[['num_sold', 'yhat']].plot.line(alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trend-Cycle Component: Parametric, Explore Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_TREND_CYCLE_TO_ONEHOT = [\n",
    "    'country', \n",
    "    'store',\n",
    "    'product',\n",
    "    'country_store',\n",
    "    'country_product',\n",
    "    'store_product',\n",
    "    ]\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    parameters = {'alpha': trial.suggest_float(\"alpha\", 1e-1, 1_000, log=True)}\n",
    "\n",
    "    kfolds_evaluation = []\n",
    "    for is_training, is_validation in [KFOLDS_ALTERNATIVES['validate_2016']]:\n",
    "\n",
    "        feature_transform_pipeline_global_model = ColumnTransformer([\n",
    "            ('transformer_onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), FEATURES_TREND_CYCLE_TO_ONEHOT),\n",
    "            ('transformer_std', StandardScaler(), FEATURES_TREND_CYCLE_NUMERIC_CONTINUOUS)\n",
    "            ],\n",
    "            verbose_feature_names_out=False,\n",
    "            remainder='drop'\n",
    "            )\n",
    "        feature_transform_pipeline_global_model.set_output(transform='pandas')\n",
    "\n",
    "        pipeline_e2e = Pipeline([\n",
    "            ('transform_features', feature_transform_pipeline_global_model), \n",
    "            ('model', Ridge(**parameters))\n",
    "            ])\n",
    "\n",
    "        pipeline_e2e.fit(XY.loc[is_training], XY.loc[is_training, 'num_sold_log'])\n",
    "        \n",
    "        predictions = (\n",
    "            XY\n",
    "            .copy()\n",
    "            .assign(yhat = lambda df_: np.exp(pipeline_e2e.predict(df_)))\n",
    "            )\n",
    "        \n",
    "        scores = {\n",
    "            'validation': {\n",
    "                'nobs': predictions.loc[is_validation].shape[0],\n",
    "                'score': mean_absolute_percentage_error( \n",
    "                    predictions.loc[is_validation, 'num_sold'],\n",
    "                    predictions.loc[is_validation, 'yhat']\n",
    "                    )\n",
    "                },\n",
    "            'train': {\n",
    "                'nobs': predictions.loc[is_training].shape[0],\n",
    "                'score': mean_absolute_percentage_error( \n",
    "                    predictions.loc[is_training, 'num_sold'],\n",
    "                    predictions.loc[is_training, 'yhat']\n",
    "                    )\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        kfolds_evaluation.append(scores)\n",
    "\n",
    "    score_overall = np.mean([ \n",
    "        kfolds_evaluation[0]['validation']['score']\n",
    "        ])\n",
    "    \n",
    "    return score_overall\n",
    "\n",
    "study = optuna.create_study()\n",
    "\n",
    "study.optimize(\n",
    "    objective,\n",
    "    n_trials=50,\n",
    "    catch=(ValueError,),\n",
    "    n_jobs=-1,\n",
    "    timeout=12 * 60 * 60,\n",
    ")\n",
    "\n",
    "RIDGE_ALPHA_TREND_CYCLE_GLOBAL_MODEL = study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_trial, study.best_params, study.best_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seasonal & Remainder Component: Nonparametric, Explore Params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curiously, haven't improved global model by implementing xgboost remainder model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def objective(trial):\n",
    "\n",
    "#     ROUNDS_COUNT = 1_000\n",
    "\n",
    "#     param = {\n",
    "#         \"objective\": \"reg:squarederror\",\n",
    "#         \"booster\": \"gbtree\",\n",
    "#         \"max_depth\": trial.suggest_int(\"max_depth\", 1, 9, step=2),\n",
    "#         \"subsample\": trial.suggest_float(\"subsample\", 0.4, 1.0),\n",
    "#         \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n",
    "#         }\n",
    "\n",
    "#     kfolds_evaluation = []\n",
    "#     for is_training, is_validation in [KFOLDS_ALTERNATIVES['validate_2016']]:\n",
    "\n",
    "#         feature_transform_pipeline_trend_model = ColumnTransformer([\n",
    "#             ('transformer_onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), FEATURES_TREND_CYCLE_TO_ONEHOT),\n",
    "#             ('transformer_std', StandardScaler(), FEATURES_TREND_CYCLE_NUMERIC_CONTINUOUS)\n",
    "#             ],\n",
    "#             verbose_feature_names_out=False,\n",
    "#             remainder='drop'\n",
    "#             ).set_output(transform='pandas')\n",
    "        \n",
    "#         pipeline_trend_e2e = Pipeline([\n",
    "#             ('transform_features', feature_transform_pipeline_trend_model), \n",
    "#             ('model', Ridge(**RIDGE_ALPHA_TREND_CYCLE_GLOBAL_MODEL))\n",
    "#             ])\n",
    "#         pipeline_trend_e2e.fit(XY.loc[is_training], XY.loc[is_training, 'num_sold_log'])\n",
    "\n",
    "#         predictions = (\n",
    "#             XY\n",
    "#             .copy()\n",
    "#             .assign(yhat_trend_log = lambda df_: pipeline_trend_e2e.predict(df_))\n",
    "#             .assign(num_sold_log_detrend = lambda df_: df_['num_sold_log'] - df_['yhat_trend_log'])\n",
    "#             )\n",
    "\n",
    "\n",
    "#         remainder_feature_transformer = ColumnTransformer([\n",
    "#             ('transformer_onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), FEATURES_SEASONALITY_TO_ONEHOT),\n",
    "#             # no transforms required\n",
    "#             (\n",
    "#                 'select_others', \n",
    "#                 'passthrough', \n",
    "#                 FEATURES_SEASONALITY_NUMERIC_CONTINUOUS + FEATURES_UNIVERSE_ALREADY_ONEHOT + ['yhat_trend_log']\n",
    "#             )\n",
    "#             ],\n",
    "#             verbose_feature_names_out=False,\n",
    "#             remainder='drop'\n",
    "#             ).set_output(transform='pandas')\n",
    "        \n",
    "#         X_remainder_train = remainder_feature_transformer.fit_transform(predictions.loc[is_training])        \n",
    "#         dtrain = xgb.DMatrix(X_remainder_train, label=predictions.loc[is_training, 'num_sold_log_detrend'])\n",
    "#         model_remainder = xgb.train(param, dtrain, ROUNDS_COUNT)\n",
    "\n",
    "#         dtest = xgb.DMatrix(remainder_feature_transformer.transform(predictions))\n",
    "#         yhat_remainder_log = model_remainder.predict(dtest)\n",
    "#         predictions = (\n",
    "#             predictions\n",
    "#             .assign(yhat_remainder_log = yhat_remainder_log)\n",
    "#             .assign(yhat = lambda df_: np.exp(df_['yhat_trend_log'] + df_['yhat_remainder_log']))\n",
    "#             )\n",
    "\n",
    "\n",
    "#         scores = {\n",
    "#             'validation': {\n",
    "#                 'nobs': predictions.loc[is_validation].shape[0],\n",
    "#                 'score': mean_absolute_percentage_error( \n",
    "#                     predictions.loc[is_validation, 'num_sold'],\n",
    "#                     predictions.loc[is_validation, 'yhat']\n",
    "#                     )\n",
    "#                 },\n",
    "#             'train': {\n",
    "#                 'nobs': predictions.loc[is_training].shape[0],\n",
    "#                 'score': mean_absolute_percentage_error( \n",
    "#                     predictions.loc[is_training, 'num_sold'],\n",
    "#                     predictions.loc[is_training, 'yhat']\n",
    "#                     )\n",
    "#                 }\n",
    "#             }\n",
    "        \n",
    "#         kfolds_evaluation.append(scores)\n",
    "\n",
    "#     score_overall = kfolds_evaluation[0]['validation']['score']\n",
    "\n",
    "#     return score_overall\n",
    "\n",
    "\n",
    "# study = optuna.create_study()\n",
    "# # if optuna returns nulls in y_pred, don't fail the entire study\n",
    "# study.optimize(\n",
    "#     objective,\n",
    "#     n_trials=10,\n",
    "#     catch=(ValueError,),\n",
    "#     n_jobs=-1,\n",
    "#     timeout=12 * 60 * 60,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMAINDER_XGBOOST_GLOBAL_MODEL_BEST_PARAMS = {\n",
    "#     \"objective\": \"reg:squarederror\",\n",
    "#     \"booster\": \"gbtree\",\n",
    "#     'max_depth': 1,\n",
    "#     'subsample': 0.64,\n",
    "#     'colsample_bytree': 0.6\n",
    "#     }\n",
    "\n",
    "# kfolds_evaluation = []\n",
    "# for is_training, is_validation in [KFOLDS_ALTERNATIVES['validate_2016']]:\n",
    "\n",
    "#     trend_level_feature_transformer = ColumnTransformer([\n",
    "#         ('transformer_onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), FEATURES_TREND_CYCLE_TO_ONEHOT),\n",
    "#         ('transformer_std', StandardScaler(), FEATURES_TREND_CYCLE_NUMERIC_CONTINUOUS)\n",
    "#         ],\n",
    "#         verbose_feature_names_out=False,\n",
    "#         remainder='drop'\n",
    "#         ).set_output(transform='pandas')\n",
    "\n",
    "#     remainder_feature_transformer = ColumnTransformer([\n",
    "#         ('transformer_onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), FEATURES_UNIVERSE_TO_ONEHOT),\n",
    "#         # no transforms required\n",
    "#         (\n",
    "#             'select_others', \n",
    "#             'passthrough', \n",
    "#             FEATURES_SEASONALITY_NUMERIC_CONTINUOUS + FEATURES_UNIVERSE_ALREADY_ONEHOT\n",
    "#         )\n",
    "#         ],\n",
    "#         verbose_feature_names_out=False,\n",
    "#         remainder='drop'\n",
    "#         ).set_output(transform='pandas')\n",
    "\n",
    "#     model_global_pipeline = TrendRemainderModelPipeline(\n",
    "#         trend_level_feature_transformer,\n",
    "#         RIDGE_ALPHA_TREND_CYCLE_GLOBAL_MODEL['alpha'],\n",
    "#         remainder_feature_transformer,\n",
    "#         'xgboost',\n",
    "#         1_000,\n",
    "#         REMAINDER_XGBOOST_GLOBAL_MODEL_BEST_PARAMS\n",
    "#         )\n",
    "#     model_global_pipeline.fit(XY.loc[is_training], XY.loc[is_training, 'num_sold_log'])\n",
    "    \n",
    "#     predictions = XY.copy()\n",
    "#     predictions = (\n",
    "#         predictions\n",
    "#         .assign(yhat_log = lambda df_: model_global_pipeline.predict(df_))\n",
    "#         .assign(yhat = lambda df_: np.exp(df_['yhat_log']) )\n",
    "#         )\n",
    "    \n",
    "#     scores = {\n",
    "#         'validation': {\n",
    "#             'nobs': predictions.loc[is_validation].shape[0],\n",
    "#             'score': mean_absolute_percentage_error( \n",
    "#                 predictions.loc[is_validation, 'num_sold'],\n",
    "#                 predictions.loc[is_validation, 'yhat']\n",
    "#                 )\n",
    "#             },\n",
    "#         'train': {\n",
    "#             'nobs': predictions.loc[is_training].shape[0],\n",
    "#             'score': mean_absolute_percentage_error( \n",
    "#                 predictions.loc[is_training, 'num_sold'],\n",
    "#                 predictions.loc[is_training, 'yhat']\n",
    "#                 )\n",
    "#             }\n",
    "#         }\n",
    "    \n",
    "#     kfolds_evaluation.append(scores)\n",
    "\n",
    "# kfolds_evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seasonal & Remainder Component: Nonparametric, Fix Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfolds_evaluation = []\n",
    "for is_training, is_validation in [KFOLDS_ALTERNATIVES['validate_2016']]:\n",
    "\n",
    "    trend_level_feature_transformer = ColumnTransformer([\n",
    "        ('transformer_onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), FEATURES_TREND_CYCLE_TO_ONEHOT),\n",
    "        ('transformer_std', StandardScaler(), FEATURES_TREND_CYCLE_NUMERIC_CONTINUOUS)\n",
    "        ],\n",
    "        verbose_feature_names_out=False,\n",
    "        remainder='drop'\n",
    "        ).set_output(transform='pandas')\n",
    "\n",
    "    remainder_feature_transformer = ColumnTransformer([\n",
    "        ('transformer_onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), FEATURES_UNIVERSE_TO_ONEHOT),\n",
    "        # no transforms required\n",
    "        (\n",
    "            'select_others', \n",
    "            'passthrough', \n",
    "            FEATURES_SEASONALITY_NUMERIC_CONTINUOUS + FEATURES_UNIVERSE_ALREADY_ONEHOT\n",
    "        )\n",
    "        ],\n",
    "        verbose_feature_names_out=False,\n",
    "        remainder='drop'\n",
    "        ).set_output(transform='pandas')\n",
    "\n",
    "    model_global_pipeline = TrendRemainderModelPipeline(\n",
    "        trend_level_feature_transformer,\n",
    "        RIDGE_ALPHA_TREND_CYCLE_GLOBAL_MODEL['alpha'],\n",
    "        remainder_feature_transformer,\n",
    "        'random_forest'\n",
    "        )\n",
    "    model_global_pipeline.fit(XY.loc[is_training], XY.loc[is_training, 'num_sold_log'])\n",
    "    \n",
    "    predictions = XY.copy()\n",
    "    predictions = (\n",
    "        predictions\n",
    "        .assign(yhat_log = lambda df_: model_global_pipeline.predict(df_))\n",
    "        .assign(yhat = lambda df_: np.exp(df_['yhat_log']) )\n",
    "        )\n",
    "    \n",
    "    scores = {\n",
    "        'validation': {\n",
    "            'nobs': predictions.loc[is_validation].shape[0],\n",
    "            'score': mean_absolute_percentage_error( \n",
    "                predictions.loc[is_validation, 'num_sold'],\n",
    "                predictions.loc[is_validation, 'yhat']\n",
    "                )\n",
    "            },\n",
    "        'train': {\n",
    "            'nobs': predictions.loc[is_training].shape[0],\n",
    "            'score': mean_absolute_percentage_error( \n",
    "                predictions.loc[is_training, 'num_sold'],\n",
    "                predictions.loc[is_training, 'yhat']\n",
    "                )\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    kfolds_evaluation.append(scores)\n",
    "\n",
    "kfolds_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    predictions\n",
    "    .assign(ape = lambda df_: abs(100 * (df_['num_sold'] / df_['yhat'] - 1)))\n",
    "    .groupby('country')\n",
    "    ['ape']\n",
    "    .agg(['mean', 'size'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_sample = predictions.query(\"series_id == 'Norway|Stickers for Less|Kaggle'\")\n",
    "# predictions_sample = predictions.query(\"series_id == 'Norway|Premium Sticker Mart|Kaggle'\")\n",
    "\n",
    "predictions_sample.set_index('date')[['num_sold', 'yhat']].plot.line(alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WIP\n",
    "\n",
    "# kfolds = [\n",
    "\n",
    "#     # validation set 2014-16 matches ultimate test set's length, 2017-19\n",
    "#     ( \n",
    "#         ( (XY['date'] >= pd.to_datetime('2010-01-01')) & (XY['date'] < pd.to_datetime(\"2014-01-01\")) ),\n",
    "#         XY['date'] >= pd.to_datetime(\"2014-01-01\")\n",
    "#     ),\n",
    "\n",
    "#     # ( \n",
    "#     #     ( (XY['date'] >= pd.to_datetime('2010-01-01')) & (XY['date'] < pd.to_datetime(\"2016-01-01\")) ),\n",
    "#     #     XY['date'] >= pd.to_datetime(\"2016-01-01\")\n",
    "#     # )\n",
    "\n",
    "#     ]\n",
    "\n",
    "# def objective(trial):\n",
    "\n",
    "#     ROUNDS_COUNT = 1_000\n",
    "\n",
    "#     param = {\n",
    "#         \"objective\": \"reg:squarederror\",\n",
    "#         \"booster\": trial.suggest_categorical(\n",
    "#             \"booster\", [\"gbtree\", \"gblinear\", \"dart\"]\n",
    "#         ),\n",
    "#         \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "#         \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "#         \"subsample\": trial.suggest_float(\"subsample\", 0.4, 1.0),\n",
    "#         \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n",
    "#         'device': 'cuda',\n",
    "#         'tree_method': 'hist'\n",
    "#     }\n",
    "\n",
    "#     if param[\"booster\"] in [\"gbtree\", \"dart\"]:\n",
    "#         param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 1, 9, step=2)\n",
    "\n",
    "#         param[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 1, 10)\n",
    "#         param[\"eta\"] = trial.suggest_float(\"eta\", 1e-5, 0.01, log=True)\n",
    "\n",
    "#         param[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
    "#         param[\"grow_policy\"] = trial.suggest_categorical(\n",
    "#             \"grow_policy\", [\"depthwise\", \"lossguide\"]\n",
    "#         )\n",
    "\n",
    "#     if param[\"booster\"] == \"dart\":\n",
    "#         param[\"sample_type\"] = trial.suggest_categorical(\n",
    "#             \"sample_type\", [\"uniform\", \"weighted\"]\n",
    "#         )\n",
    "#         param[\"normalize_type\"] = trial.suggest_categorical(\n",
    "#             \"normalize_type\", [\"tree\", \"forest\"]\n",
    "#         )\n",
    "#         param[\"rate_drop\"] = trial.suggest_float(\n",
    "#             \"rate_drop\", 1e-8, 1.0, log=True\n",
    "#         )\n",
    "#         param[\"skip_drop\"] = trial.suggest_float(\n",
    "#             \"skip_drop\", 1e-8, 1.0, log=True\n",
    "#         )\n",
    "\n",
    "\n",
    "#     kfolds_evaluation = []\n",
    "#     for is_training, is_validation in kfolds:\n",
    "\n",
    "#         feature_transform_pipeline_global_model = ColumnTransformer([\n",
    "#             ('transformer_onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), FEATURES_UNIVERSE_TO_ONEHOT),\n",
    "#             ('transformer_std', StandardScaler(), FEATURES_UNIVERSE_NUMERIC_CONTINUOUS)\n",
    "#             ],\n",
    "#             verbose_feature_names_out=False,\n",
    "#             # caller expected to provide some features that require no transforms.\n",
    "#             # don't drop those just because they received no special operations.\n",
    "#             # but do be sure to drop non-features in advance!\n",
    "#             remainder='passthrough'\n",
    "#             )\n",
    "#         feature_transform_pipeline_global_model.set_output(transform='pandas')\n",
    "\n",
    "#         X_train = (\n",
    "#             feature_transform_pipeline_global_model\n",
    "#             .fit_transform(XY.loc[is_training, FEATURES_GLOBAL_MODEL])\n",
    "#             )\n",
    "#         dtrain = xgb.DMatrix(\n",
    "#             X_train, \n",
    "#             label=XY.loc[is_training, 'num_sold_log']\n",
    "#             )\n",
    "        \n",
    "#         X_test = feature_transform_pipeline_global_model.transform(XY)\n",
    "#         dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "#         model_global = xgb.train(param, dtrain, ROUNDS_COUNT)\n",
    "        \n",
    "#         yhat_log = model_global.predict(dtest)\n",
    "#         predictions = (\n",
    "#             XY\n",
    "#             .copy()\n",
    "#             .assign(yhat = np.exp(yhat_log))\n",
    "#             )\n",
    "\n",
    "#         scores = {\n",
    "#             'validation': mean_absolute_percentage_error( \n",
    "#                 predictions.loc[is_validation, 'num_sold'],\n",
    "#                 predictions.loc[is_validation, 'yhat']\n",
    "#                 ),\n",
    "#             'train': mean_absolute_percentage_error( \n",
    "#                 predictions.loc[is_training, 'num_sold'],\n",
    "#                 predictions.loc[is_training, 'yhat']\n",
    "#                 )\n",
    "#             }\n",
    "        \n",
    "#         kfolds_evaluation.append(scores)\n",
    "\n",
    "#     score_overall = np.mean([ \n",
    "#         kfolds_evaluation[0]['validation'], \n",
    "#         # kfolds_evaluation[1]['validation'] \n",
    "#         ])\n",
    "    \n",
    "#     return score_overall\n",
    "\n",
    "# study = optuna.create_study()\n",
    "\n",
    "# study.optimize(\n",
    "#     objective,\n",
    "#     n_trials=50,\n",
    "#     catch=(ValueError,),\n",
    "#     n_jobs=-1,\n",
    "#     timeout=12 * 60 * 60,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trend_level_feature_transformer = ColumnTransformer([\n",
    "    ('transformer_onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), FEATURES_TREND_CYCLE_TO_ONEHOT),\n",
    "    ('transformer_std', StandardScaler(), FEATURES_TREND_CYCLE_NUMERIC_CONTINUOUS)\n",
    "    ],\n",
    "    verbose_feature_names_out=False,\n",
    "    remainder='drop'\n",
    "    ).set_output(transform='pandas')\n",
    "\n",
    "remainder_feature_transformer = ColumnTransformer([\n",
    "    ('transformer_onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), FEATURES_UNIVERSE_TO_ONEHOT),\n",
    "    # no transforms required\n",
    "    (\n",
    "        'select_others', \n",
    "        'passthrough', \n",
    "        FEATURES_SEASONALITY_NUMERIC_CONTINUOUS + FEATURES_UNIVERSE_ALREADY_ONEHOT\n",
    "    )\n",
    "    ],\n",
    "    verbose_feature_names_out=False,\n",
    "    remainder='drop'\n",
    "    ).set_output(transform='pandas')\n",
    "\n",
    "MODEL_GLOBAL = TrendRemainderModelPipeline(\n",
    "    trend_level_feature_transformer,\n",
    "    RIDGE_ALPHA_TREND_CYCLE_GLOBAL_MODEL['alpha'],\n",
    "    remainder_feature_transformer,\n",
    "    'random_forest'\n",
    "    )\n",
    "\n",
    "MODEL_GLOBAL.fit(XY, XY['num_sold_log'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_test_daily = pd.read_csv(\"./data/external/test.csv\").assign(\n",
    "\n",
    "    date = lambda df_: pd.to_datetime(df_['date']),\n",
    "\n",
    "    country_store = lambda df_: df_['country'].str.cat(df_['store'], sep='|'),\n",
    "    country_product = lambda df_: df_['country'].str.cat(df_['product'], sep='|'),\n",
    "    store_product = lambda df_: df_['store'].str.cat(df_['product'], sep='|'),\n",
    "    country_store_product = lambda df_: df_['country'].str.cat([df_['store'], df_['product']], sep='|')\n",
    "\n",
    "    ).assign(series_id = lambda df_: df_['country_store_product'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_test_daily.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_test_daily['date'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_test_daily = transform_calendar_features(sales_test_daily)\n",
    "sales_test_daily = integrate_external_features(sales_test_daily)\n",
    "sales_test_daily = sales_test_daily.assign(\n",
    "    num_sold = None,\n",
    "    num_sold_log = None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tried lower-clipping to historical floor -- public LB suffered!\n",
    "OUTCOME_HISTORICAL_MIN = 5\n",
    "\n",
    "segments_X_test = {grp: df for grp, df in sales_test_daily.groupby('series_id')}\n",
    "\n",
    "segments_predictions_test = []\n",
    "novel_series_count = 0\n",
    "for grp, df in segments_X_test.items():\n",
    "\n",
    "    if grp in SEGMENTS_MODELS:\n",
    "        df = df.assign(yhat = lambda df_: np.exp(SEGMENTS_MODELS[grp].predict(df_)))  \n",
    "        \n",
    "    else:\n",
    "        df = df.assign(yhat = lambda df_: np.exp(MODEL_GLOBAL.predict(df_)))\n",
    "        novel_series_count += 1\n",
    "\n",
    "    segments_predictions_test.append(df)\n",
    "\n",
    "predictions_test = pd.concat(segments_predictions_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test_submit = (\n",
    "    predictions_test\n",
    "    [['id', 'yhat']]\n",
    "    .rename(columns={'yhat': 'num_sold'})\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert predictions_test_submit.shape[0] == 98_550\n",
    "assert predictions_test_submit.notnull().all().all()\n",
    "predictions_test_submit.to_csv(\"./data/processed/submission_features_selected_cv_stepwise.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differential Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_compare = (\n",
    "    pd.read_csv(\"./data/processed/submission_retry_features_select_trend_periodicity.csv\")\n",
    "    .rename(columns={'num_sold': 'num_sold_prev'})\n",
    "    )\n",
    "predictions_compare = (\n",
    "    pd.merge(predictions_compare, predictions_test_submit, how='left')\n",
    "    .assign(diff = lambda df_: 100 * (df_['num_sold'] / df_['num_sold_prev'] - 1))\n",
    "    )\n",
    "assert predictions_compare.shape[0] == 98_550\n",
    "\n",
    "predictions_compare = pd.merge(\n",
    "    predictions_compare, \n",
    "    sales_test_daily[['id', 'country', 'product', 'store', 'year', 'series_id']]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_compare['diff'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_compare.groupby('country')['diff'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_compare.query(\"year == 2017\").groupby('country')['diff'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_compare.groupby('year')['diff'].describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forecast_stickers_kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
