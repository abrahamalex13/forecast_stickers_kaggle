{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import requests\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import optuna\n",
    "import xgboost as xgb\n",
    "from dateutil.easter import easter\n",
    "from datetime import timedelta\n",
    "import plotnine as p9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily = (\n",
    "    pd.read_csv(\"./data/external/train.csv\")\n",
    "    .assign(\n",
    "\n",
    "        date = lambda df_: pd.to_datetime(df_['date']),\n",
    "\n",
    "        country_store = lambda df_: df_['country'].str.cat(df_['store'], sep='|'),\n",
    "        country_product = lambda df_: df_['country'].str.cat(df_['product'], sep='|'),\n",
    "        store_product = lambda df_: df_['store'].str.cat(df_['product'], sep='|'),\n",
    "        country_store_product = lambda df_: df_['country'].str.cat([df_['store'], df_['product']], sep='|')\n",
    "\n",
    "    )\n",
    "    .assign(series_id = lambda df_: df_['country_store_product'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_gdp_per_capita(country_code, year):\n",
    "    \"\"\"\n",
    "    Adapted from https://www.kaggle.com/competitions/playground-series-s5e1/discussion/554349.\n",
    "    \"\"\"\n",
    "\n",
    "    url='https://api.worldbank.org/v2/country/{0}/indicator/NY.GDP.PCAP.CD?date={1}&format=json'\n",
    "    response = requests.get(url.format(country_code, year)).json()\n",
    "\n",
    "    return response[1][0]['value']\n",
    "\n",
    "# per CountryCode-year: request GDP per capita.\n",
    "# concatenate dataframe of CountryCode | Country | Year | GDP, for integration to Kaggle source\n",
    "\n",
    "countries_code_map = {\n",
    "    'Canada': 'CAN', \n",
    "    'Finland': 'FIN',\n",
    "    'Italy': 'ITA',\n",
    "    'Kenya': 'KEN',\n",
    "    'Norway': 'NOR',\n",
    "    'Singapore': 'SGP'\n",
    "    }\n",
    "\n",
    "countries_gdp_yearly = []\n",
    "for country_title, country_code in countries_code_map.items():\n",
    "    \n",
    "    values_yearly = [\n",
    "        {'year': i, 'gdp_per_capita': extract_gdp_per_capita(country_code, i)}\n",
    "        for i in range(2010, 2019+1)\n",
    "        ]\n",
    "    values_yearly = [pd.DataFrame(x, index=[0]) for x in values_yearly]\n",
    "    values_yearly = pd.concat(values_yearly, axis=0)\n",
    "    \n",
    "    values_yearly = values_yearly.assign(\n",
    "        country = country_title,\n",
    "        country_code = country_code\n",
    "        )\n",
    "    \n",
    "    countries_gdp_yearly.append(values_yearly)\n",
    "\n",
    "    print(f\"{country_title} ({country_code}) GDP Per Capita extraction complete.\")\n",
    "\n",
    "countries_gdp_yearly = pd.concat(countries_gdp_yearly, axis=0)\n",
    "\n",
    "countries_gdp_yearly = countries_gdp_yearly.assign(\n",
    "    gdp_per_capita_log = lambda df_: np.log(df_['gdp_per_capita'])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    countries_gdp_yearly\n",
    "    .set_index('year')\n",
    "    .groupby('country')\n",
    "    ['gdp_per_capita']\n",
    "    .plot(legend=True)\n",
    ")\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    countries_gdp_yearly\n",
    "    .query(\"country == 'Kenya'\")\n",
    "    .set_index('year')\n",
    "    .groupby('country')\n",
    "    ['gdp_per_capita']\n",
    "    .plot(legend=True)\n",
    ")\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: is there a better practice for holidays feature representation?\n",
    "    # in Hyndman's Electricity Load Forecasting Kaggle (https://robjhyndman.com/papers/kaggle-competition.pdf),\n",
    "    # \"holiday effect modelled with a factor variable, taking value zero on a non-work day,\n",
    "    # some non-zero value day before a non-work day, and a different value the day after a non-work day.\"\n",
    "    # meaning -- holiday days pooled, before & after estimated separately?\n",
    "\n",
    "days_easter0 = [easter(x) for x in range(2010, 2019+1)]\n",
    "days_easter = pd.DataFrame({'date': days_easter0}).assign(is_easter = 1)\n",
    "\n",
    "# motivated by exploratory analysis of model errors.\n",
    "# appears that errors concentrate on days shortly after Easter\n",
    "dfs_days_special_relative_easter = [days_easter]\n",
    "for delta_days in [2, 3, 4, 5, 6, 7]:\n",
    "\n",
    "    df_special = (\n",
    "        days_easter\n",
    "        .copy()\n",
    "        .assign(date = lambda df_: df_['date'] + timedelta(days=delta_days))\n",
    "        .rename(columns={'is_easter': f'is_easter_plus{delta_days}'})\n",
    "        )\n",
    "    dfs_days_special_relative_easter.append(df_special)\n",
    "\n",
    "days_special_relative_easter = (\n",
    "    pd.concat(dfs_days_special_relative_easter, axis=0)\n",
    "    .fillna(0)\n",
    "    .assign(date = lambda df_: pd.to_datetime(df_['date']))\n",
    "    )\n",
    "\n",
    "assert days_special_relative_easter['date'].is_unique\n",
    "\n",
    "FEATURES_EASTER = [x for x in days_special_relative_easter.columns if 'easter' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_calendar_features(df):\n",
    "\n",
    "    df = (\n",
    "        df\n",
    "        .assign(\n",
    "            year = lambda df_: df_['date'].dt.year,\n",
    "            month = lambda df_: df_['date'].dt.month,\n",
    "            week_of_year = lambda df_: df_['date'].dt.isocalendar().week,\n",
    "            day_of_week = lambda df_: df_['date'].dt.day_name(),\n",
    "            # President's Day is the 'third Monday in February'\n",
    "            day_of_month = lambda df_: df_['date'].dt.day,\n",
    "            day_of_year = lambda df_: df_['date'].dt.dayofyear,\n",
    "            # week of month would be ambiguous because, one week may span 2 months,\n",
    "            days_since_start = lambda df_: (df['date'] - pd.to_datetime(\"2010-01-01\")).dt.days\n",
    "            )\n",
    "        .assign(\n",
    "            # TODO: are periodic feature transforms too rigid?\n",
    "\n",
    "            # as day_of_year rises, don't expect monotonic relationship with outcome.\n",
    "            # rather, expect periodic (sinusoidal) relationship.\n",
    "            # as sin(x) rises, so too does outcome ...\n",
    "            # ensure one cycle over one year.\n",
    "            # at baseline, one sinusoidal cycle occurs per 2Ï€\n",
    "            day_of_year_sin = lambda df_: np.sin(df_['day_of_year'] * 2 * np.pi / 365),\n",
    "            day_of_year_cos = lambda df_: np.cos(df_['day_of_year'] * 2 * np.pi / 365),\n",
    "\n",
    "            day_of_month_sin = lambda df_: np.sin(df_['day_of_month'] * 2 * np.pi / 30),\n",
    "            day_of_month_cos = lambda df_: np.cos(df_['day_of_month'] * 2 * np.pi / 30),\n",
    "\n",
    "            # exploratory visuals suggest ~2-year cycles\n",
    "            days_since_start_macro_sin = lambda df_: np.sin(df_['days_since_start'] * 2 * np.pi / 730),\n",
    "            days_since_start_macro_cos = lambda df_: np.cos(df_['days_since_start'] * 2 * np.pi / 730),\n",
    "            )\n",
    "        .assign(\n",
    "            is_yearend = lambda df_: (\n",
    "                (df_['month'] == 12) & (df_['day_of_month'].isin([28, 29, 30, 31]))\n",
    "                ).astype(int)\n",
    "            )\n",
    "\n",
    "        )\n",
    "    \n",
    "    df = pd.merge(df, days_special_relative_easter, how='left')\n",
    "    assert df['is_easter'].notnull().any()\n",
    "    df[FEATURES_EASTER] = df[FEATURES_EASTER].fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def integrate_external_features(df):\n",
    "\n",
    "    df = pd.merge(df, countries_gdp_yearly, how='left')\n",
    "    assert df['gdp_per_capita'].notnull().all().all()\n",
    "\n",
    "    return df\n",
    "\n",
    "def transform_lagged_predictors(df):\n",
    "\n",
    "    # to ensure proper within-series outcome lags\n",
    "    # TODO: with lagged features coming into play, how to enforce proper order via indexes?\n",
    "    df = (\n",
    "        df\n",
    "        .sort_values(['series_id', 'date'])\n",
    "        .assign(\n",
    "            num_sold_lag1 = lambda df_: df_.groupby('series_id')['num_sold'].shift(1),\n",
    "            num_sold_lag7 = lambda df_: df_.groupby('series_id')['num_sold'].shift(7)\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    return df\n",
    "\n",
    "def transform_logs(df):\n",
    "\n",
    "    df = (\n",
    "        df\n",
    "        .assign(\n",
    "            num_sold_log = lambda df_: np.log(df_['num_sold']),\n",
    "            num_sold_lag1_log = lambda df_: np.log(df_['num_sold_lag1']),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "sales_daily = transform_calendar_features(sales_daily)\n",
    "sales_daily = integrate_external_features(sales_daily)\n",
    "sales_daily = transform_lagged_predictors(sales_daily)\n",
    "sales_daily = transform_logs(sales_daily)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description Report: \"Surface Properties\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily['id'].is_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Volumetric Analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily['series_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily['date'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily['series_id'].value_counts().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily['product'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily['country'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily['store'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fields' Types and Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily['date'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily['num_sold'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily.isnull().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# are null sales events concentrated on a particular date?\n",
    "# doesn't appear so\n",
    "sales_daily.query(\"num_sold.isnull()\")['date'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily.query(\"num_sold.isnull()\")['series_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily.groupby('series_id')['num_sold'].sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a naive model: series' historical average sales, daily\n",
    "\n",
    "is_training = (\n",
    "    (sales_daily['date'] >= pd.to_datetime('2010-01-01'))\n",
    "    & (sales_daily['date'] < pd.to_datetime(\"2014-01-01\"))\n",
    ")\n",
    "\n",
    "is_validation = sales_daily['date'] >= pd.to_datetime(\"2014-01-01\")\n",
    "\n",
    "# is_training.sum(), is_validation.sum()\n",
    "\n",
    "# with original train data: using strictly train segment, groupby average\n",
    "predictions_naive = (\n",
    "    sales_daily\n",
    "    .loc[is_training]\n",
    "    .groupby('series_id')\n",
    "    [['num_sold']]\n",
    "    .agg('mean')\n",
    "    .reset_index(drop=False)\n",
    ")\n",
    "\n",
    "predictions_naive_evaluate = pd.merge(\n",
    "    sales_daily.loc[is_validation],\n",
    "    predictions_naive.rename(columns={'num_sold': 'yhat'}),\n",
    "    how='left'\n",
    "    )\n",
    "\n",
    "# expect a couple series with all null\n",
    "predictions_naive.isnull().sum()\n",
    "# recommended in this discussion: https://www.kaggle.com/competitions/playground-series-s5e1/discussion/554553 \n",
    "predictions_naive_evaluate = predictions_naive_evaluate.dropna()\n",
    "\n",
    "mean_absolute_percentage_error(\n",
    "    predictions_naive_evaluate['num_sold'],\n",
    "    predictions_naive_evaluate['yhat']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_naive_evaluate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a naive model: series' historical average sales, daily\n",
    "\n",
    "is_training = (\n",
    "    (sales_daily['date'] >= pd.to_datetime('2013-01-01'))\n",
    "    & (sales_daily['date'] < pd.to_datetime(\"2014-01-01\"))\n",
    ")\n",
    "\n",
    "is_validation = sales_daily['date'] >= pd.to_datetime(\"2014-01-01\")\n",
    "\n",
    "# is_training.sum(), is_validation.sum()\n",
    "\n",
    "# with original train data: using strictly train segment, groupby average\n",
    "predictions_naive = (\n",
    "    sales_daily\n",
    "    .loc[is_training]\n",
    "    .groupby('series_id')\n",
    "    [['num_sold']]\n",
    "    .agg('mean')\n",
    "    .reset_index(drop=False)\n",
    ")\n",
    "\n",
    "predictions_naive_evaluate = pd.merge(\n",
    "    sales_daily.loc[is_validation],\n",
    "    predictions_naive.rename(columns={'num_sold': 'yhat'}),\n",
    "    how='left'\n",
    "    )\n",
    "\n",
    "# expect a couple series with all null\n",
    "predictions_naive.isnull().sum()\n",
    "# recommended in this discussion: https://www.kaggle.com/competitions/playground-series-s5e1/discussion/554553 \n",
    "predictions_naive_evaluate = predictions_naive_evaluate.dropna()\n",
    "\n",
    "mean_absolute_percentage_error(\n",
    "    predictions_naive_evaluate['num_sold'],\n",
    "    predictions_naive_evaluate['yhat']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_sample_daily = sales_daily.query(\"series_id == 'Norway|Premium Sticker Mart|Kaggle'\")\n",
    "\n",
    "(\n",
    "    sales_sample_daily\n",
    "    .loc[is_training]\n",
    "    [['date', 'num_sold']]\n",
    "    .set_index('date')\n",
    "    .plot\n",
    "    .line()\n",
    ")\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# descend level of abstraction -- \n",
    "\n",
    "    # across-year patterns (multi-year business cycles)\n",
    "    # within-year,\n",
    "        # month-of-year seasonality\n",
    "        # week-of-year seasonality\n",
    "        # day-of-month seasonality\n",
    "        # day-of-week seasonality\n",
    "\n",
    "# outcome vs predictors\n",
    "    # lagged outcome\n",
    "    # gdp\n",
    "    # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    p9.ggplot(sales_daily.groupby(['date'])[['num_sold']].agg('sum').reset_index(drop=False)) + \n",
    "    p9.theme_bw() + \n",
    "    p9.geom_point(p9.aes('date', 'num_sold'), alpha=0.25)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    p9.ggplot(sales_daily) + \n",
    "    p9.theme_bw() + \n",
    "    p9.geom_boxplot(p9.aes('month', 'num_sold', group='month'), alpha=0.1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    p9.ggplot(sales_daily.query(\"series_id == 'Norway|Premium Sticker Mart|Kaggle'\")) + \n",
    "    p9.theme_bw() + \n",
    "    p9.geom_boxplot(p9.aes('month', 'num_sold', group='month'), alpha=0.1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    p9.ggplot((\n",
    "        sales_daily\n",
    "        .assign(day_of_week = lambda df_: pd.Categorical(\n",
    "            df_['day_of_week'], \n",
    "            ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']\n",
    "            ))\n",
    "    )) + \n",
    "    p9.theme_bw() + \n",
    "    p9.geom_boxplot(p9.aes('day_of_week', 'num_sold', group='day_of_week'), alpha=0.1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    p9.ggplot(sales_daily.query(\"series_id == 'Norway|Premium Sticker Mart|Kaggle'\")) + \n",
    "    p9.theme_bw() + \n",
    "    p9.geom_point(p9.aes('num_sold_lag1', 'num_sold'), alpha=0.1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    p9.ggplot(sales_daily.query(\"series_id == 'Norway|Premium Sticker Mart|Kaggle'\")) + \n",
    "    p9.theme_bw() + \n",
    "    p9.geom_point(p9.aes('num_sold_lag7', 'num_sold'), alpha=0.1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features set varies by model -- _global_ and _local_. \n",
    "\n",
    "\"Universe\" implies, all features that could be used. A subset enters into each \"building block\" of the overall modeling system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_UNIVERSE_TO_ONEHOT = [\n",
    "    'country', \n",
    "    'store',\n",
    "    'product',\n",
    "    'country_store',\n",
    "    'country_product',\n",
    "    'store_product',\n",
    "    'country_store_product',\n",
    "    # year attempted, but then omitted. Because \n",
    "    # exogenous factors should help explain year-to-year shifts,\n",
    "    # so that out-of-sample years' forecasts aren't flat\n",
    "    'month', \n",
    "    'week_of_year', \n",
    "    'day_of_week'\n",
    "    ]\n",
    "\n",
    "# local model fits by country-store-product segment, \n",
    "# so those onehots would be invariant\n",
    "FEATURES_LOCAL_MODEL_TO_ONEHOT = [\n",
    "    x for x in FEATURES_UNIVERSE_TO_ONEHOT \n",
    "    if not any(stem in x for stem in ['country', 'store', 'product'])\n",
    "    ]\n",
    "\n",
    "FEATURES_UNIVERSE_NUMERIC_CONTINUOUS = [\n",
    "    'gdp_per_capita_log',\n",
    "    'day_of_month', \n",
    "    'day_of_month_sin',\n",
    "    'day_of_month_cos',\n",
    "    'day_of_year_sin',\n",
    "    'day_of_year_cos',\n",
    "    'day_of_year',\n",
    "    'days_since_start_macro_sin',\n",
    "    'days_since_start_macro_cos',\n",
    "    'days_since_start'\n",
    "    ]\n",
    "\n",
    "FEATURES_UNIVERSE_ALREADY_ONEHOT = ['is_yearend'] + FEATURES_EASTER\n",
    "\n",
    "FEATURES_AUTOREGRESSIVE = ['num_sold_lag1_log']\n",
    "\n",
    "FEATURES_GLOBAL_MODEL = (\n",
    "    FEATURES_UNIVERSE_TO_ONEHOT + \n",
    "    FEATURES_UNIVERSE_NUMERIC_CONTINUOUS + \n",
    "    FEATURES_UNIVERSE_ALREADY_ONEHOT\n",
    "    )\n",
    "\n",
    "FEATURES_LOCAL_MODEL = (\n",
    "    FEATURES_LOCAL_MODEL_TO_ONEHOT + \n",
    "    FEATURES_UNIVERSE_NUMERIC_CONTINUOUS + \n",
    "    FEATURES_UNIVERSE_ALREADY_ONEHOT\n",
    "    )\n",
    "\n",
    "FEATURES_LOCAL_REMAINDER_MODEL = list( set(FEATURES_LOCAL_MODEL).intersection(set(FEATURES_GLOBAL_MODEL)) )\n",
    "FEATURES_LOCAL_REMAINDER_MODEL.remove('gdp_per_capita_log')\n",
    "\n",
    "ATTRIBUTES = ['series_id', 'date', 'id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily_complete = sales_daily.dropna(subset=['num_sold', 'num_sold_lag1_log'])\n",
    "\n",
    "# shorter alias\n",
    "XY = sales_daily_complete\n",
    "\n",
    "# from previous retail forecasting competitions' leaders,\n",
    "# plus theoretically expected heterogeneity between series: \n",
    "# one model per segment\n",
    "\n",
    "segments_XY = {grp: df for grp, df in XY.groupby('series_id')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrendRemainderModelPipeline:\n",
    "    def __init__(self, trend_model_ridge_alpha):\n",
    "\n",
    "        self.trend_level_model_ridge_alpha = trend_model_ridge_alpha\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        self.fit_trend_level_model(X, y)\n",
    "        yhat_trend_level = self.predict_trend_level_model(X)\n",
    "        y_detrended = y - yhat_trend_level\n",
    "\n",
    "        self.fit_remainder_model(X, y_detrended)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        yhat_trend_level = self.predict_trend_level_model(X)\n",
    "        yhat_remainder = self.predict_remainder_model(X)\n",
    "        preds = yhat_trend_level + yhat_remainder\n",
    "\n",
    "        return preds\n",
    "\n",
    "    def fit_trend_level_model(self, X, y):\n",
    "        \"\"\"\n",
    "        'Weak learner', strictly intended to explain trend (level) shifts between years.\n",
    "        \"\"\"\n",
    "\n",
    "        self.trend_level_feature_transformer = ColumnTransformer([\n",
    "            ('transformer_onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), FEATURES_UNIVERSE_TO_ONEHOT),\n",
    "            ('transformer_std', StandardScaler(), ['gdp_per_capita_log'])\n",
    "            ],\n",
    "            verbose_feature_names_out=False,\n",
    "            remainder='drop'\n",
    "            )\n",
    "        X = self.trend_level_feature_transformer.fit_transform(X)\n",
    "\n",
    "        model_trend_level = Ridge(self.trend_level_model_ridge_alpha)\n",
    "        model_trend_level.fit(X, y)\n",
    "\n",
    "        self.trend_level_model = model_trend_level\n",
    "\n",
    "    def fit_remainder_model(self, X, y):\n",
    "\n",
    "        self.remainder_feature_transformer = ColumnTransformer([\n",
    "            ('transformer_onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), FEATURES_UNIVERSE_TO_ONEHOT),\n",
    "            ('transformer_std', StandardScaler(), FEATURES_UNIVERSE_NUMERIC_CONTINUOUS)\n",
    "            ],\n",
    "            verbose_feature_names_out=False,\n",
    "            remainder='passthrough'\n",
    "            )\n",
    "        X = self.remainder_feature_transformer.fit_transform(X)\n",
    "\n",
    "        model_remainder = RandomForestRegressor(n_estimators=100, n_jobs=-1)\n",
    "        model_remainder.fit(X, y)\n",
    "\n",
    "        self.remainder_model = model_remainder\n",
    "\n",
    "    def predict_trend_level_model(self, X):\n",
    "        X = self.trend_level_feature_transformer.transform(X)\n",
    "        preds = self.trend_level_model.predict(X) \n",
    "        return preds\n",
    "    \n",
    "    def predict_remainder_model(self, X):\n",
    "        X = self.remainder_feature_transformer.transform(X)\n",
    "        preds = self.remainder_model.predict(X)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfolds = [\n",
    "\n",
    "    # validation set 2014-16 matches ultimate test set's length, 2017-19\n",
    "    ( \n",
    "        ( (XY['date'] >= pd.to_datetime('2010-01-01')) & (XY['date'] < pd.to_datetime(\"2014-01-01\")) ),\n",
    "        XY['date'] >= pd.to_datetime(\"2014-01-01\")\n",
    "    ),\n",
    "\n",
    "    ( \n",
    "        ( (XY['date'] >= pd.to_datetime('2010-01-01')) & (XY['date'] < pd.to_datetime(\"2016-01-01\")) ),\n",
    "        XY['date'] >= pd.to_datetime(\"2016-01-01\")\n",
    "    )\n",
    "\n",
    "    ]\n",
    "\n",
    "kfolds_evaluation = []\n",
    "for is_training, is_validation in kfolds:\n",
    "\n",
    "    feature_transform_pipeline_global_model = ColumnTransformer([\n",
    "        ('transformer_onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), FEATURES_UNIVERSE_TO_ONEHOT),\n",
    "        ('transformer_std', StandardScaler(), FEATURES_UNIVERSE_NUMERIC_CONTINUOUS)\n",
    "        ],\n",
    "        verbose_feature_names_out=False,\n",
    "        # caller expected to provide some features that require no transforms.\n",
    "        # don't drop those just because they received no special operations.\n",
    "        # but do be sure to drop non-features in advance!\n",
    "        remainder='passthrough'\n",
    "        )\n",
    "    feature_transform_pipeline_global_model.set_output(transform='pandas')\n",
    "    \n",
    "    pipeline_e2e = Pipeline([\n",
    "        ('transform_features', feature_transform_pipeline_global_model), \n",
    "        ('model', RandomForestRegressor(n_estimators=100, n_jobs=-1))\n",
    "        ])\n",
    "\n",
    "    pipeline_e2e.fit(\n",
    "        XY.loc[is_training, FEATURES_GLOBAL_MODEL],\n",
    "        XY.loc[is_training, 'num_sold_log']\n",
    "        )\n",
    "    \n",
    "    predictions = (\n",
    "        XY\n",
    "        .copy()\n",
    "        .assign(yhat = lambda df_: np.exp(pipeline_e2e.predict(df_[FEATURES_GLOBAL_MODEL])))\n",
    "        )\n",
    "    \n",
    "    scores = {\n",
    "        'validation': {\n",
    "            'nobs': predictions.loc[is_validation].shape[0],\n",
    "            'score': mean_absolute_percentage_error( \n",
    "                predictions.loc[is_validation, 'num_sold'],\n",
    "                predictions.loc[is_validation, 'yhat']\n",
    "                )\n",
    "            },\n",
    "        'train': {\n",
    "            'nobs': predictions.loc[is_training].shape[0],\n",
    "            'score': mean_absolute_percentage_error( \n",
    "                predictions.loc[is_training, 'num_sold'],\n",
    "                predictions.loc[is_training, 'yhat']\n",
    "                )\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    kfolds_evaluation.append(scores)\n",
    "\n",
    "kfolds_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_sample = predictions.query(\"series_id == 'Norway|Stickers for Less|Kaggle'\")\n",
    "\n",
    "predictions_sample.set_index('date')[['num_sold', 'yhat']].plot.line(alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.inspection import permutation_importance\n",
    "\n",
    "# result = permutation_importance(\n",
    "#     model_global, \n",
    "#     XY.loc[is_validation, FEATURES_GLOBAL_MODEL],\n",
    "#     XY.loc[is_validation, 'num_sold_log'], \n",
    "#     n_repeats=5, \n",
    "#     random_state=777, \n",
    "#     n_jobs=-1\n",
    "# )\n",
    "# importances = pd.Series(result.importances_mean, index=FEATURES_GLOBAL_MODEL)\n",
    "\n",
    "# importances = pd.Series(model_global.feature_importances_, index=FEATURES_GLOBAL_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfolds = [\n",
    "\n",
    "    # validation set 2014-16 matches ultimate test set's length, 2017-19\n",
    "    ( \n",
    "        ( (XY['date'] >= pd.to_datetime('2010-01-01')) & (XY['date'] < pd.to_datetime(\"2014-01-01\")) ),\n",
    "        XY['date'] >= pd.to_datetime(\"2014-01-01\")\n",
    "    ),\n",
    "\n",
    "    ( \n",
    "        ( (XY['date'] >= pd.to_datetime('2010-01-01')) & (XY['date'] < pd.to_datetime(\"2016-01-01\")) ),\n",
    "        XY['date'] >= pd.to_datetime(\"2016-01-01\")\n",
    "    )\n",
    "\n",
    "    ]\n",
    "\n",
    "kfolds_evaluation = []\n",
    "for is_training, is_validation in kfolds:\n",
    "\n",
    "    feature_transform_pipeline_global_model = ColumnTransformer([\n",
    "        ('transformer_onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), FEATURES_UNIVERSE_TO_ONEHOT),\n",
    "        ('transformer_std', StandardScaler(), FEATURES_UNIVERSE_NUMERIC_CONTINUOUS)\n",
    "        ],\n",
    "        verbose_feature_names_out=False,\n",
    "        remainder='passthrough'\n",
    "        )\n",
    "    feature_transform_pipeline_global_model.set_output(transform='pandas')\n",
    "\n",
    "    pipeline_e2e = Pipeline([\n",
    "        ('transform_features', feature_transform_pipeline_global_model), \n",
    "        ('model', Ridge(1e-2))\n",
    "        ])\n",
    "\n",
    "    pipeline_e2e.fit(\n",
    "        XY.loc[is_training, FEATURES_GLOBAL_MODEL],\n",
    "        XY.loc[is_training, 'num_sold_log']\n",
    "        )\n",
    "    \n",
    "    predictions = (\n",
    "        XY\n",
    "        .copy()\n",
    "        .assign(yhat = lambda df_: np.exp(pipeline_e2e.predict(df_[FEATURES_GLOBAL_MODEL])))\n",
    "        )\n",
    "    \n",
    "    scores = {\n",
    "        'validation': {\n",
    "            'nobs': predictions.loc[is_validation].shape[0],\n",
    "            'score': mean_absolute_percentage_error( \n",
    "                predictions.loc[is_validation, 'num_sold'],\n",
    "                predictions.loc[is_validation, 'yhat']\n",
    "                )\n",
    "            },\n",
    "        'train': {\n",
    "            'nobs': predictions.loc[is_training].shape[0],\n",
    "            'score': mean_absolute_percentage_error( \n",
    "                predictions.loc[is_training, 'num_sold'],\n",
    "                predictions.loc[is_training, 'yhat']\n",
    "                )\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    kfolds_evaluation.append(scores)\n",
    "\n",
    "kfolds_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_sample = predictions.query(\"series_id == 'Norway|Stickers for Less|Kaggle'\")\n",
    "# predictions_sample = predictions.query(\"series_id == 'Norway|Premium Sticker Mart|Kaggle'\")\n",
    "\n",
    "predictions_sample.set_index('date')[['num_sold', 'yhat']].plot.line(alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfolds = [\n",
    "\n",
    "    # validation set 2014-16 matches ultimate test set's length, 2017-19\n",
    "    ( \n",
    "        ( (XY['date'] >= pd.to_datetime('2010-01-01')) & (XY['date'] < pd.to_datetime(\"2014-01-01\")) ),\n",
    "        XY['date'] >= pd.to_datetime(\"2014-01-01\")\n",
    "    ),\n",
    "\n",
    "    ( \n",
    "        ( (XY['date'] >= pd.to_datetime('2010-01-01')) & (XY['date'] < pd.to_datetime(\"2016-01-01\")) ),\n",
    "        XY['date'] >= pd.to_datetime(\"2016-01-01\")\n",
    "    )\n",
    "\n",
    "    ]\n",
    "\n",
    "kfolds_evaluation = []\n",
    "for is_training, is_validation in kfolds:\n",
    "\n",
    "    model_global_pipeline = TrendRemainderModelPipeline(trend_model_ridge_alpha=1e-2)\n",
    "    model_global_pipeline.fit(\n",
    "        XY.loc[is_training, FEATURES_GLOBAL_MODEL],\n",
    "        XY.loc[is_training, 'num_sold_log']\n",
    "        )\n",
    "    \n",
    "    predictions = XY.copy()\n",
    "    predictions = (\n",
    "        predictions\n",
    "        .assign(yhat_log = lambda df_: model_global_pipeline.predict(df_))\n",
    "        .assign(yhat = lambda df_: np.exp(df_['yhat_log']) )\n",
    "        )\n",
    "    \n",
    "    scores = {\n",
    "        'validation': {\n",
    "            'nobs': predictions.loc[is_validation].shape[0],\n",
    "            'score': mean_absolute_percentage_error( \n",
    "                predictions.loc[is_validation, 'num_sold'],\n",
    "                predictions.loc[is_validation, 'yhat']\n",
    "                )\n",
    "            },\n",
    "        'train': {\n",
    "            'nobs': predictions.loc[is_training].shape[0],\n",
    "            'score': mean_absolute_percentage_error( \n",
    "                predictions.loc[is_training, 'num_sold'],\n",
    "                predictions.loc[is_training, 'yhat']\n",
    "                )\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    kfolds_evaluation.append(scores)\n",
    "\n",
    "kfolds_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_sample = predictions.query(\"series_id == 'Norway|Stickers for Less|Kaggle'\")\n",
    "# predictions_sample = predictions.query(\"series_id == 'Norway|Premium Sticker Mart|Kaggle'\")\n",
    "\n",
    "predictions_sample.set_index('date')[['num_sold', 'yhat']].plot.line(alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def objective(trial):\n",
    "\n",
    "#     ROUNDS_COUNT = 500\n",
    "\n",
    "#     param = {\n",
    "#         \"objective\": \"reg:squarederror\",\n",
    "#         \"booster\": trial.suggest_categorical(\n",
    "#             \"booster\", [\"gbtree\", \"gblinear\", \"dart\"]\n",
    "#         ),\n",
    "#         \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "#         \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "#         \"subsample\": trial.suggest_float(\"subsample\", 0.4, 1.0),\n",
    "#         \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n",
    "#         'device': 'cuda',\n",
    "#         'tree_method': 'hist'\n",
    "#     }\n",
    "\n",
    "#     if param[\"booster\"] in [\"gbtree\", \"dart\"]:\n",
    "#         param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 1, 9, step=2)\n",
    "\n",
    "#         param[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 1, 10)\n",
    "#         param[\"eta\"] = trial.suggest_float(\"eta\", 1e-5, 0.01, log=True)\n",
    "\n",
    "#         param[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
    "#         param[\"grow_policy\"] = trial.suggest_categorical(\n",
    "#             \"grow_policy\", [\"depthwise\", \"lossguide\"]\n",
    "#         )\n",
    "\n",
    "#     if param[\"booster\"] == \"dart\":\n",
    "#         param[\"sample_type\"] = trial.suggest_categorical(\n",
    "#             \"sample_type\", [\"uniform\", \"weighted\"]\n",
    "#         )\n",
    "#         param[\"normalize_type\"] = trial.suggest_categorical(\n",
    "#             \"normalize_type\", [\"tree\", \"forest\"]\n",
    "#         )\n",
    "#         param[\"rate_drop\"] = trial.suggest_float(\n",
    "#             \"rate_drop\", 1e-8, 1.0, log=True\n",
    "#         )\n",
    "#         param[\"skip_drop\"] = trial.suggest_float(\n",
    "#             \"skip_drop\", 1e-8, 1.0, log=True\n",
    "#         )\n",
    "\n",
    "\n",
    "#     kfolds_evaluation = []\n",
    "#     for is_training, is_validation in kfolds:\n",
    "\n",
    "#         model_global = Ridge(1e-2)\n",
    "#         model_global.fit(\n",
    "#             XY.loc[is_training, FEATURES_GLOBAL_TREND_LEVEL_MODEL],\n",
    "#             XY.loc[is_training, 'num_sold_log']\n",
    "#             )\n",
    "        \n",
    "#         predictions = (\n",
    "#             XY\n",
    "#             .copy()\n",
    "#             .assign(yhat_trend_log = lambda df_: model_global.predict(df_[FEATURES_GLOBAL_TREND_LEVEL_MODEL]))\n",
    "#             .assign(num_sold_log_detrend = lambda df_: df_['num_sold_log'] - df_['yhat_trend_log'])\n",
    "#             )\n",
    "\n",
    "#         dtrain = xgb.DMatrix(\n",
    "#             predictions.loc[is_training, FEATURES_GLOBAL_MODEL], \n",
    "#             label=predictions.loc[is_training, 'num_sold_log_detrend']\n",
    "#             )\n",
    "#         dtest = xgb.DMatrix(predictions[FEATURES_GLOBAL_MODEL])\n",
    "\n",
    "#         model_global = xgb.train(param, dtrain, ROUNDS_COUNT)\n",
    "        \n",
    "#         yhat_remainder = model_global.predict(dtest)\n",
    "#         predictions = (\n",
    "#             predictions\n",
    "#             .assign(yhat_remainder = yhat_remainder)\n",
    "#             .assign(yhat = lambda df_: np.exp(df_['yhat_trend_log'] + df_['yhat_remainder']))\n",
    "#             )\n",
    "\n",
    "#         scores = {\n",
    "#             'validation': mean_absolute_percentage_error( \n",
    "#                 predictions.loc[is_validation, 'num_sold'],\n",
    "#                 predictions.loc[is_validation, 'yhat']\n",
    "#                 ),\n",
    "#             'train': mean_absolute_percentage_error( \n",
    "#                 predictions.loc[is_training, 'num_sold'],\n",
    "#                 predictions.loc[is_training, 'yhat']\n",
    "#                 )\n",
    "#             }\n",
    "        \n",
    "#         kfolds_evaluation.append(scores)\n",
    "\n",
    "#     score_overall = np.mean([ \n",
    "#         kfolds_evaluation[0]['validation'], \n",
    "#         kfolds_evaluation[1]['validation'] \n",
    "#         ])\n",
    "    \n",
    "#     return score_overall\n",
    "\n",
    "# study = optuna.create_study()\n",
    "\n",
    "# study.optimize(\n",
    "#     objective,\n",
    "#     n_trials=50,\n",
    "#     catch=(ValueError,),\n",
    "#     n_jobs=-1,\n",
    "#     timeout=12 * 60 * 60,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_GLOBAL = TrendRemainderModelPipeline(trend_model_ridge_alpha=1e-2)\n",
    "MODEL_GLOBAL.fit(XY[FEATURES_GLOBAL_MODEL], XY['num_sold_log'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# even when split into many dataframes, *index-based* subsets.\n",
    "# indexes maintained when dataframe splits.\n",
    "kfolds = [\n",
    "\n",
    "    # validation set 2014-16 matches ultimate test set's length, 2017-19\n",
    "    ( \n",
    "        ( (XY['date'] >= pd.to_datetime('2010-01-01')) & (XY['date'] < pd.to_datetime(\"2014-01-01\")) ),\n",
    "        XY['date'] >= pd.to_datetime(\"2014-01-01\")\n",
    "    ),\n",
    "\n",
    "    ( \n",
    "        ( (XY['date'] >= pd.to_datetime('2010-01-01')) & (XY['date'] < pd.to_datetime(\"2016-01-01\")) ),\n",
    "        XY['date'] >= pd.to_datetime(\"2016-01-01\")\n",
    "    )\n",
    "\n",
    "    ]\n",
    "\n",
    "kfolds_evaluation = []\n",
    "for is_training, is_validation in kfolds:\n",
    "\n",
    "    segments_models = {}\n",
    "    segments_predictions = []\n",
    "\n",
    "    for grp, XY_grp in segments_XY.items():\n",
    "\n",
    "        feature_transform_pipeline_local_model = ColumnTransformer([\n",
    "            ('transformer_onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), FEATURES_LOCAL_MODEL_TO_ONEHOT),\n",
    "            ('transformer_std', StandardScaler(), FEATURES_UNIVERSE_NUMERIC_CONTINUOUS)\n",
    "            ],\n",
    "            verbose_feature_names_out=False,\n",
    "            remainder='passthrough'\n",
    "            )\n",
    "        feature_transform_pipeline_local_model.set_output(transform='pandas')\n",
    "\n",
    "        pipeline_e2e = Pipeline([\n",
    "            ('transform_features', feature_transform_pipeline_local_model), \n",
    "            ('model', Ridge(1e-1))\n",
    "            ])\n",
    "\n",
    "        pipeline_e2e.fit(\n",
    "            XY_grp.loc[is_training, FEATURES_LOCAL_MODEL],\n",
    "            XY_grp.loc[is_training, 'num_sold_log']\n",
    "            )\n",
    "        \n",
    "        segments_models[grp] = pipeline_e2e\n",
    "\n",
    "        predictions = (\n",
    "            XY_grp\n",
    "            .copy()\n",
    "            .assign(yhat = lambda df_: np.exp(pipeline_e2e.predict(df_)))\n",
    "            )\n",
    "        segments_predictions.append(predictions)\n",
    "\n",
    "    predictions = pd.concat(segments_predictions, axis=0)\n",
    "\n",
    "    scores = {\n",
    "        'validation': {\n",
    "            'nobs': predictions.loc[is_validation].shape[0],\n",
    "            'score': mean_absolute_percentage_error( \n",
    "                predictions.loc[is_validation, 'num_sold'],\n",
    "                predictions.loc[is_validation, 'yhat']\n",
    "                )\n",
    "            },\n",
    "        'train': {\n",
    "            'nobs': predictions.loc[is_training].shape[0],\n",
    "            'score': mean_absolute_percentage_error( \n",
    "                predictions.loc[is_training, 'num_sold'],\n",
    "                predictions.loc[is_training, 'yhat']\n",
    "                )\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    kfolds_evaluation.append(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfolds_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_sample = predictions.query(\"series_id == 'Norway|Premium Sticker Mart|Kaggle'\")\n",
    "\n",
    "predictions_sample.set_index('date')[['num_sold', 'yhat']].plot.line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    parameters = {'alpha': trial.suggest_float(\"alpha\", 1e-1, 50, log=True)}\n",
    "\n",
    "    kfolds_evaluation = []\n",
    "    for is_training, is_validation in kfolds:\n",
    "\n",
    "        segments_models = {}\n",
    "        segments_predictions = []\n",
    "\n",
    "        for grp, XY_grp in segments_XY.items():\n",
    "\n",
    "            feature_transform_pipeline_local_model = ColumnTransformer([\n",
    "                ('transformer_onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), FEATURES_LOCAL_MODEL_TO_ONEHOT),\n",
    "                ('transformer_std', StandardScaler(), FEATURES_UNIVERSE_NUMERIC_CONTINUOUS)\n",
    "                ],\n",
    "                verbose_feature_names_out=False,\n",
    "                remainder='passthrough'\n",
    "                )\n",
    "            feature_transform_pipeline_local_model.set_output(transform='pandas')\n",
    "\n",
    "            pipeline_e2e = Pipeline([\n",
    "                ('transform_features', feature_transform_pipeline_local_model), \n",
    "                ('model', Ridge(**parameters))\n",
    "                ])\n",
    "\n",
    "            pipeline_e2e.fit(\n",
    "                XY_grp.loc[is_training, FEATURES_LOCAL_MODEL],\n",
    "                XY_grp.loc[is_training, 'num_sold_log']\n",
    "                )\n",
    "            \n",
    "            segments_models[grp] = pipeline_e2e\n",
    "\n",
    "            predictions = (\n",
    "                XY_grp\n",
    "                .copy()\n",
    "                .assign(yhat = lambda df_: np.exp(pipeline_e2e.predict(df_)))\n",
    "                )\n",
    "            segments_predictions.append(predictions)\n",
    "\n",
    "\n",
    "        predictions = pd.concat(segments_predictions, axis=0)\n",
    "\n",
    "        scores = {\n",
    "            'validation': mean_absolute_percentage_error( \n",
    "                predictions.loc[is_validation, 'num_sold'],\n",
    "                predictions.loc[is_validation, 'yhat']\n",
    "                ),\n",
    "            'train': mean_absolute_percentage_error( \n",
    "                predictions.loc[is_training, 'num_sold'],\n",
    "                predictions.loc[is_training, 'yhat']\n",
    "                )\n",
    "            }\n",
    "        \n",
    "        kfolds_evaluation.append(scores)\n",
    "\n",
    "    score_overall = np.mean([ \n",
    "        # kfolds_evaluation[0]['validation'], \n",
    "        kfolds_evaluation[1]['validation'] \n",
    "        ])\n",
    "    \n",
    "    return score_overall\n",
    "\n",
    "study = optuna.create_study()\n",
    "\n",
    "study.optimize(\n",
    "    objective,\n",
    "    n_trials=50,\n",
    "    catch=(ValueError,),\n",
    "    n_jobs=-1,\n",
    "    timeout=12 * 60 * 60,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_trial, study.best_params, study.best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INCOMPLETE: stalled after ~20 minute runtime, and score not better than 0.1\n",
    "\n",
    "# def objective(trial):\n",
    "\n",
    "#     ROUNDS_COUNT = 100\n",
    "\n",
    "#     param = {\n",
    "#         \"objective\": \"reg:squarederror\",\n",
    "#         \"booster\": trial.suggest_categorical(\n",
    "#             \"booster\", [\"gbtree\", \"gblinear\", \"dart\"]\n",
    "#         ),\n",
    "#         \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "#         \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "#         \"subsample\": trial.suggest_float(\"subsample\", 0.4, 1.0),\n",
    "#         \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n",
    "#     }\n",
    "\n",
    "#     if param[\"booster\"] in [\"gbtree\", \"dart\"]:\n",
    "#         param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 1, 9, step=2)\n",
    "\n",
    "#         param[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 1, 10)\n",
    "#         param[\"eta\"] = trial.suggest_float(\"eta\", 1e-5, 0.01, log=True)\n",
    "\n",
    "#         param[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
    "#         param[\"grow_policy\"] = trial.suggest_categorical(\n",
    "#             \"grow_policy\", [\"depthwise\", \"lossguide\"]\n",
    "#         )\n",
    "\n",
    "#     if param[\"booster\"] == \"dart\":\n",
    "#         param[\"sample_type\"] = trial.suggest_categorical(\n",
    "#             \"sample_type\", [\"uniform\", \"weighted\"]\n",
    "#         )\n",
    "#         param[\"normalize_type\"] = trial.suggest_categorical(\n",
    "#             \"normalize_type\", [\"tree\", \"forest\"]\n",
    "#         )\n",
    "#         param[\"rate_drop\"] = trial.suggest_float(\n",
    "#             \"rate_drop\", 1e-8, 1.0, log=True\n",
    "#         )\n",
    "#         param[\"skip_drop\"] = trial.suggest_float(\n",
    "#             \"skip_drop\", 1e-8, 1.0, log=True\n",
    "#         )\n",
    "\n",
    "#     kfolds_evaluation = []\n",
    "#     for is_training, is_validation in kfolds:\n",
    "\n",
    "#         segments_models = {}\n",
    "#         segments_predictions = []\n",
    "#         for grp, XY in segments_XY.items():\n",
    "\n",
    "#             dtrain = xgb.DMatrix(\n",
    "#                 XY.loc[is_training, FEATURES_LOCAL_MODEL], \n",
    "#                 label=XY.loc[is_training, 'num_sold_log']\n",
    "#                 )\n",
    "#             dtest = xgb.DMatrix(XY[FEATURES_LOCAL_MODEL])\n",
    "\n",
    "#             model_global = xgb.train(param, dtrain, ROUNDS_COUNT)\n",
    "#             segments_models[grp] = model_global\n",
    "        \n",
    "#             yhat = model_global.predict(dtest)\n",
    "#             predictions = (\n",
    "#                 XY\n",
    "#                 .copy()\n",
    "#                 .assign(yhat = lambda df_: np.exp(yhat))\n",
    "#                 )\n",
    "#             segments_predictions.append(predictions)\n",
    "\n",
    "#         predictions = pd.concat(segments_predictions, axis=0)\n",
    "\n",
    "#         scores = {\n",
    "#             'validation': mean_absolute_percentage_error( \n",
    "#                 predictions.loc[is_validation, 'num_sold'],\n",
    "#                 predictions.loc[is_validation, 'yhat']\n",
    "#                 ),\n",
    "#             'train': mean_absolute_percentage_error( \n",
    "#                 predictions.loc[is_training, 'num_sold'],\n",
    "#                 predictions.loc[is_training, 'yhat']\n",
    "#                 )\n",
    "#             }\n",
    "        \n",
    "#         kfolds_evaluation.append(scores)\n",
    "\n",
    "#     score_overall = np.mean([ \n",
    "#         kfolds_evaluation[0]['validation'], \n",
    "#         kfolds_evaluation[1]['validation'] \n",
    "#         ])\n",
    "    \n",
    "#     return score_overall\n",
    "\n",
    "# study = optuna.create_study()\n",
    "\n",
    "# study.optimize(\n",
    "#     objective,\n",
    "#     n_trials=25,\n",
    "#     catch=(ValueError,),\n",
    "#     n_jobs=-1,\n",
    "#     timeout=60 * 10,\n",
    "# )\n",
    "\n",
    "# study.best_trial, study.best_params, study.best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RIDGE_ALPHA_TUNED = study.best_params['alpha']\n",
    "\n",
    "# even when split into many dataframes, *index-based* subsets.\n",
    "# indexes maintained when dataframe splits.\n",
    "kfolds = [\n",
    "    \n",
    "    # validation set 2014-16 matches ultimate test set's length, 2017-19\n",
    "    ( \n",
    "        ( (XY['date'] >= pd.to_datetime('2010-01-01')) & (XY['date'] < pd.to_datetime(\"2014-01-01\")) ),\n",
    "        XY['date'] >= pd.to_datetime(\"2014-01-01\")\n",
    "    ),\n",
    "\n",
    "    ( \n",
    "        ( (XY['date'] >= pd.to_datetime('2010-01-01')) & (XY['date'] < pd.to_datetime(\"2016-01-01\")) ),\n",
    "        XY['date'] >= pd.to_datetime(\"2016-01-01\")\n",
    "    )\n",
    "\n",
    "    ]\n",
    "\n",
    "kfolds_evaluation = []\n",
    "for is_training, is_validation in kfolds:\n",
    "\n",
    "    segments_models = {}\n",
    "    segments_predictions = []\n",
    "\n",
    "    for grp, XY_grp in segments_XY.items():\n",
    "\n",
    "        feature_transform_pipeline_local_model = ColumnTransformer([\n",
    "            ('transformer_onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), FEATURES_LOCAL_MODEL_TO_ONEHOT),\n",
    "            ('transformer_std', StandardScaler(), FEATURES_UNIVERSE_NUMERIC_CONTINUOUS)\n",
    "            ],\n",
    "            verbose_feature_names_out=False,\n",
    "            remainder='passthrough'\n",
    "            )\n",
    "        feature_transform_pipeline_local_model.set_output(transform='pandas')\n",
    "\n",
    "        pipeline_e2e = Pipeline([\n",
    "            ('transform_features', feature_transform_pipeline_local_model), \n",
    "            ('model', Ridge(RIDGE_ALPHA_TUNED))\n",
    "            ])\n",
    "\n",
    "        pipeline_e2e.fit(\n",
    "            XY_grp.loc[is_training, FEATURES_LOCAL_MODEL],\n",
    "            XY_grp.loc[is_training, 'num_sold_log']\n",
    "            )\n",
    "        \n",
    "        segments_models[grp] = pipeline_e2e\n",
    "\n",
    "        predictions = (\n",
    "            XY_grp\n",
    "            .copy()\n",
    "            .assign(yhat = lambda df_: np.exp(pipeline_e2e.predict(df_)))\n",
    "            )\n",
    "        segments_predictions.append(predictions)\n",
    "\n",
    "\n",
    "    predictions = pd.concat(segments_predictions, axis=0)\n",
    "\n",
    "    scores = {\n",
    "        'validation': {\n",
    "            'nobs': predictions.loc[is_validation].shape[0],\n",
    "            'score': mean_absolute_percentage_error( \n",
    "                predictions.loc[is_validation, 'num_sold'],\n",
    "                predictions.loc[is_validation, 'yhat']\n",
    "                )\n",
    "            },\n",
    "        'train': {\n",
    "            'nobs': predictions.loc[is_training].shape[0],\n",
    "            'score': mean_absolute_percentage_error( \n",
    "                predictions.loc[is_training, 'num_sold'],\n",
    "                predictions.loc[is_training, 'yhat']\n",
    "                )\n",
    "            }\n",
    "        }\n",
    "\n",
    "    kfolds_evaluation.append(scores)\n",
    "\n",
    "kfolds_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert predictions.shape[0] == sales_daily_complete.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_sample = predictions.query(\"series_id == 'Norway|Premium Sticker Mart|Kaggle'\")\n",
    "\n",
    "predictions_sample.set_index('date')[['num_sold', 'yhat']].plot.line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = segments_models['Norway|Premium Sticker Mart|Kaggle'].named_steps.model\n",
    "# coefs = pd.Series(model.coef_, index=model.feature_names_in_)\n",
    "# coefs.to_csv(\"coefs.csv\")\n",
    "# coefs.sort_values(ascending=False).to_frame().head(25)\n",
    "# coefs.sort_values(ascending=True).to_frame().head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_sample = (\n",
    "    predictions_sample\n",
    "    .assign(residual = lambda df_: df_['num_sold'] - df_['yhat'])\n",
    "    .sort_values('date')\n",
    "    .assign(\n",
    "        residual_lag1 = lambda df_: df_['residual'].shift(1),\n",
    "        residual_lag2 = lambda df_: df_['residual'].shift(2),\n",
    "        residual_lag3 = lambda df_: df_['residual'].shift(3),\n",
    "        residual_lag5 = lambda df_: df_['residual'].shift(5),\n",
    "        residual_lag7 = lambda df_: df_['residual'].shift(7),\n",
    "        residual_lag14 = lambda df_: df_['residual'].shift(14),\n",
    "        )\n",
    "    )\n",
    "\n",
    "(\n",
    "    p9.ggplot((\n",
    "        predictions_sample\n",
    "        .loc[predictions_sample['date'] <= pd.to_datetime(\"2016-01-01\")]\n",
    "        )) + \n",
    "    p9.theme_bw() + \n",
    "    p9.geom_point(p9.aes('residual_lag1', 'residual')) + \n",
    "    p9.geom_smooth(p9.aes('residual_lag1', 'residual'), method='lm', se=False, color='lightblue')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (\n",
    "#     predictions\n",
    "#     .assign(mape = lambda df_: np.abs(100 * (df_['num_sold']/df_['yhat'] - 1)) )\n",
    "#     .sort_values('mape', ascending=False)\n",
    "#     .head(100)\n",
    "#     .to_csv(\"./data/processed/predictions_errors_local_model.csv\", index=False)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    predictions\n",
    "    .assign(mape = lambda df_: np.abs(100 * (df_['num_sold']/df_['yhat'] - 1)) )\n",
    "    .sort_values('mape', ascending=False)\n",
    "    .head(50)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segments_XY = {grp: df for grp, df in XY.groupby('series_id')}\n",
    "\n",
    "# # even when split into many dataframes, *index-based* subsets.\n",
    "# # indexes maintained when dataframe splits.\n",
    "# kfolds = [\n",
    "\n",
    "#     # validation set 2014-16 matches ultimate test set's length, 2017-19\n",
    "#     ( \n",
    "#         ( (XY['date'] >= pd.to_datetime('2010-01-01')) & (XY['date'] < pd.to_datetime(\"2014-01-01\")) ),\n",
    "#         XY['date'] >= pd.to_datetime(\"2014-01-01\")\n",
    "#     ),\n",
    "\n",
    "#     ( \n",
    "#         ( (XY['date'] >= pd.to_datetime('2010-01-01')) & (XY['date'] < pd.to_datetime(\"2016-01-01\")) ),\n",
    "#         XY['date'] >= pd.to_datetime(\"2016-01-01\")\n",
    "#     )\n",
    "\n",
    "#     ]\n",
    "\n",
    "# kfolds_evaluation = []\n",
    "# FEATURES_LOCAL_TREND_MODEL = FEATURES_LOCAL_MODEL\n",
    "# for is_training, is_validation in kfolds:\n",
    "    \n",
    "#     segments_models = {\n",
    "#         grp: (\n",
    "#             TrendRemainderModelPipeline(FEATURES_LOCAL_TREND_MODEL, RIDGE_ALPHA_TUNED, ['day_of_year'])\n",
    "#             .fit(XY.loc[is_training, FEATURES_LOCAL_MODEL], XY.loc[is_training, 'num_sold_log'])\n",
    "#             )\n",
    "#         for grp, XY in segments_XY.items()\n",
    "#         }\n",
    "\n",
    "#     segments_predictions = [\n",
    "#         df.assign(\n",
    "#             yhat = lambda df_: np.exp(segments_models[grp].predict(df_[FEATURES_LOCAL_MODEL])) \n",
    "#             )\n",
    "#         for grp, df in segments_XY.items()\n",
    "#         ]\n",
    "\n",
    "#     predictions = pd.concat(segments_predictions, axis=0)\n",
    "\n",
    "#     scores = {\n",
    "#         'validation': mean_absolute_percentage_error( \n",
    "#             predictions.loc[is_validation, 'num_sold'],\n",
    "#             predictions.loc[is_validation, 'yhat']\n",
    "#             ),\n",
    "#         'train': mean_absolute_percentage_error( \n",
    "#             predictions.loc[is_training, 'num_sold'],\n",
    "#             predictions.loc[is_training, 'yhat']\n",
    "#             )\n",
    "#         }\n",
    "    \n",
    "#     kfolds_evaluation.append(scores)\n",
    "\n",
    "# kfolds_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions_sample = predictions.query(\"series_id == 'Norway|Premium Sticker Mart|Kaggle'\")\n",
    "\n",
    "# predictions_sample.set_index('date')[['num_sold', 'yhat']].plot.line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEGMENTS_MODELS = {}\n",
    "\n",
    "for grp, XY_grp in segments_XY.items():\n",
    "\n",
    "    feature_transform_pipeline_local_model = ColumnTransformer([\n",
    "        ('transformer_onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), FEATURES_LOCAL_MODEL_TO_ONEHOT),\n",
    "        ('transformer_std', StandardScaler(), FEATURES_UNIVERSE_NUMERIC_CONTINUOUS)\n",
    "        ],\n",
    "        verbose_feature_names_out=False,\n",
    "        remainder='passthrough'\n",
    "        )\n",
    "    feature_transform_pipeline_local_model.set_output(transform='pandas')\n",
    "\n",
    "    pipeline_e2e = Pipeline([\n",
    "        ('transform_features', feature_transform_pipeline_local_model), \n",
    "        ('model', Ridge(RIDGE_ALPHA_TUNED))\n",
    "        ])\n",
    "\n",
    "    pipeline_e2e.fit(XY_grp[FEATURES_LOCAL_MODEL], XY_grp['num_sold_log'])\n",
    "    \n",
    "    SEGMENTS_MODELS[grp] = pipeline_e2e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_test_daily = pd.read_csv(\"./data/external/test.csv\").assign(\n",
    "\n",
    "    date = lambda df_: pd.to_datetime(df_['date']),\n",
    "\n",
    "    country_store = lambda df_: df_['country'].str.cat(df_['store'], sep='|'),\n",
    "    country_product = lambda df_: df_['country'].str.cat(df_['product'], sep='|'),\n",
    "    store_product = lambda df_: df_['store'].str.cat(df_['product'], sep='|'),\n",
    "    country_store_product = lambda df_: df_['country'].str.cat([df_['store'], df_['product']], sep='|')\n",
    "\n",
    "    ).assign(series_id = lambda df_: df_['country_store_product'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_test_daily.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_test_daily['date'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_test_daily = transform_calendar_features(sales_test_daily)\n",
    "sales_test_daily = integrate_external_features(sales_test_daily)\n",
    "sales_test_daily = sales_test_daily.assign(\n",
    "    num_sold = None,\n",
    "    num_sold_log = None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_X_test = {grp: df for grp, df in sales_test_daily.groupby('series_id')}\n",
    "\n",
    "segments_predictions_test = []\n",
    "for grp, df in segments_X_test.items():\n",
    "\n",
    "    if grp in SEGMENTS_MODELS:\n",
    "        df = df.assign(\n",
    "            yhat = lambda df_: np.exp(SEGMENTS_MODELS[grp].predict(df_[FEATURES_LOCAL_MODEL])) \n",
    "            )  \n",
    "        \n",
    "    else:\n",
    "        df = df.assign(\n",
    "            yhat = lambda df_: np.exp(MODEL_GLOBAL.predict(df_[FEATURES_GLOBAL_MODEL])) \n",
    "            )\n",
    "\n",
    "    segments_predictions_test.append(df)\n",
    "\n",
    "predictions_test = pd.concat(segments_predictions_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test_submit = (\n",
    "    predictions_test\n",
    "    [['id', 'yhat']]\n",
    "    .rename(columns={'yhat': 'num_sold'})\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert predictions_test_submit.shape[0] == 98_550\n",
    "assert predictions_test_submit.notnull().all().all()\n",
    "predictions_test_submit.to_csv(\"./data/processed/submission_pipelines_fix-gamma.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differential Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_compare = pd.read_csv(\"./data/processed/submission5.csv\").rename(columns={'num_sold': 'num_sold_prev'})\n",
    "predictions_compare = (\n",
    "    pd.merge(predictions_compare, predictions_test_submit, how='left')\n",
    "    .assign(diff = lambda df_: 100 * (df_['num_sold'] / df_['num_sold_prev'] - 1))\n",
    "    )\n",
    "assert predictions_compare.shape[0] == 98_550\n",
    "\n",
    "predictions_compare = pd.merge(\n",
    "    predictions_compare, \n",
    "    sales_test_daily[['id', 'country', 'product', 'store', 'year', 'series_id']]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_compare['diff'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_compare.groupby('country')['diff'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_compare.groupby('year')['diff'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forecast_stickers_kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
