{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import requests\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "import optuna\n",
    "import xgboost as xgb\n",
    "from dateutil.easter import easter\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily = pd.read_csv(\"./data/external/train.csv\").assign(\n",
    "\n",
    "    date = lambda df_: pd.to_datetime(df_['date']),\n",
    "\n",
    "    country_store = lambda df_: df_['country'].str.cat(df_['store'], sep='|'),\n",
    "    country_product = lambda df_: df_['country'].str.cat(df_['product'], sep='|'),\n",
    "    store_product = lambda df_: df_['store'].str.cat(df_['product'], sep='|'),\n",
    "    country_store_product = lambda df_: df_['country'].str.cat([df_['store'], df_['product']], sep='|')\n",
    "\n",
    "    ).assign(series_id = lambda df_: df_['country_store_product'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_gdp_per_capita(country_code, year):\n",
    "    \"\"\"\n",
    "    Adapted from https://www.kaggle.com/competitions/playground-series-s5e1/discussion/554349.\n",
    "    \"\"\"\n",
    "\n",
    "    url='https://api.worldbank.org/v2/country/{0}/indicator/NY.GDP.PCAP.CD?date={1}&format=json'\n",
    "    response = requests.get(url.format(country_code, year)).json()\n",
    "\n",
    "    return response[1][0]['value']\n",
    "\n",
    "# per CountryCode-year: request GDP per capita.\n",
    "# concatenate dataframe of CountryCode | Country | Year | GDP, for integration to Kaggle source\n",
    "\n",
    "countries_code_map = {\n",
    "    'Canada': 'CAN', \n",
    "    'Finland': 'FIN',\n",
    "    'Italy': 'ITA',\n",
    "    'Kenya': 'KEN',\n",
    "    'Norway': 'NOR',\n",
    "    'Singapore': 'SGP'\n",
    "    }\n",
    "\n",
    "countries_gdp_yearly = []\n",
    "for country_title, country_code in countries_code_map.items():\n",
    "    \n",
    "    values_yearly = [\n",
    "        {'year': i, 'gdp_per_capita': extract_gdp_per_capita(country_code, i)}\n",
    "        for i in range(2010, 2019+1)\n",
    "        ]\n",
    "    values_yearly = [pd.DataFrame(x, index=[0]) for x in values_yearly]\n",
    "    values_yearly = pd.concat(values_yearly, axis=0)\n",
    "    \n",
    "    values_yearly = values_yearly.assign(\n",
    "        country = country_title,\n",
    "        country_code = country_code\n",
    "        )\n",
    "    \n",
    "    countries_gdp_yearly.append(values_yearly)\n",
    "\n",
    "    print(f\"{country_title} ({country_code}) GDP Per Capita extraction complete.\")\n",
    "\n",
    "countries_gdp_yearly = pd.concat(countries_gdp_yearly, axis=0)\n",
    "\n",
    "countries_gdp_yearly = countries_gdp_yearly.assign(\n",
    "    gdp_per_capita_log = lambda df_: np.log(df_['gdp_per_capita'])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    countries_gdp_yearly\n",
    "    .set_index('year')\n",
    "    .groupby('country')\n",
    "    ['gdp_per_capita']\n",
    "    .plot(legend=True)\n",
    ")\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_easter0 = [easter(x) for x in range(2010, 2019+1)]\n",
    "days_easter = pd.DataFrame({'date': days_easter0}).assign(is_easter = 1)\n",
    "\n",
    "# motivated by exploratory analysis of model errors.\n",
    "# appears that errors concentrate on days shortly after Easter\n",
    "dfs_days_special_relative_easter = [days_easter]\n",
    "for delta_days in [2, 3, 4, 5, 6, 7]:\n",
    "\n",
    "    df_special = (\n",
    "        days_easter\n",
    "        .copy()\n",
    "        .assign(date = lambda df_: df_['date'] + timedelta(days=delta_days))\n",
    "        .rename(columns={'is_easter': f'is_easter_plus{delta_days}'})\n",
    "        )\n",
    "    dfs_days_special_relative_easter.append(df_special)\n",
    "\n",
    "days_special_relative_easter = (\n",
    "    pd.concat(dfs_days_special_relative_easter, axis=0)\n",
    "    .fillna(0)\n",
    "    .assign(date = lambda df_: pd.to_datetime(df_['date']))\n",
    "    )\n",
    "\n",
    "assert days_special_relative_easter['date'].is_unique\n",
    "\n",
    "FEATURES_EASTER = [x for x in days_special_relative_easter.columns if 'easter' in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description Report: \"Surface Properties\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily['id'].is_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Volumetric Analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily['series_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily['date'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily['series_id'].value_counts().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily['product'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily['country'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily['store'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fields' Types and Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily['date'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily['num_sold'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily.isnull().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# are null sales events concentrated on a particular date?\n",
    "# doesn't appear so\n",
    "sales_daily.query(\"num_sold.isnull()\")['date'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily.query(\"num_sold.isnull()\")['series_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily.groupby('series_id')['num_sold'].sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a naive model: series' historical average sales, daily\n",
    "\n",
    "is_training = (\n",
    "    (sales_daily['date'] >= pd.to_datetime('2010-01-01'))\n",
    "    & (sales_daily['date'] < pd.to_datetime(\"2014-01-01\"))\n",
    ")\n",
    "\n",
    "is_validation = sales_daily['date'] >= pd.to_datetime(\"2014-01-01\")\n",
    "\n",
    "# is_training.sum(), is_validation.sum()\n",
    "\n",
    "# with original train data: using strictly train segment, groupby average\n",
    "predictions_naive = (\n",
    "    sales_daily\n",
    "    .loc[is_training]\n",
    "    .groupby('series_id')\n",
    "    [['num_sold']]\n",
    "    .agg('mean')\n",
    "    .reset_index(drop=False)\n",
    ")\n",
    "\n",
    "predictions_naive_evaluate = pd.merge(\n",
    "    sales_daily.loc[is_validation],\n",
    "    predictions_naive.rename(columns={'num_sold': 'yhat'}),\n",
    "    how='left'\n",
    "    )\n",
    "\n",
    "# expect a couple series with all null\n",
    "predictions_naive.isnull().sum()\n",
    "# recommended in this discussion: https://www.kaggle.com/competitions/playground-series-s5e1/discussion/554553 \n",
    "predictions_naive_evaluate = predictions_naive_evaluate.dropna()\n",
    "\n",
    "mean_absolute_percentage_error(\n",
    "    predictions_naive_evaluate['num_sold'],\n",
    "    predictions_naive_evaluate['yhat']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_naive_evaluate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a naive model: series' historical average sales, daily\n",
    "\n",
    "is_training = (\n",
    "    (sales_daily['date'] >= pd.to_datetime('2013-01-01'))\n",
    "    & (sales_daily['date'] < pd.to_datetime(\"2014-01-01\"))\n",
    ")\n",
    "\n",
    "is_validation = sales_daily['date'] >= pd.to_datetime(\"2014-01-01\")\n",
    "\n",
    "# is_training.sum(), is_validation.sum()\n",
    "\n",
    "# with original train data: using strictly train segment, groupby average\n",
    "predictions_naive = (\n",
    "    sales_daily\n",
    "    .loc[is_training]\n",
    "    .groupby('series_id')\n",
    "    [['num_sold']]\n",
    "    .agg('mean')\n",
    "    .reset_index(drop=False)\n",
    ")\n",
    "\n",
    "predictions_naive_evaluate = pd.merge(\n",
    "    sales_daily.loc[is_validation],\n",
    "    predictions_naive.rename(columns={'num_sold': 'yhat'}),\n",
    "    how='left'\n",
    "    )\n",
    "\n",
    "# expect a couple series with all null\n",
    "predictions_naive.isnull().sum()\n",
    "# recommended in this discussion: https://www.kaggle.com/competitions/playground-series-s5e1/discussion/554553 \n",
    "predictions_naive_evaluate = predictions_naive_evaluate.dropna()\n",
    "\n",
    "mean_absolute_percentage_error(\n",
    "    predictions_naive_evaluate['num_sold'],\n",
    "    predictions_naive_evaluate['yhat']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_sample_daily = sales_daily.query(\"series_id == 'Norway|Premium Sticker Mart|Kaggle'\")\n",
    "\n",
    "(\n",
    "    sales_sample_daily\n",
    "    .loc[is_training]\n",
    "    [['date', 'num_sold']]\n",
    "    .set_index('date')\n",
    "    .plot\n",
    "    .line()\n",
    ")\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_calendar_features(df):\n",
    "\n",
    "    df = (\n",
    "        df\n",
    "        .assign(\n",
    "            year = lambda df_: df_['date'].dt.year,\n",
    "            month = lambda df_: df_['date'].dt.month,\n",
    "            week_of_year = lambda df_: df_['date'].dt.isocalendar().week,\n",
    "            day_of_week = lambda df_: df_['date'].dt.day_name(),\n",
    "            # President's Day is the 'third Monday in February'\n",
    "            day_of_month = lambda df_: df_['date'].dt.day,\n",
    "            day_of_year = lambda df_: df_['date'].dt.dayofyear,\n",
    "            # week of month would be ambiguous because, one week may span 2 months,\n",
    "            days_since_start = lambda df_: (df['date'] - pd.to_datetime(\"2010-01-01\")).dt.days\n",
    "            )\n",
    "        .assign(\n",
    "            # as day_of_year rises, don't expect monotonic relationship with outcome.\n",
    "            # rather, expect periodic (sinusoidal) relationship.\n",
    "            # as sin(x) rises, so too does outcome ...\n",
    "            # ensure one cycle over one year.\n",
    "            # at baseline, one sinusoidal cycle occurs per 2π\n",
    "            day_of_year_sin = lambda df_: np.sin(df_['day_of_year'] * 2 * np.pi / 365),\n",
    "            day_of_year_cos = lambda df_: np.cos(df_['day_of_year'] * 2 * np.pi / 365),\n",
    "\n",
    "            day_of_month_sin = lambda df_: np.sin(df_['day_of_month'] * 2 * np.pi / 30),\n",
    "            day_of_month_cos = lambda df_: np.cos(df_['day_of_month'] * 2 * np.pi / 30),\n",
    "\n",
    "            # exploratory visuals suggest ~2-year cycles\n",
    "            days_since_start_macro_sin = lambda df_: np.sin(df_['days_since_start'] * 2 * np.pi / 730),\n",
    "            days_since_start_macro_cos = lambda df_: np.cos(df_['days_since_start'] * 2 * np.pi / 730),\n",
    "            )\n",
    "        .assign(\n",
    "            is_yearend = lambda df_: (\n",
    "                (df_['month'] == 12) & (df_['day_of_month'].isin([28, 29, 30, 31]))\n",
    "                ).astype(int)\n",
    "            )\n",
    "\n",
    "        )\n",
    "    \n",
    "    df = pd.merge(df, days_special_relative_easter, how='left')\n",
    "    assert df['is_easter'].notnull().any()\n",
    "    df[FEATURES_EASTER] = df[FEATURES_EASTER].fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def integrate_external_features(df):\n",
    "\n",
    "    df = pd.merge(df, countries_gdp_yearly, how='left')\n",
    "    assert df['gdp_per_capita'].notnull().all().all()\n",
    "\n",
    "    return df\n",
    "\n",
    "sales_daily = transform_calendar_features(sales_daily)\n",
    "sales_daily = integrate_external_features(sales_daily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_TO_ONEHOT = [\n",
    "    'country', \n",
    "    'store',\n",
    "    'product',\n",
    "    'country_store',\n",
    "    'country_product',\n",
    "    'store_product',\n",
    "    'country_store_product',\n",
    "    # year attempted, but then omitted, because year-grained shifts\n",
    "    # should be explained by exogenous factors, \n",
    "    # out-of-sample forecasts that aren't flat\n",
    "    'month', \n",
    "    'week_of_year', \n",
    "    'day_of_week'\n",
    "    ]\n",
    "FEATURES_NUMERIC_CONTINUOUS = [\n",
    "    'gdp_per_capita_log',\n",
    "    'day_of_month', \n",
    "    'day_of_month_sin',\n",
    "    'day_of_month_cos',\n",
    "    'day_of_year_sin',\n",
    "    'day_of_year_cos',\n",
    "    'day_of_year',\n",
    "    'days_since_start_macro_sin',\n",
    "    'days_since_start_macro_cos',\n",
    "    'days_since_start'\n",
    "    ]\n",
    "FEATURES_ONEHOT = ['is_yearend'] + FEATURES_EASTER\n",
    "FEATURES_SOURCE_FORM = FEATURES_TO_ONEHOT + FEATURES_NUMERIC_CONTINUOUS + FEATURES_ONEHOT\n",
    "\n",
    "ATTRIBUTES = ['series_id', 'date', 'id']\n",
    "\n",
    "sales_daily_complete = sales_daily.dropna(subset='num_sold')\n",
    "\n",
    "XY = sales_daily_complete[['num_sold'] + ATTRIBUTES + FEATURES_SOURCE_FORM]\n",
    "\n",
    "XY = XY.assign(num_sold_log = lambda df_: np.log(df_['num_sold']))\n",
    "\n",
    "feature_transform_pipeline = ColumnTransformer([\n",
    "    ('transformer_onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), FEATURES_TO_ONEHOT),\n",
    "    ('transformer_std', StandardScaler(), FEATURES_NUMERIC_CONTINUOUS)\n",
    "    ],\n",
    "    verbose_feature_names_out=False,\n",
    "    remainder='passthrough'\n",
    "    )\n",
    "feature_transform_pipeline.set_output(transform='pandas')\n",
    "\n",
    "XY = feature_transform_pipeline.fit_transform(XY)\n",
    "\n",
    "FEATURES_UNIVERSE = list( set(XY.columns).difference(set(ATTRIBUTES + ['num_sold', 'num_sold_log'])) )\n",
    "\n",
    "FEATURES_GLOBAL_MODEL = [\n",
    "    x for x in FEATURES_UNIVERSE\n",
    "    # inclusion yields flat predictions.\n",
    "    # moreover, sinusoidal transforms drag performance\n",
    "    if not any(stem in x for stem in ['days_since_start'])\n",
    "    ]\n",
    "\n",
    "FEATURES_GLOBAL_TREND_LEVEL_MODEL = [\n",
    "    x for x in FEATURES_UNIVERSE \n",
    "    if any(stem in x for stem in ['country', 'store', 'product', 'gdp']) & ('country_store_product' not in x)\n",
    "    ]\n",
    "\n",
    "FEATURES_LOCAL_MODEL = [\n",
    "    x for x in FEATURES_UNIVERSE \n",
    "    if not any(stem in x for stem in ['country', 'store', 'product'])\n",
    "    ]\n",
    "\n",
    "FEATURES_LOCAL_REMAINDER_MODEL = list( set(FEATURES_LOCAL_MODEL).intersection(set(FEATURES_GLOBAL_MODEL)) )\n",
    "FEATURES_LOCAL_REMAINDER_MODEL.remove('gdp_per_capita_log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrendRemainderModelPipeline:\n",
    "    def __init__(\n",
    "        self, \n",
    "        trend_model_features, \n",
    "        trend_model_ridge_alpha, \n",
    "        remainder_model_features\n",
    "        ):\n",
    "\n",
    "        self.trend_model_features = trend_model_features\n",
    "        self.trend_level_model_ridge_alpha = trend_model_ridge_alpha\n",
    "\n",
    "        self.remainder_model_features = remainder_model_features\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        self.fit_trend_level_model(X, y)\n",
    "        yhat_trend_level = self.predict_trend_level_model(X)\n",
    "        y_detrended = y - yhat_trend_level\n",
    "\n",
    "        self.fit_remainder_model(X, y_detrended)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        yhat_trend_level = self.predict_trend_level_model(X)\n",
    "        yhat_remainder = self.predict_remainder_model(X)\n",
    "\n",
    "        return yhat_trend_level + yhat_remainder\n",
    "\n",
    "    def fit_trend_level_model(self, X, y):\n",
    "\n",
    "        model_trend_level = Ridge(self.trend_level_model_ridge_alpha)\n",
    "        model_trend_level.fit(X[self.trend_model_features], y)\n",
    "\n",
    "        self.trend_level_model = model_trend_level\n",
    "\n",
    "    def fit_remainder_model(self, X, y):\n",
    "\n",
    "        model_remainder = RandomForestRegressor(n_estimators=100, n_jobs=-1)\n",
    "        model_remainder.fit(X[self.remainder_model_features], y)\n",
    "\n",
    "        self.remainder_model = model_remainder\n",
    "\n",
    "    def predict_trend_level_model(self, X):\n",
    "        return self.trend_level_model.predict(X[self.trend_model_features])\n",
    "    \n",
    "    def predict_remainder_model(self, X):\n",
    "        return self.remainder_model.predict(X[self.remainder_model_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfolds = [\n",
    "\n",
    "    # validation set 2014-16 matches ultimate test set's length, 2017-19\n",
    "    ( \n",
    "        ( (XY['date'] >= pd.to_datetime('2010-01-01')) & (XY['date'] < pd.to_datetime(\"2014-01-01\")) ),\n",
    "        XY['date'] >= pd.to_datetime(\"2014-01-01\")\n",
    "    ),\n",
    "\n",
    "    ( \n",
    "        ( (XY['date'] >= pd.to_datetime('2010-01-01')) & (XY['date'] < pd.to_datetime(\"2016-01-01\")) ),\n",
    "        XY['date'] >= pd.to_datetime(\"2016-01-01\")\n",
    "    )\n",
    "\n",
    "    ]\n",
    "\n",
    "kfolds_evaluation = []\n",
    "for is_training, is_validation in kfolds:\n",
    "\n",
    "    model_global = RandomForestRegressor(n_estimators=100, n_jobs=-1)\n",
    "    model_global.fit(\n",
    "        XY.loc[is_training, FEATURES_GLOBAL_MODEL],\n",
    "        XY.loc[is_training, 'num_sold_log']\n",
    "        )\n",
    "    \n",
    "    predictions = (\n",
    "        XY\n",
    "        .copy()\n",
    "        .assign(yhat = lambda df_: np.exp(model_global.predict(df_[FEATURES_GLOBAL_MODEL])))\n",
    "        )\n",
    "    \n",
    "    scores = {\n",
    "        'validation': mean_absolute_percentage_error( \n",
    "            predictions.loc[is_validation, 'num_sold'],\n",
    "            predictions.loc[is_validation, 'yhat']\n",
    "            ),\n",
    "        'train': mean_absolute_percentage_error( \n",
    "            predictions.loc[is_training, 'num_sold'],\n",
    "            predictions.loc[is_training, 'yhat']\n",
    "            )\n",
    "        }\n",
    "    \n",
    "    kfolds_evaluation.append(scores)\n",
    "\n",
    "kfolds_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_sample = predictions.query(\"series_id == 'Norway|Stickers for Less|Kaggle'\")\n",
    "# predictions_sample = predictions.query(\"series_id == 'Norway|Premium Sticker Mart|Kaggle'\")\n",
    "\n",
    "predictions_sample.set_index('date')[['num_sold', 'yhat']].plot.line(alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# result = permutation_importance(\n",
    "#     model_global, \n",
    "#     XY.loc[is_validation, FEATURES_GLOBAL_MODEL],\n",
    "#     XY.loc[is_validation, 'num_sold_log'], \n",
    "#     n_repeats=5, \n",
    "#     random_state=777, \n",
    "#     n_jobs=-1\n",
    "# )\n",
    "# importances = pd.Series(result.importances_mean, index=FEATURES_GLOBAL_MODEL)\n",
    "\n",
    "importances = pd.Series(model_global.feature_importances_, index=FEATURES_GLOBAL_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances.sort_values(ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfolds = [\n",
    "\n",
    "    # validation set 2014-16 matches ultimate test set's length, 2017-19\n",
    "    ( \n",
    "        ( (XY['date'] >= pd.to_datetime('2010-01-01')) & (XY['date'] < pd.to_datetime(\"2014-01-01\")) ),\n",
    "        XY['date'] >= pd.to_datetime(\"2014-01-01\")\n",
    "    ),\n",
    "\n",
    "    ( \n",
    "        ( (XY['date'] >= pd.to_datetime('2010-01-01')) & (XY['date'] < pd.to_datetime(\"2016-01-01\")) ),\n",
    "        XY['date'] >= pd.to_datetime(\"2016-01-01\")\n",
    "    )\n",
    "\n",
    "    ]\n",
    "\n",
    "kfolds_evaluation = []\n",
    "for is_training, is_validation in kfolds:\n",
    "\n",
    "    model_global = Ridge(1e-2)\n",
    "    model_global.fit(\n",
    "        XY.loc[is_training, FEATURES_GLOBAL_MODEL],\n",
    "        XY.loc[is_training, 'num_sold_log']\n",
    "        )\n",
    "    \n",
    "    predictions = (\n",
    "        XY\n",
    "        .copy()\n",
    "        .assign(yhat = lambda df_: np.exp(model_global.predict(df_[FEATURES_GLOBAL_MODEL])))\n",
    "        )\n",
    "    \n",
    "    scores = {\n",
    "        'validation': mean_absolute_percentage_error( \n",
    "            predictions.loc[is_validation, 'num_sold'],\n",
    "            predictions.loc[is_validation, 'yhat']\n",
    "            ),\n",
    "        'train': mean_absolute_percentage_error( \n",
    "            predictions.loc[is_training, 'num_sold'],\n",
    "            predictions.loc[is_training, 'yhat']\n",
    "            )\n",
    "        }\n",
    "    \n",
    "    kfolds_evaluation.append(scores)\n",
    "\n",
    "kfolds_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_sample = predictions.query(\"series_id == 'Norway|Stickers for Less|Kaggle'\")\n",
    "# predictions_sample = predictions.query(\"series_id == 'Norway|Premium Sticker Mart|Kaggle'\")\n",
    "\n",
    "predictions_sample.set_index('date')[['num_sold', 'yhat']].plot.line(alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfolds = [\n",
    "\n",
    "    # validation set 2014-16 matches ultimate test set's length, 2017-19\n",
    "    ( \n",
    "        ( (XY['date'] >= pd.to_datetime('2010-01-01')) & (XY['date'] < pd.to_datetime(\"2014-01-01\")) ),\n",
    "        XY['date'] >= pd.to_datetime(\"2014-01-01\")\n",
    "    ),\n",
    "\n",
    "    ( \n",
    "        ( (XY['date'] >= pd.to_datetime('2010-01-01')) & (XY['date'] < pd.to_datetime(\"2016-01-01\")) ),\n",
    "        XY['date'] >= pd.to_datetime(\"2016-01-01\")\n",
    "    )\n",
    "\n",
    "    ]\n",
    "\n",
    "kfolds_evaluation = []\n",
    "for is_training, is_validation in kfolds:\n",
    "\n",
    "    model_global_pipeline = TrendRemainderModelPipeline(\n",
    "        trend_model_features=FEATURES_GLOBAL_TREND_LEVEL_MODEL,\n",
    "        trend_model_ridge_alpha=1e-2,\n",
    "        remainder_model_features=FEATURES_GLOBAL_MODEL\n",
    "        )\n",
    "    model_global_pipeline.fit(\n",
    "        XY.loc[is_training].drop(columns='num_sold_log'),\n",
    "        XY.loc[is_training, 'num_sold_log']\n",
    "        )\n",
    "    \n",
    "    predictions = XY.copy()\n",
    "    predictions = (\n",
    "        predictions\n",
    "        .assign(yhat_log = lambda df_: model_global_pipeline.predict(df_))\n",
    "        .assign(yhat = lambda df_: np.exp(df_['yhat_log']) )\n",
    "        )\n",
    "    \n",
    "    scores = {\n",
    "        'validation': mean_absolute_percentage_error( \n",
    "            predictions.loc[is_validation, 'num_sold'],\n",
    "            predictions.loc[is_validation, 'yhat']\n",
    "            ),\n",
    "        'train': mean_absolute_percentage_error( \n",
    "            predictions.loc[is_training, 'num_sold'],\n",
    "            predictions.loc[is_training, 'yhat']\n",
    "            )\n",
    "        }\n",
    "    \n",
    "    kfolds_evaluation.append(scores)\n",
    "\n",
    "kfolds_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_sample = predictions.query(\"series_id == 'Norway|Stickers for Less|Kaggle'\")\n",
    "# predictions_sample = predictions.query(\"series_id == 'Norway|Premium Sticker Mart|Kaggle'\")\n",
    "\n",
    "predictions_sample.set_index('date')[['num_sold', 'yhat']].plot.line(alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient boosting machine does not appear to improve global model.\n",
    "\n",
    "# def objective(trial):\n",
    "\n",
    "#     ROUNDS_COUNT = 100\n",
    "\n",
    "#     param = {\n",
    "#         \"objective\": \"reg:squarederror\",\n",
    "#         \"booster\": trial.suggest_categorical(\n",
    "#             \"booster\", [\"gbtree\", \"gblinear\", \"dart\"]\n",
    "#         ),\n",
    "#         \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "#         \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "#         \"subsample\": trial.suggest_float(\"subsample\", 0.4, 1.0),\n",
    "#         \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n",
    "#     }\n",
    "\n",
    "#     if param[\"booster\"] in [\"gbtree\", \"dart\"]:\n",
    "#         param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 1, 9, step=2)\n",
    "\n",
    "#         param[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 1, 10)\n",
    "#         param[\"eta\"] = trial.suggest_float(\"eta\", 1e-5, 0.01, log=True)\n",
    "\n",
    "#         param[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
    "#         param[\"grow_policy\"] = trial.suggest_categorical(\n",
    "#             \"grow_policy\", [\"depthwise\", \"lossguide\"]\n",
    "#         )\n",
    "\n",
    "#     if param[\"booster\"] == \"dart\":\n",
    "#         param[\"sample_type\"] = trial.suggest_categorical(\n",
    "#             \"sample_type\", [\"uniform\", \"weighted\"]\n",
    "#         )\n",
    "#         param[\"normalize_type\"] = trial.suggest_categorical(\n",
    "#             \"normalize_type\", [\"tree\", \"forest\"]\n",
    "#         )\n",
    "#         param[\"rate_drop\"] = trial.suggest_float(\n",
    "#             \"rate_drop\", 1e-8, 1.0, log=True\n",
    "#         )\n",
    "#         param[\"skip_drop\"] = trial.suggest_float(\n",
    "#             \"skip_drop\", 1e-8, 1.0, log=True\n",
    "#         )\n",
    "\n",
    "\n",
    "#     kfolds_evaluation = []\n",
    "#     for is_training, is_validation in kfolds:\n",
    "\n",
    "#         dtrain = xgb.DMatrix(\n",
    "#             XY.loc[is_training, FEATURES_GLOBAL_MODEL], \n",
    "#             label=XY.loc[is_training, 'num_sold_log']\n",
    "#             )\n",
    "#         dtest = xgb.DMatrix(XY[FEATURES_GLOBAL_MODEL])\n",
    "\n",
    "#         model_global = xgb.train(param, dtrain, ROUNDS_COUNT)\n",
    "        \n",
    "#         yhat = model_global.predict(dtest)\n",
    "#         predictions = (\n",
    "#             XY\n",
    "#             .copy()\n",
    "#             .assign(yhat = lambda df_: np.exp(yhat))\n",
    "#             )\n",
    "\n",
    "#         scores = {\n",
    "#             'validation': mean_absolute_percentage_error( \n",
    "#                 predictions.loc[is_validation, 'num_sold'],\n",
    "#                 predictions.loc[is_validation, 'yhat']\n",
    "#                 ),\n",
    "#             'train': mean_absolute_percentage_error( \n",
    "#                 predictions.loc[is_training, 'num_sold'],\n",
    "#                 predictions.loc[is_training, 'yhat']\n",
    "#                 )\n",
    "#             }\n",
    "        \n",
    "#         kfolds_evaluation.append(scores)\n",
    "\n",
    "#     score_overall = np.mean([ \n",
    "#         kfolds_evaluation[0]['validation'], \n",
    "#         kfolds_evaluation[1]['validation'] \n",
    "#         ])\n",
    "    \n",
    "#     return score_overall\n",
    "\n",
    "# study = optuna.create_study()\n",
    "\n",
    "# study.optimize(\n",
    "#     objective,\n",
    "#     n_trials=50,\n",
    "#     catch=(ValueError,),\n",
    "#     n_jobs=-1,\n",
    "#     timeout=12 * 60 * 60,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_global = TrendRemainderModelPipeline(\n",
    "        trend_model_features=FEATURES_GLOBAL_TREND_LEVEL_MODEL,\n",
    "        trend_model_ridge_alpha=1e-2,\n",
    "        remainder_model_features=FEATURES_GLOBAL_MODEL\n",
    "        )\n",
    "model_global.fit(XY.drop(columns='num_sold_log'), XY['num_sold_log'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from previous retail forecasting competitions' leaders,\n",
    "# plus theoretically expected heterogeneity between series: \n",
    "# one model per segment\n",
    "\n",
    "segments_XY = {grp: df for grp, df in XY.groupby('series_id')}\n",
    "\n",
    "# even when split into many dataframes, *index-based* subsets.\n",
    "# indexes maintained when dataframe splits.\n",
    "kfolds = [\n",
    "\n",
    "    # validation set 2014-16 matches ultimate test set's length, 2017-19\n",
    "    ( \n",
    "        ( (XY['date'] >= pd.to_datetime('2010-01-01')) & (XY['date'] < pd.to_datetime(\"2014-01-01\")) ),\n",
    "        XY['date'] >= pd.to_datetime(\"2014-01-01\")\n",
    "    ),\n",
    "\n",
    "    ( \n",
    "        ( (XY['date'] >= pd.to_datetime('2010-01-01')) & (XY['date'] < pd.to_datetime(\"2016-01-01\")) ),\n",
    "        XY['date'] >= pd.to_datetime(\"2016-01-01\")\n",
    "    )\n",
    "\n",
    "    ]\n",
    "\n",
    "kfolds_evaluation = []\n",
    "for is_training, is_validation in kfolds:\n",
    "\n",
    "    # per group: fit model on train, predict on validation\n",
    "    # WARNING: one model instance, then fitting it on each segment,\n",
    "    # does not imply separate model objects. they're all tied together.\n",
    "        # model = LinearRegression()\n",
    "    segments_models = {\n",
    "        grp: Ridge(1e-1).fit(\n",
    "            XY.loc[is_training, FEATURES_LOCAL_MODEL], \n",
    "            XY.loc[is_training, 'num_sold_log']\n",
    "            )\n",
    "        for grp, XY in segments_XY.items()\n",
    "        }\n",
    "\n",
    "    segments_predictions = [\n",
    "        df.assign(\n",
    "            yhat = lambda df_: np.exp(segments_models[grp].predict(df_[FEATURES_LOCAL_MODEL])) \n",
    "            )\n",
    "        for grp, df in segments_XY.items()\n",
    "        ]\n",
    "\n",
    "    predictions = pd.concat(segments_predictions, axis=0)\n",
    "\n",
    "    scores = {\n",
    "        'validation': mean_absolute_percentage_error( \n",
    "            predictions.loc[is_validation, 'num_sold'],\n",
    "            predictions.loc[is_validation, 'yhat']\n",
    "            ),\n",
    "        'train': mean_absolute_percentage_error( \n",
    "            predictions.loc[is_training, 'num_sold'],\n",
    "            predictions.loc[is_training, 'yhat']\n",
    "            )\n",
    "        }\n",
    "    \n",
    "    kfolds_evaluation.append(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfolds_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    parameters = {'alpha': trial.suggest_float(\"alpha\", 1e-4, 1, log=True)}\n",
    "\n",
    "    kfolds_evaluation = []\n",
    "    for is_training, is_validation in kfolds:\n",
    "\n",
    "        # per group: fit model on train, predict on validation\n",
    "        segments_models = {\n",
    "            grp: Ridge(**parameters).fit(\n",
    "                XY.loc[is_training, FEATURES_LOCAL_MODEL], \n",
    "                XY.loc[is_training, 'num_sold_log']\n",
    "                )\n",
    "            for grp, XY in segments_XY.items()\n",
    "            }\n",
    "\n",
    "        segments_predictions = [\n",
    "            df.assign(\n",
    "                yhat = lambda df_: np.exp(segments_models[grp].predict(df_[FEATURES_LOCAL_MODEL])) \n",
    "                )\n",
    "            for grp, df in segments_XY.items()\n",
    "            ]\n",
    "\n",
    "        predictions = pd.concat(segments_predictions, axis=0)\n",
    "\n",
    "        scores = {\n",
    "            'validation': mean_absolute_percentage_error( \n",
    "                predictions.loc[is_validation, 'num_sold'],\n",
    "                predictions.loc[is_validation, 'yhat']\n",
    "                ),\n",
    "            'train': mean_absolute_percentage_error( \n",
    "                predictions.loc[is_training, 'num_sold'],\n",
    "                predictions.loc[is_training, 'yhat']\n",
    "                )\n",
    "            }\n",
    "        \n",
    "        kfolds_evaluation.append(scores)\n",
    "\n",
    "    score_overall = np.mean([ \n",
    "        kfolds_evaluation[0]['validation'], \n",
    "        kfolds_evaluation[1]['validation'] \n",
    "        ])\n",
    "    \n",
    "    return score_overall\n",
    "\n",
    "study = optuna.create_study()\n",
    "\n",
    "study.optimize(\n",
    "    objective,\n",
    "    n_trials=50,\n",
    "    catch=(ValueError,),\n",
    "    n_jobs=-1,\n",
    "    timeout=12 * 60 * 60,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_trial, study.best_params, study.best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INCOMPLETE: stalled after ~20 minute runtime, and score not better than 0.1\n",
    "\n",
    "# def objective(trial):\n",
    "\n",
    "#     ROUNDS_COUNT = 100\n",
    "\n",
    "#     param = {\n",
    "#         \"objective\": \"reg:squarederror\",\n",
    "#         \"booster\": trial.suggest_categorical(\n",
    "#             \"booster\", [\"gbtree\", \"gblinear\", \"dart\"]\n",
    "#         ),\n",
    "#         \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "#         \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "#         \"subsample\": trial.suggest_float(\"subsample\", 0.4, 1.0),\n",
    "#         \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n",
    "#     }\n",
    "\n",
    "#     if param[\"booster\"] in [\"gbtree\", \"dart\"]:\n",
    "#         param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 1, 9, step=2)\n",
    "\n",
    "#         param[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 1, 10)\n",
    "#         param[\"eta\"] = trial.suggest_float(\"eta\", 1e-5, 0.01, log=True)\n",
    "\n",
    "#         param[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
    "#         param[\"grow_policy\"] = trial.suggest_categorical(\n",
    "#             \"grow_policy\", [\"depthwise\", \"lossguide\"]\n",
    "#         )\n",
    "\n",
    "#     if param[\"booster\"] == \"dart\":\n",
    "#         param[\"sample_type\"] = trial.suggest_categorical(\n",
    "#             \"sample_type\", [\"uniform\", \"weighted\"]\n",
    "#         )\n",
    "#         param[\"normalize_type\"] = trial.suggest_categorical(\n",
    "#             \"normalize_type\", [\"tree\", \"forest\"]\n",
    "#         )\n",
    "#         param[\"rate_drop\"] = trial.suggest_float(\n",
    "#             \"rate_drop\", 1e-8, 1.0, log=True\n",
    "#         )\n",
    "#         param[\"skip_drop\"] = trial.suggest_float(\n",
    "#             \"skip_drop\", 1e-8, 1.0, log=True\n",
    "#         )\n",
    "\n",
    "#     kfolds_evaluation = []\n",
    "#     for is_training, is_validation in kfolds:\n",
    "\n",
    "#         segments_models = {}\n",
    "#         segments_predictions = []\n",
    "#         for grp, XY in segments_XY.items():\n",
    "\n",
    "#             dtrain = xgb.DMatrix(\n",
    "#                 XY.loc[is_training, FEATURES_LOCAL_MODEL], \n",
    "#                 label=XY.loc[is_training, 'num_sold_log']\n",
    "#                 )\n",
    "#             dtest = xgb.DMatrix(XY[FEATURES_LOCAL_MODEL])\n",
    "\n",
    "#             model_global = xgb.train(param, dtrain, ROUNDS_COUNT)\n",
    "#             segments_models[grp] = model_global\n",
    "        \n",
    "#             yhat = model_global.predict(dtest)\n",
    "#             predictions = (\n",
    "#                 XY\n",
    "#                 .copy()\n",
    "#                 .assign(yhat = lambda df_: np.exp(yhat))\n",
    "#                 )\n",
    "#             segments_predictions.append(predictions)\n",
    "\n",
    "#         predictions = pd.concat(segments_predictions, axis=0)\n",
    "\n",
    "#         scores = {\n",
    "#             'validation': mean_absolute_percentage_error( \n",
    "#                 predictions.loc[is_validation, 'num_sold'],\n",
    "#                 predictions.loc[is_validation, 'yhat']\n",
    "#                 ),\n",
    "#             'train': mean_absolute_percentage_error( \n",
    "#                 predictions.loc[is_training, 'num_sold'],\n",
    "#                 predictions.loc[is_training, 'yhat']\n",
    "#                 )\n",
    "#             }\n",
    "        \n",
    "#         kfolds_evaluation.append(scores)\n",
    "\n",
    "#     score_overall = np.mean([ \n",
    "#         kfolds_evaluation[0]['validation'], \n",
    "#         kfolds_evaluation[1]['validation'] \n",
    "#         ])\n",
    "    \n",
    "#     return score_overall\n",
    "\n",
    "# study = optuna.create_study()\n",
    "\n",
    "# study.optimize(\n",
    "#     objective,\n",
    "#     n_trials=25,\n",
    "#     catch=(ValueError,),\n",
    "#     n_jobs=-1,\n",
    "#     timeout=60 * 10,\n",
    "# )\n",
    "\n",
    "# study.best_trial, study.best_params, study.best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RIDGE_ALPHA_TUNED = study.best_params['alpha']\n",
    "\n",
    "# from previous retail forecasting competitions' leaders,\n",
    "# plus theoretically expected heterogeneity between series: \n",
    "# one model per segment\n",
    "\n",
    "segments_XY = {grp: df for grp, df in XY.groupby('series_id')}\n",
    "\n",
    "# even when split into many dataframes, *index-based* subsets.\n",
    "# indexes maintained when dataframe splits.\n",
    "kfolds = [\n",
    "\n",
    "    # validation set 2014-16 matches ultimate test set's length, 2017-19\n",
    "    ( \n",
    "        ( (XY['date'] >= pd.to_datetime('2010-01-01')) & (XY['date'] < pd.to_datetime(\"2014-01-01\")) ),\n",
    "        XY['date'] >= pd.to_datetime(\"2014-01-01\")\n",
    "    ),\n",
    "\n",
    "    ( \n",
    "        ( (XY['date'] >= pd.to_datetime('2010-01-01')) & (XY['date'] < pd.to_datetime(\"2016-01-01\")) ),\n",
    "        XY['date'] >= pd.to_datetime(\"2016-01-01\")\n",
    "    )\n",
    "\n",
    "    ]\n",
    "\n",
    "kfolds_evaluation = []\n",
    "for is_training, is_validation in kfolds:\n",
    "\n",
    "    # per group: fit model on train, predict on validation\n",
    "    # WARNING: one model instance, then fitting it on each segment,\n",
    "    # does not imply separate model objects. they're all tied together.\n",
    "        # model = LinearRegression()\n",
    "    segments_models = {\n",
    "        grp: Ridge(RIDGE_ALPHA_TUNED).fit(\n",
    "            XY.loc[is_training, FEATURES_LOCAL_MODEL], \n",
    "            XY.loc[is_training, 'num_sold_log']\n",
    "            )\n",
    "        for grp, XY in segments_XY.items()\n",
    "        }\n",
    "\n",
    "    segments_predictions = [\n",
    "        df.assign(\n",
    "            yhat = lambda df_: np.exp(segments_models[grp].predict(df_[FEATURES_LOCAL_MODEL])) \n",
    "            )\n",
    "        for grp, df in segments_XY.items()\n",
    "        ]\n",
    "\n",
    "    predictions = pd.concat(segments_predictions, axis=0)\n",
    "\n",
    "    scores = {\n",
    "        'validation': mean_absolute_percentage_error( \n",
    "            predictions.loc[is_validation, 'num_sold'],\n",
    "            predictions.loc[is_validation, 'yhat']\n",
    "            ),\n",
    "        'train': mean_absolute_percentage_error( \n",
    "            predictions.loc[is_training, 'num_sold'],\n",
    "            predictions.loc[is_training, 'yhat']\n",
    "            )\n",
    "        }\n",
    "    \n",
    "    kfolds_evaluation.append(scores)\n",
    "\n",
    "kfolds_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_sample = predictions.query(\"series_id == 'Norway|Premium Sticker Mart|Kaggle'\")\n",
    "\n",
    "predictions_sample.set_index('date')[['num_sold', 'yhat']].plot.line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    predictions\n",
    "    .assign(mape = lambda df_: np.abs(100 * (df_['num_sold']/df_['yhat'] - 1)) )\n",
    "    .sort_values('mape', ascending=False)\n",
    "    .head(100)\n",
    "    .to_csv(\"./data/processed/predictions_errors_local_model.csv\", index=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    predictions\n",
    "    .assign(mape = lambda df_: np.abs(100 * (df_['num_sold']/df_['yhat'] - 1)) )\n",
    "    .sort_values('mape', ascending=False)\n",
    "    .head(50)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_sample = predictions.query(\"series_id == 'Norway|Premium Sticker Mart|Kaggle Tiers'\")\n",
    "\n",
    "predictions_sample.set_index('date')[['num_sold', 'yhat']].plot.line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_sample = predictions.query(\"series_id == 'Kenya|Stickers for Less|Holographic Goose'\")\n",
    "\n",
    "predictions_sample.set_index('date')[['num_sold', 'yhat']].plot.line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_XY = {grp: df for grp, df in XY.groupby('series_id')}\n",
    "\n",
    "# even when split into many dataframes, *index-based* subsets.\n",
    "# indexes maintained when dataframe splits.\n",
    "kfolds = [\n",
    "\n",
    "    # validation set 2014-16 matches ultimate test set's length, 2017-19\n",
    "    ( \n",
    "        ( (XY['date'] >= pd.to_datetime('2010-01-01')) & (XY['date'] < pd.to_datetime(\"2014-01-01\")) ),\n",
    "        XY['date'] >= pd.to_datetime(\"2014-01-01\")\n",
    "    ),\n",
    "\n",
    "    ( \n",
    "        ( (XY['date'] >= pd.to_datetime('2010-01-01')) & (XY['date'] < pd.to_datetime(\"2016-01-01\")) ),\n",
    "        XY['date'] >= pd.to_datetime(\"2016-01-01\")\n",
    "    )\n",
    "\n",
    "    ]\n",
    "\n",
    "kfolds_evaluation = []\n",
    "FEATURES_LOCAL_TREND_MODEL = FEATURES_LOCAL_MODEL\n",
    "for is_training, is_validation in kfolds:\n",
    "    \n",
    "    segments_models = {\n",
    "        grp: (\n",
    "            TrendRemainderModelPipeline(FEATURES_LOCAL_TREND_MODEL, RIDGE_ALPHA_TUNED, ['day_of_year'])\n",
    "            .fit(XY.loc[is_training, FEATURES_LOCAL_MODEL], XY.loc[is_training, 'num_sold_log'])\n",
    "            )\n",
    "        for grp, XY in segments_XY.items()\n",
    "        }\n",
    "\n",
    "    segments_predictions = [\n",
    "        df.assign(\n",
    "            yhat = lambda df_: np.exp(segments_models[grp].predict(df_[FEATURES_LOCAL_MODEL])) \n",
    "            )\n",
    "        for grp, df in segments_XY.items()\n",
    "        ]\n",
    "\n",
    "    predictions = pd.concat(segments_predictions, axis=0)\n",
    "\n",
    "    scores = {\n",
    "        'validation': mean_absolute_percentage_error( \n",
    "            predictions.loc[is_validation, 'num_sold'],\n",
    "            predictions.loc[is_validation, 'yhat']\n",
    "            ),\n",
    "        'train': mean_absolute_percentage_error( \n",
    "            predictions.loc[is_training, 'num_sold'],\n",
    "            predictions.loc[is_training, 'yhat']\n",
    "            )\n",
    "        }\n",
    "    \n",
    "    kfolds_evaluation.append(scores)\n",
    "\n",
    "kfolds_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_sample = predictions.query(\"series_id == 'Norway|Premium Sticker Mart|Kaggle'\")\n",
    "\n",
    "predictions_sample.set_index('date')[['num_sold', 'yhat']].plot.line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEGMENTS_MODELS = {\n",
    "    grp: Ridge(RIDGE_ALPHA_TUNED).fit(XY[FEATURES_LOCAL_MODEL], XY['num_sold_log'])\n",
    "    for grp, XY in segments_XY.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_test_daily = pd.read_csv(\"./data/external/test.csv\").assign(\n",
    "\n",
    "    date = lambda df_: pd.to_datetime(df_['date']),\n",
    "\n",
    "    country_store = lambda df_: df_['country'].str.cat(df_['store'], sep='|'),\n",
    "    country_product = lambda df_: df_['country'].str.cat(df_['product'], sep='|'),\n",
    "    store_product = lambda df_: df_['store'].str.cat(df_['product'], sep='|'),\n",
    "    country_store_product = lambda df_: df_['country'].str.cat([df_['store'], df_['product']], sep='|')\n",
    "\n",
    "    ).assign(series_id = lambda df_: df_['country_store_product'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_test_daily.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_test_daily['date'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_test_daily = transform_calendar_features(sales_test_daily)\n",
    "sales_test_daily = integrate_external_features(sales_test_daily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_test_daily = sales_test_daily.assign(\n",
    "    num_sold = None,\n",
    "    num_sold_log = None\n",
    ")\n",
    "sales_test_enriched_daily = feature_transform_pipeline.transform(sales_test_daily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_X_test = {grp: df for grp, df in sales_test_enriched_daily.groupby('series_id')}\n",
    "\n",
    "segments_predictions_test = []\n",
    "for grp, df in segments_X_test.items():\n",
    "\n",
    "    if grp in SEGMENTS_MODELS:\n",
    "        df = df.assign(\n",
    "            yhat = lambda df_: np.exp(SEGMENTS_MODELS[grp].predict(df_[FEATURES_LOCAL_MODEL])) \n",
    "            )  \n",
    "    else:\n",
    "        df = df.assign(\n",
    "            yhat = lambda df_: np.exp(model_global.predict(df_[FEATURES_GLOBAL_MODEL])) \n",
    "            )\n",
    "\n",
    "    segments_predictions_test.append(df)\n",
    "\n",
    "predictions_test = pd.concat(segments_predictions_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test_submit = (\n",
    "    predictions_test\n",
    "    [['id', 'yhat']]\n",
    "    .rename(columns={'yhat': 'num_sold'})\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert predictions_test_submit.shape[0] == 98_550\n",
    "assert predictions_test_submit.notnull().all().all()\n",
    "predictions_test_submit.to_csv(\"./data/processed/submission4.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forecast_stickers_kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
