{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import requests\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import optuna\n",
    "import xgboost as xgb\n",
    "from dateutil.easter import easter\n",
    "from datetime import timedelta\n",
    "import plotnine as p9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily = (\n",
    "    pd.read_csv(\"./data/external/train.csv\")\n",
    "    .assign(\n",
    "\n",
    "        date = lambda df_: pd.to_datetime(df_['date']),\n",
    "\n",
    "        country_store = lambda df_: df_['country'].str.cat(df_['store'], sep='|'),\n",
    "        country_product = lambda df_: df_['country'].str.cat(df_['product'], sep='|'),\n",
    "        store_product = lambda df_: df_['store'].str.cat(df_['product'], sep='|'),\n",
    "        country_store_product = lambda df_: df_['country'].str.cat([df_['store'], df_['product']], sep='|')\n",
    "\n",
    "    )\n",
    "    .assign(series_id = lambda df_: df_['country_store_product'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_gdp_per_capita(country_code, year):\n",
    "    \"\"\"\n",
    "    Adapted from https://www.kaggle.com/competitions/playground-series-s5e1/discussion/554349.\n",
    "    TODO: what about GDP level, which accounts also for population?\n",
    "\n",
    "    Source reference: https://datahelpdesk.worldbank.org/knowledgebase/articles/889392-about-the-indicators-api-documentation.\n",
    "    \"\"\"\n",
    "\n",
    "    url='https://api.worldbank.org/v2/country/{0}/indicator/NY.GDP.PCAP.CD?date={1}&format=json'\n",
    "    response = requests.get(url.format(country_code, year)).json()\n",
    "\n",
    "    return response[1][0]['value']\n",
    "\n",
    "# per CountryCode-year: request GDP per capita.\n",
    "# concatenate dataframe of CountryCode | Country | Year | GDP, for integration to Kaggle source\n",
    "\n",
    "countries_code_map = {\n",
    "    'Canada': 'CAN', \n",
    "    'Finland': 'FIN',\n",
    "    'Italy': 'ITA',\n",
    "    'Kenya': 'KEN',\n",
    "    'Norway': 'NOR',\n",
    "    'Singapore': 'SGP'\n",
    "    }\n",
    "\n",
    "countries_gdp_yearly = []\n",
    "for country_title, country_code in countries_code_map.items():\n",
    "    \n",
    "    values_yearly = [\n",
    "        {'year': i, 'gdp_per_capita': extract_gdp_per_capita(country_code, i)}\n",
    "        for i in range(2010, 2019+1)\n",
    "        ]\n",
    "    values_yearly = [pd.DataFrame(x, index=[0]) for x in values_yearly]\n",
    "    values_yearly = pd.concat(values_yearly, axis=0)\n",
    "    \n",
    "    values_yearly = (\n",
    "        values_yearly\n",
    "        .assign(\n",
    "            country = country_title,\n",
    "            country_code = country_code\n",
    "            )\n",
    "        .sort_values('year')\n",
    "        .assign(\n",
    "            gdp_per_capita_lag1 = lambda df_: df_['gdp_per_capita'].shift(1),\n",
    "            gdp_per_capita_lag2 = lambda df_: df_['gdp_per_capita'].shift(2),\n",
    "            gdp_per_capita_lag3 = lambda df_: df_['gdp_per_capita'].shift(3)\n",
    "            )\n",
    "        .assign(\n",
    "            # first observation won't have a preceding lag\n",
    "            is_recession = lambda df_: (df_['gdp_per_capita'] < df_['gdp_per_capita_lag1']).astype(int).fillna(0),\n",
    "            gdp_per_capita_growth1 = lambda df_: np.log(df_['gdp_per_capita']) - np.log(df_['gdp_per_capita_lag1']),\n",
    "            gdp_per_capita_growth2 = lambda df_: np.log(df_['gdp_per_capita']) - np.log(df_['gdp_per_capita_lag2']),\n",
    "            gdp_per_capita_growth3 = lambda df_: np.log(df_['gdp_per_capita']) - np.log(df_['gdp_per_capita_lag3'])\n",
    "            )\n",
    "        .assign(\n",
    "            # asymmetric effects of growth & contraction\n",
    "            gdp_per_capita_growth1_is_positive = lambda df_: np.where(df_['gdp_per_capita_growth1'] > 0, df_['gdp_per_capita_growth1'], 0),\n",
    "            gdp_per_capita_growth1_is_negative = lambda df_: np.where(df_['gdp_per_capita_growth1'] < 0, df_['gdp_per_capita_growth1'], 0),\n",
    "\n",
    "            gdp_per_capita_growth2_is_positive = lambda df_: np.where(df_['gdp_per_capita_growth2'] > 0, df_['gdp_per_capita_growth2'], 0),\n",
    "            gdp_per_capita_growth2_is_negative = lambda df_: np.where(df_['gdp_per_capita_growth2'] < 0, df_['gdp_per_capita_growth2'], 0),\n",
    "\n",
    "            gdp_per_capita_growth3_is_positive = lambda df_: np.where(df_['gdp_per_capita_growth3'] > 0, df_['gdp_per_capita_growth3'], 0),\n",
    "            gdp_per_capita_growth3_is_negative = lambda df_: np.where(df_['gdp_per_capita_growth3'] < 0, df_['gdp_per_capita_growth3'], 0),\n",
    "        )\n",
    "        )\n",
    "    \n",
    "    countries_gdp_yearly.append(values_yearly)\n",
    "\n",
    "    print(f\"{country_title} ({country_code}) GDP Per Capita extraction complete.\")\n",
    "\n",
    "countries_gdp_yearly = pd.concat(countries_gdp_yearly, axis=0)\n",
    "\n",
    "countries_gdp_yearly = (\n",
    "    countries_gdp_yearly\n",
    "    .assign(gdp_per_capita_log = lambda df_: np.log(df_['gdp_per_capita']))\n",
    "    )\n",
    "countries_gdp_yearly['gdp_per_capita_log^2'] = countries_gdp_yearly['gdp_per_capita_log']**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_gdp_yearly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    countries_gdp_yearly\n",
    "    [['year', 'country', 'is_recession', 'gdp_per_capita']]\n",
    "    .set_index(['country', 'year'])\n",
    "    .unstack(-1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    countries_gdp_yearly\n",
    "    .set_index('year')\n",
    "    .groupby('country')\n",
    "    ['gdp_per_capita']\n",
    "    .plot(legend=True, marker='o')\n",
    ")\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    countries_gdp_yearly\n",
    "    .query(\"country == 'Kenya'\")\n",
    "    .set_index('year')\n",
    "    .groupby('country')\n",
    "    ['gdp_per_capita']\n",
    "    .plot(legend=True)\n",
    ")\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_population(country_code, year):\n",
    "\n",
    "    url='https://api.worldbank.org/v2/country/{0}/indicator/SP.POP.TOTL?date={1}&format=json'\n",
    "    response = requests.get(url.format(country_code, year)).json()\n",
    "\n",
    "    return response[1][0]['value']\n",
    "\n",
    "# concatenate dataframe of CountryCode | Country | Year | KPI, for integration to Kaggle source\n",
    "\n",
    "countries_population_yearly = []\n",
    "for country_title, country_code in countries_code_map.items():\n",
    "    \n",
    "    values_yearly = [\n",
    "        {'year': i, 'population': extract_population(country_code, i)}\n",
    "        for i in range(2010, 2019+1)\n",
    "        ]\n",
    "    values_yearly = [pd.DataFrame(x, index=[0]) for x in values_yearly]\n",
    "    values_yearly = pd.concat(values_yearly, axis=0)\n",
    "    \n",
    "    values_yearly = (\n",
    "        values_yearly\n",
    "        .assign(\n",
    "            country = country_title,\n",
    "            country_code = country_code\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    countries_population_yearly.append(values_yearly)\n",
    "\n",
    "    print(f\"{country_title} ({country_code}) population extraction complete.\")\n",
    "\n",
    "countries_population_yearly = pd.concat(countries_population_yearly, axis=0)\n",
    "assert countries_population_yearly.notnull().all().all()\n",
    "\n",
    "countries_population_yearly = (\n",
    "    countries_population_yearly\n",
    "    .assign(population_log = lambda df_: np.log(df_['population']))\n",
    "    )\n",
    "countries_population_yearly['population_log^2'] = countries_population_yearly['population_log']**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    countries_population_yearly\n",
    "    .set_index('year')\n",
    "    .groupby('country')\n",
    "    ['population_log']\n",
    "    .plot(legend=True, marker='o')\n",
    ")\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: is there a better practice for holidays feature representation?\n",
    "    # in Hyndman's Electricity Load Forecasting Kaggle (https://robjhyndman.com/papers/kaggle-competition.pdf),\n",
    "    # \"holiday effect modelled with a factor variable, taking value zero on a non-work day,\n",
    "    # some non-zero value day before a non-work day, and a different value the day after a non-work day.\"\n",
    "    # meaning -- holiday days pooled, before & after estimated separately?\n",
    "\n",
    "def onehot_encode_dates_delta(dates_base_feature: pd.DataFrame, deltas_day: list):\n",
    "    \"\"\"\n",
    "    Given dates where `is_easter` (dates_base_feature): \n",
    "    - create the Easter-1 days, title `is_easter_sub1day`.\n",
    "    - create the Easter+1 days, title `is_easter_add1day`.\n",
    "    ... So on, for deltas_day beyond [-1, 1].\n",
    "\n",
    "    Motivated by analysis of model errors.\n",
    "    \"\"\"\n",
    "\n",
    "    base_feature_stem = list(dates_base_feature.columns)\n",
    "    base_feature_stem.remove('date')\n",
    "    base_feature_stem = base_feature_stem[0]\n",
    "\n",
    "    frames_date_deltas = [dates_base_feature]\n",
    "    for delta in deltas_day: \n",
    "\n",
    "        if delta < 0:\n",
    "            sgn = 'sub'\n",
    "        else:\n",
    "            sgn = 'add'\n",
    "\n",
    "        df_delta = (\n",
    "            dates_base_feature\n",
    "            .copy()\n",
    "            .assign(date = lambda df_: df_['date'] + timedelta(days=delta))\n",
    "            .rename(columns={base_feature_stem: f\"{base_feature_stem}_{sgn}{abs(delta)}\"})\n",
    "            )\n",
    "        \n",
    "        frames_date_deltas.append(df_delta)\n",
    "\n",
    "    dates_features = (\n",
    "        pd.concat(frames_date_deltas, axis=0)\n",
    "        .fillna(0)\n",
    "        .assign(date = lambda df_: pd.to_datetime(df_['date']))\n",
    "        )\n",
    "\n",
    "    assert dates_features['date'].is_unique\n",
    "\n",
    "    return dates_features\n",
    "\n",
    "\n",
    "days_easter0 = [easter(x) for x in range(2010, 2019+1)]\n",
    "days_easter = pd.DataFrame({'date': days_easter0}).assign(is_easter = 1)\n",
    "days_special_relative_easter = onehot_encode_dates_delta(days_easter, [i for i in range(2, 7+1)])\n",
    "FEATURES_EASTER = list(days_special_relative_easter.columns)\n",
    "FEATURES_EASTER.remove('date')\n",
    "\n",
    "days_new_years_eve = [pd.to_datetime(f\"{yr}-12-31\") for yr in range(2010, 2019+1)]\n",
    "days_new_years_eve = pd.DataFrame({'date': days_new_years_eve}).assign(is_new_years_eve = 1)\n",
    "days_special_relative_new_years_eve = onehot_encode_dates_delta(\n",
    "    days_new_years_eve, \n",
    "    [-3, -2, -1, 1, 2, 3]\n",
    "    )\n",
    "FEATURES_NEW_YEARS_EVE = list(days_special_relative_new_years_eve.columns)\n",
    "FEATURES_NEW_YEARS_EVE.remove('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_calendar_features(df):\n",
    "\n",
    "    df = (\n",
    "        df\n",
    "        .assign(\n",
    "            year = lambda df_: df_['date'].dt.year,\n",
    "            month = lambda df_: df_['date'].dt.month,\n",
    "            week_of_year = lambda df_: df_['date'].dt.isocalendar().week,\n",
    "            day_of_week = lambda df_: df_['date'].dt.day_name(),\n",
    "            # President's Day is the 'third Monday in February'\n",
    "            day_of_month = lambda df_: df_['date'].dt.day,\n",
    "            day_of_year = lambda df_: df_['date'].dt.dayofyear,\n",
    "            # week of month would be ambiguous because, one week may span 2 months,\n",
    "            days_since_start = lambda df_: (df['date'] - pd.to_datetime(\"2010-01-01\")).dt.days\n",
    "            )\n",
    "        .assign(\n",
    "            # TODO: are periodic feature transforms too rigid?\n",
    "\n",
    "            # as day_of_year rises, don't expect monotonic relationship with outcome.\n",
    "            # rather, expect periodic (sinusoidal) relationship.\n",
    "            # as sin(x) rises, so too does outcome ...\n",
    "            # ensure one cycle over one year.\n",
    "            # at baseline, one sinusoidal cycle occurs per 2π\n",
    "            day_of_year_sin = lambda df_: np.sin(df_['day_of_year'] * 2 * np.pi / 365),\n",
    "            day_of_year_cos = lambda df_: np.cos(df_['day_of_year'] * 2 * np.pi / 365),\n",
    "\n",
    "            day_of_month_sin = lambda df_: np.sin(df_['day_of_month'] * 2 * np.pi / 30),\n",
    "            day_of_month_cos = lambda df_: np.cos(df_['day_of_month'] * 2 * np.pi / 30),\n",
    "\n",
    "            # exploratory visuals suggest multi-year cycles.\n",
    "            # moreover, for out-of-sample predictions to follow a periodic dynamic,\n",
    "            # need a strong parameterized form like this.\n",
    "            days_since_start_2year_sin = lambda df_: np.sin(df_['days_since_start'] * 2 * np.pi / 730),\n",
    "            days_since_start_5year_sin = lambda df_: np.sin(df_['days_since_start'] * 2 * np.pi / (365*5)),\n",
    "\n",
    "            days_since_start_2year_cos = lambda df_: np.cos(df_['days_since_start'] * 2 * np.pi / 730),\n",
    "            days_since_start_5year_cos = lambda df_: np.cos(df_['days_since_start'] * 2 * np.pi / (365*5)),\n",
    "            )\n",
    "\n",
    "        )\n",
    "    \n",
    "    df = pd.merge(df, days_special_relative_easter, how='left')\n",
    "    assert df['is_easter'].notnull().any()\n",
    "    df[FEATURES_EASTER] = df[FEATURES_EASTER].fillna(0)\n",
    "\n",
    "    df = pd.merge(df, days_special_relative_new_years_eve, how='left')\n",
    "    assert df['is_new_years_eve'].notnull().any()\n",
    "    df[FEATURES_NEW_YEARS_EVE] = df[FEATURES_NEW_YEARS_EVE].fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def integrate_external_features(df):\n",
    "\n",
    "    df = pd.merge(df, countries_gdp_yearly, how='left')\n",
    "    assert df['gdp_per_capita'].notnull().all().all()\n",
    "\n",
    "    df = pd.merge(df, countries_population_yearly, how='left')\n",
    "    assert df['population'].notnull().all().all()\n",
    "\n",
    "    return df\n",
    "\n",
    "def transform_lagged_predictors(df):\n",
    "\n",
    "    # to ensure proper within-series outcome lags\n",
    "    # TODO: with lagged features coming into play, how to enforce proper order via indexes?\n",
    "    df = (\n",
    "        df\n",
    "        .sort_values(['series_id', 'date'])\n",
    "        .assign(\n",
    "            num_sold_lag1 = lambda df_: df_.groupby('series_id')['num_sold'].shift(1),\n",
    "            num_sold_lag7 = lambda df_: df_.groupby('series_id')['num_sold'].shift(7)\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    return df\n",
    "\n",
    "def transform_logs(df):\n",
    "\n",
    "    df = (\n",
    "        df\n",
    "        .assign(\n",
    "            num_sold_log = lambda df_: np.log(df_['num_sold']),\n",
    "            num_sold_lag1_log = lambda df_: np.log(df_['num_sold_lag1']),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "sales_daily = transform_calendar_features(sales_daily)\n",
    "sales_daily = integrate_external_features(sales_daily)\n",
    "sales_daily = transform_lagged_predictors(sales_daily)\n",
    "sales_daily = transform_logs(sales_daily)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description Report: \"Surface Properties\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily['id'].is_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Volumetric Analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily['series_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily['date'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily['series_id'].value_counts().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily['product'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily['country'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily['store'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fields' Types and Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily['date'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily['num_sold'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily.isnull().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# are null sales events concentrated on a particular date?\n",
    "# doesn't appear so\n",
    "sales_daily.query(\"num_sold.isnull()\")['date'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily.query(\"num_sold.isnull()\")['series_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily.groupby('series_id')['num_sold'].sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a naive model: series' historical average sales, daily\n",
    "\n",
    "is_training = (\n",
    "    (sales_daily['date'] >= pd.to_datetime('2010-01-01'))\n",
    "    & (sales_daily['date'] < pd.to_datetime(\"2014-01-01\"))\n",
    ")\n",
    "\n",
    "is_validation = sales_daily['date'] >= pd.to_datetime(\"2014-01-01\")\n",
    "\n",
    "# is_training.sum(), is_validation.sum()\n",
    "\n",
    "# with original train data: using strictly train segment, groupby average\n",
    "predictions_naive = (\n",
    "    sales_daily\n",
    "    .loc[is_training]\n",
    "    .groupby('series_id')\n",
    "    [['num_sold']]\n",
    "    .agg('mean')\n",
    "    .reset_index(drop=False)\n",
    ")\n",
    "\n",
    "predictions_naive_evaluate = pd.merge(\n",
    "    sales_daily.loc[is_validation],\n",
    "    predictions_naive.rename(columns={'num_sold': 'yhat'}),\n",
    "    how='left'\n",
    "    )\n",
    "\n",
    "# expect a couple series with all null\n",
    "predictions_naive.isnull().sum()\n",
    "# recommended in this discussion: https://www.kaggle.com/competitions/playground-series-s5e1/discussion/554553 \n",
    "predictions_naive_evaluate = predictions_naive_evaluate.dropna()\n",
    "\n",
    "mean_absolute_percentage_error(\n",
    "    predictions_naive_evaluate['num_sold'],\n",
    "    predictions_naive_evaluate['yhat']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_naive_evaluate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a naive model: series' historical average sales, daily\n",
    "\n",
    "is_training = (\n",
    "    (sales_daily['date'] >= pd.to_datetime('2013-01-01'))\n",
    "    & (sales_daily['date'] < pd.to_datetime(\"2014-01-01\"))\n",
    ")\n",
    "\n",
    "is_validation = sales_daily['date'] >= pd.to_datetime(\"2014-01-01\")\n",
    "\n",
    "# is_training.sum(), is_validation.sum()\n",
    "\n",
    "# with original train data: using strictly train segment, groupby average\n",
    "predictions_naive = (\n",
    "    sales_daily\n",
    "    .loc[is_training]\n",
    "    .groupby('series_id')\n",
    "    [['num_sold']]\n",
    "    .agg('mean')\n",
    "    .reset_index(drop=False)\n",
    ")\n",
    "\n",
    "predictions_naive_evaluate = pd.merge(\n",
    "    sales_daily.loc[is_validation],\n",
    "    predictions_naive.rename(columns={'num_sold': 'yhat'}),\n",
    "    how='left'\n",
    "    )\n",
    "\n",
    "# expect a couple series with all null\n",
    "predictions_naive.isnull().sum()\n",
    "# recommended in this discussion: https://www.kaggle.com/competitions/playground-series-s5e1/discussion/554553 \n",
    "predictions_naive_evaluate = predictions_naive_evaluate.dropna()\n",
    "\n",
    "mean_absolute_percentage_error(\n",
    "    predictions_naive_evaluate['num_sold'],\n",
    "    predictions_naive_evaluate['yhat']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_sample_daily = sales_daily.query(\"series_id == 'Norway|Premium Sticker Mart|Kaggle'\")\n",
    "\n",
    "(\n",
    "    sales_sample_daily\n",
    "    .loc[is_training]\n",
    "    [['date', 'num_sold']]\n",
    "    .set_index('date')\n",
    "    .plot\n",
    "    .line()\n",
    ")\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# descend level of abstraction -- \n",
    "\n",
    "    # across-year patterns (multi-year business cycles)\n",
    "    # within-year,\n",
    "        # month-of-year seasonality\n",
    "        # week-of-year seasonality\n",
    "        # day-of-month seasonality\n",
    "        # day-of-week seasonality\n",
    "\n",
    "# outcome vs predictors\n",
    "    # lagged outcome\n",
    "    # gdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    p9.ggplot(sales_daily.groupby(['year'])[['num_sold']].agg('sum').reset_index(drop=False)) + \n",
    "    p9.theme_bw() + \n",
    "    p9.geom_col(p9.aes('year', 'num_sold'), alpha=0.25)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    p9.ggplot(sales_daily.groupby(['country', 'year'])[['num_sold']].agg('sum').reset_index(drop=False)) + \n",
    "    p9.theme_bw() + \n",
    "    p9.geom_line(p9.aes('year', 'num_sold', group='country', color='country')) + \n",
    "    p9.theme(figure_size=(8,4))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    p9.ggplot(sales_daily.groupby(['date'])[['num_sold']].agg('sum').reset_index(drop=False)) + \n",
    "    p9.theme_bw() + \n",
    "    p9.geom_point(p9.aes('date', 'num_sold'), alpha=0.25)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    p9.ggplot(sales_daily) + \n",
    "    p9.theme_bw() + \n",
    "    p9.geom_boxplot(p9.aes('month', 'num_sold', group='month'), alpha=0.1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    p9.ggplot(sales_daily.query(\"series_id == 'Norway|Premium Sticker Mart|Kaggle'\")) + \n",
    "    p9.theme_bw() + \n",
    "    p9.geom_boxplot(p9.aes('month', 'num_sold', group='month'), alpha=0.1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    p9.ggplot((\n",
    "        sales_daily\n",
    "        .assign(day_of_week = lambda df_: pd.Categorical(\n",
    "            df_['day_of_week'], \n",
    "            ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']\n",
    "            ))\n",
    "    )) + \n",
    "    p9.theme_bw() + \n",
    "    p9.geom_boxplot(p9.aes('day_of_week', 'num_sold', group='day_of_week'), alpha=0.1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    p9.ggplot(sales_daily.query(\"series_id == 'Norway|Premium Sticker Mart|Kaggle'\")) + \n",
    "    p9.theme_bw() + \n",
    "    p9.geom_point(p9.aes('num_sold_lag1', 'num_sold'), alpha=0.1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    p9.ggplot(sales_daily.query(\"series_id == 'Norway|Premium Sticker Mart|Kaggle'\")) + \n",
    "    p9.theme_bw() + \n",
    "    p9.geom_point(p9.aes('num_sold_lag7', 'num_sold'), alpha=0.1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    p9.ggplot((\n",
    "        sales_daily\n",
    "        .assign(num_sold_log_mean = lambda df_: df_.groupby('series_id')['num_sold_log'].transform('mean'))\n",
    "        .assign(num_sold_log_demean = lambda df_: df_['num_sold_log'] - df_['num_sold_log_mean'])\n",
    "        )) + \n",
    "    p9.theme_bw() + \n",
    "    p9.geom_point(p9.aes('gdp_per_capita_log', 'num_sold_log_demean'), alpha=0.5) + \n",
    "    p9.geom_smooth(p9.aes('gdp_per_capita_log', 'num_sold_log_demean', group='country', color='country'), method='lowess')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features set varies by model -- _global_ and _local_. \n",
    "\n",
    "\"Universe\" implies, all features that could be used. A subset enters into each \"building block\" of the overall modeling system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_UNIVERSE_TO_ONEHOT = [\n",
    "    'country', \n",
    "    'store',\n",
    "    'product',\n",
    "    'country_store',\n",
    "    'country_product',\n",
    "    'store_product',\n",
    "    # omit country-store-product, that's perfect collinearity with above.\n",
    "    # year attempted, but then omitted. Because \n",
    "    # exogenous factors should help explain year-to-year shifts,\n",
    "    # so that out-of-sample years' forecasts aren't flat\n",
    "    'month', \n",
    "    'week_of_year', \n",
    "    'day_of_week'\n",
    "    ]\n",
    "\n",
    "# local model fits by country-store-product segment, \n",
    "# so those onehots would be invariant\n",
    "FEATURES_LOCAL_MODEL_TO_ONEHOT = [\n",
    "    x for x in FEATURES_UNIVERSE_TO_ONEHOT \n",
    "    if not any(stem in x for stem in ['country', 'store', 'product'])\n",
    "    ]\n",
    "\n",
    "FEATURES_UNIVERSE_NUMERIC_CONTINUOUS = [\n",
    "    'gdp_per_capita_log',\n",
    "    'gdp_per_capita_log^2',\n",
    "    'population_log',\n",
    "    'day_of_month', \n",
    "    'day_of_month_sin',\n",
    "    'day_of_month_cos',\n",
    "    'day_of_year_sin',\n",
    "    'day_of_year_cos',\n",
    "    'day_of_year',\n",
    "    'days_since_start_2year_sin',\n",
    "    'days_since_start_2year_cos',\n",
    "    'days_since_start'\n",
    "    ]\n",
    "# from experimentation, tree-based models' out-of-sample forecasts flat\n",
    "# when incorporating days_since_start based feature. (or onehot-encoded year)\n",
    "\n",
    "FEATURES_UNIVERSE_ALREADY_ONEHOT = FEATURES_EASTER + FEATURES_NEW_YEARS_EVE\n",
    "\n",
    "FEATURES_AUTOREGRESSIVE = ['num_sold_lag1_log']\n",
    "\n",
    "FEATURES_GLOBAL_MODEL = (\n",
    "    FEATURES_UNIVERSE_TO_ONEHOT + \n",
    "    FEATURES_UNIVERSE_NUMERIC_CONTINUOUS + \n",
    "    FEATURES_UNIVERSE_ALREADY_ONEHOT\n",
    "    )\n",
    "\n",
    "FEATURES_LOCAL_MODEL = (\n",
    "    FEATURES_LOCAL_MODEL_TO_ONEHOT + \n",
    "    FEATURES_UNIVERSE_NUMERIC_CONTINUOUS + \n",
    "    FEATURES_UNIVERSE_ALREADY_ONEHOT\n",
    "    )\n",
    "\n",
    "ATTRIBUTES = ['series_id', 'date', 'id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_daily_complete = sales_daily.dropna(subset=['num_sold', 'num_sold_lag1_log'])\n",
    "\n",
    "# shorter alias\n",
    "XY = sales_daily_complete\n",
    "\n",
    "# from previous retail forecasting competitions' leaders,\n",
    "# plus theoretically expected heterogeneity between series: \n",
    "# one model per segment\n",
    "\n",
    "segments_XY = {grp: df for grp, df in XY.groupby('series_id')}\n",
    "\n",
    "# have gone back-and-forth about the validation set decision.\n",
    "# validation set 2014-16 ensures, more consistently accurate model (trend-cycle component, especially).\n",
    "# validation 2016 ensures, higher accuracy on data very recent to 2017-19 test.\n",
    "    # *if relationships are time-varying*, and sample size insufficient to explicitly estimate time-varying parameters -- \n",
    "    # then prefer to upweight data more recent to test period. \n",
    "KFOLDS_ALTERNATIVES = {\n",
    "\n",
    "    'validate_2014_2016': ( \n",
    "        ( (XY['date'] >= pd.to_datetime('2010-01-01')) & (XY['date'] < pd.to_datetime(\"2014-01-01\")) ),\n",
    "        XY['date'] >= pd.to_datetime(\"2014-01-01\")\n",
    "        ),\n",
    "\n",
    "    'validate_2015_2016': ( \n",
    "        ( (XY['date'] >= pd.to_datetime('2010-01-01')) & (XY['date'] < pd.to_datetime(\"2015-01-01\")) ),\n",
    "        XY['date'] >= pd.to_datetime(\"2015-01-01\")\n",
    "        ),\n",
    "\n",
    "    'validate_2016': ( \n",
    "        ( (XY['date'] >= pd.to_datetime('2010-01-01')) & (XY['date'] < pd.to_datetime(\"2016-01-01\")) ),\n",
    "        XY['date'] >= pd.to_datetime(\"2016-01-01\")\n",
    "        )\n",
    "\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "class TrendRemainderModelPipeline:\n",
    "    \"\"\"\n",
    "    Y = TrendComponent + RemainderComponent\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        trend_level_feature_transformer: ColumnTransformer,\n",
    "        trend_model_ridge_alpha,\n",
    "        remainder_feature_transformer: ColumnTransformer,\n",
    "        remainder_model_kind: Literal[\"random_forest\", \"xgboost\"],\n",
    "        remainder_model_xgboost_rounds_count=1_000,\n",
    "        remainder_model_xgboost_param=None\n",
    "        ):\n",
    "\n",
    "        self.trend_level_feature_transformer = trend_level_feature_transformer\n",
    "        self.trend_level_model_ridge_alpha = trend_model_ridge_alpha\n",
    "\n",
    "        self.remainder_feature_transformer = remainder_feature_transformer\n",
    "        self.remainder_model_kind = remainder_model_kind\n",
    "        \n",
    "        self.remainder_model_xgboost_rounds_count = remainder_model_xgboost_rounds_count\n",
    "        self.remainder_model_xgboost_param = remainder_model_xgboost_param\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        self.fit_trend_level_model(X, y)\n",
    "        yhat_trend_level = self.predict_trend_level_model(X)\n",
    "        y_detrended = y - yhat_trend_level\n",
    "\n",
    "        self.fit_remainder_model(X, y_detrended)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        yhat_trend_level = self.predict_trend_level_model(X)\n",
    "        yhat_remainder = self.predict_remainder_model(X)\n",
    "        preds = yhat_trend_level + yhat_remainder\n",
    "\n",
    "        return preds\n",
    "\n",
    "    def fit_trend_level_model(self, X, y):\n",
    "        \"\"\"\n",
    "        'Weak learner', strictly intended to explain trend (level) shifts between years.\n",
    "        \"\"\"\n",
    "\n",
    "        X = self.trend_level_feature_transformer.fit_transform(X)\n",
    "\n",
    "        model_trend_level = Ridge(self.trend_level_model_ridge_alpha)\n",
    "        model_trend_level.fit(X, y)\n",
    "\n",
    "        self.trend_level_model = model_trend_level\n",
    "\n",
    "    def _fit_xgboost_remainder_model(self, X, y):\n",
    "        \"\"\"\n",
    "        Use xgboost-optimized data structures.\n",
    "        Precondition: transformed X\n",
    "        \"\"\"\n",
    "\n",
    "        dtrain = xgb.DMatrix(X, label=y)\n",
    "\n",
    "        self.remainder_model = xgb.train(\n",
    "            self.remainder_model_xgboost_param, \n",
    "            dtrain, \n",
    "            self.remainder_model_xgboost_rounds_count\n",
    "            )\n",
    "\n",
    "    def _fit_random_forest_remainder_model(self, X, y):\n",
    "        \"\"\"Precondition: transformed X\"\"\"\n",
    "\n",
    "        self.remainder_model = RandomForestRegressor(n_estimators=100, n_jobs=-1)\n",
    "        self.remainder_model.fit(X, y)\n",
    "\n",
    "    def fit_remainder_model(self, X, y):\n",
    "\n",
    "        X = self.remainder_feature_transformer.fit_transform(X)\n",
    "\n",
    "        if self.remainder_model_kind == 'xgboost':\n",
    "            self._fit_xgboost_remainder_model(X, y)\n",
    "\n",
    "        elif self.remainder_model_kind == 'random_forest':\n",
    "            self._fit_random_forest_remainder_model(X, y)\n",
    "\n",
    "    def predict_trend_level_model(self, X):\n",
    "        X = self.trend_level_feature_transformer.transform(X)\n",
    "        preds = self.trend_level_model.predict(X) \n",
    "        return preds\n",
    "    \n",
    "    def predict_remainder_model(self, X):\n",
    "\n",
    "        X = self.remainder_feature_transformer.transform(X)\n",
    "        if self.remainder_model_kind == 'xgboost':\n",
    "            X = xgb.DMatrix(X)\n",
    "\n",
    "        preds = self.remainder_model.predict(X)\n",
    "\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trend-Cycle Component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strongly parametric form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for model to use gdp_per_capita_log (annual measurement),\n",
    "# cannot include year. otherwise, year would partial out gdp\n",
    "FEATURES_TREND_CYCLE_NUMERIC_CONTINUOUS = [\n",
    "    'gdp_per_capita_log',\n",
    "    'gdp_per_capita_log^2',\n",
    "    # experimented with gdp_per_capita_growth lags 1-3,\n",
    "    # but these *severely* overfit! \n",
    "    # TODO: some feature for expansionary vs contractionary periods?\n",
    "    # 'gdp_per_capita_growth1_is_positive',\n",
    "    # 'gdp_per_capita_growth1_is_negative',\n",
    "    # 'gdp_per_capita_growth2_is_positive',\n",
    "    # 'gdp_per_capita_growth2_is_negative',\n",
    "    # 'gdp_per_capita_growth3_is_positive',\n",
    "    # 'gdp_per_capita_growth3_is_negative',\n",
    "\n",
    "    'population_log',\n",
    "    'population_log^2',\n",
    "    \n",
    "    'days_since_start_2year_sin',\n",
    "    'days_since_start_2year_cos',\n",
    "    # tested `year`, that did not improve\n",
    "    # tested 5-year cycle inclusion -- model worsened.\n",
    "]\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    parameters = {'alpha': trial.suggest_float(\"alpha\", 1e-1, 1_000, log=True)}\n",
    "\n",
    "    kfolds_evaluation = []\n",
    "    for is_training, is_validation in [KFOLDS_ALTERNATIVES['validate_2016']]:\n",
    "\n",
    "        segments_models = {}\n",
    "        segments_predictions = []\n",
    "\n",
    "        for grp, XY_grp in segments_XY.items():\n",
    "\n",
    "            feature_transform_pipeline_local_model = ColumnTransformer([\n",
    "                # ('transformer_onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), ['year']),\n",
    "                ('transformer_std', StandardScaler(), FEATURES_TREND_CYCLE_NUMERIC_CONTINUOUS)\n",
    "                ],\n",
    "                verbose_feature_names_out=False,\n",
    "                remainder='passthrough'\n",
    "                )\n",
    "            feature_transform_pipeline_local_model.set_output(transform='pandas')\n",
    "\n",
    "            pipeline_e2e = Pipeline([\n",
    "                ('transform_features', feature_transform_pipeline_local_model), \n",
    "                ('model', Ridge(**parameters))\n",
    "                ])\n",
    "\n",
    "            pipeline_e2e.fit(\n",
    "                XY_grp.loc[is_training, FEATURES_TREND_CYCLE_NUMERIC_CONTINUOUS],\n",
    "                XY_grp.loc[is_training, 'num_sold_log']\n",
    "                )\n",
    "            \n",
    "            segments_models[grp] = pipeline_e2e\n",
    "\n",
    "            predictions = (\n",
    "                XY_grp\n",
    "                .copy()\n",
    "                .assign(yhat = lambda df_: np.exp(pipeline_e2e.predict(df_)))\n",
    "                )\n",
    "            \n",
    "            segments_predictions.append(predictions)\n",
    "\n",
    "\n",
    "        predictions = pd.concat(segments_predictions, axis=0)\n",
    "        scores = {\n",
    "            'validation': mean_absolute_percentage_error( \n",
    "                predictions.loc[is_validation, 'num_sold'],\n",
    "                predictions.loc[is_validation, 'yhat']\n",
    "                ),\n",
    "            'train': mean_absolute_percentage_error( \n",
    "                predictions.loc[is_training, 'num_sold'],\n",
    "                predictions.loc[is_training, 'yhat']\n",
    "                )\n",
    "            }\n",
    "        \n",
    "        kfolds_evaluation.append(scores)\n",
    "\n",
    "    score_overall = np.mean([ \n",
    "        kfolds_evaluation[0]['validation'], \n",
    "        # kfolds_evaluation[1]['validation'] \n",
    "        ])\n",
    "    \n",
    "    return score_overall\n",
    "\n",
    "study = optuna.create_study()\n",
    "\n",
    "study.optimize(\n",
    "    objective,\n",
    "    n_trials=50,\n",
    "    catch=(ValueError,),\n",
    "    n_jobs=-1,\n",
    "    timeout=12 * 60 * 60,\n",
    ")\n",
    "\n",
    "RIDGE_ALPHA_TREND_CYCLE_LOCAL_MODELS = study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_trial, study.best_params, study.best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfolds_evaluation = []\n",
    "for is_training, is_validation in [KFOLDS_ALTERNATIVES['validate_2016']]:\n",
    "\n",
    "    segments_models = {}\n",
    "    segments_predictions = []\n",
    "\n",
    "    for grp, XY_grp in segments_XY.items():\n",
    "\n",
    "        feature_transform_pipeline_local_model = ColumnTransformer([\n",
    "            # ('transformer_onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), ['year']),\n",
    "            ('transformer_std', StandardScaler(), FEATURES_TREND_CYCLE_NUMERIC_CONTINUOUS)\n",
    "            ],\n",
    "            verbose_feature_names_out=False,\n",
    "            remainder='passthrough'\n",
    "            )\n",
    "        feature_transform_pipeline_local_model.set_output(transform='pandas')\n",
    "\n",
    "        pipeline_e2e = Pipeline([\n",
    "            ('transform_features', feature_transform_pipeline_local_model), \n",
    "            ('model', Ridge(**RIDGE_ALPHA_TREND_CYCLE_LOCAL_MODELS))\n",
    "            ])\n",
    "\n",
    "        pipeline_e2e.fit(\n",
    "            XY_grp.loc[is_training, FEATURES_TREND_CYCLE_NUMERIC_CONTINUOUS],\n",
    "            XY_grp.loc[is_training, 'num_sold_log']\n",
    "            )\n",
    "        \n",
    "        segments_models[grp] = pipeline_e2e\n",
    "\n",
    "        predictions = (\n",
    "            XY_grp\n",
    "            .copy()\n",
    "            .assign(yhat = lambda df_: np.exp(pipeline_e2e.predict(df_)))\n",
    "            )\n",
    "        segments_predictions.append(predictions)\n",
    "\n",
    "    predictions = pd.concat(segments_predictions, axis=0)\n",
    "\n",
    "    scores = {\n",
    "        'validation': {\n",
    "            'nobs': predictions.loc[is_validation].shape[0],\n",
    "            'score': mean_absolute_percentage_error( \n",
    "                predictions.loc[is_validation, 'num_sold'],\n",
    "                predictions.loc[is_validation, 'yhat']\n",
    "                )\n",
    "            },\n",
    "        'train': {\n",
    "            'nobs': predictions.loc[is_training].shape[0],\n",
    "            'score': mean_absolute_percentage_error( \n",
    "                predictions.loc[is_training, 'num_sold'],\n",
    "                predictions.loc[is_training, 'yhat']\n",
    "                )\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    kfolds_evaluation.append(scores)\n",
    "\n",
    "kfolds_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = segments_models['Kenya|Premium Sticker Mart|Kaggle'].named_steps.model\n",
    "pd.Series(model.coef_, index=model.feature_names_in_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = (\n",
    "    predictions\n",
    "    .assign(ape = lambda df_: abs(100 * (df_['num_sold'] / df_['yhat'] - 1)))\n",
    "    .assign(pe = lambda df_: (100 * (df_['num_sold'] / df_['yhat'] - 1)))\n",
    "    .assign(residual = lambda df_: df_['num_sold'] - df_['yhat'])\n",
    "    .assign(residual_log = lambda df_: df_['num_sold_log'] - np.log(df_['yhat']))\n",
    "    .assign(residual_abs = lambda df_: np.abs(df_['residual']))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    predictions\n",
    "    .groupby('country')\n",
    "    ['ape']\n",
    "    .agg(['mean', 'size'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    predictions\n",
    "    .groupby('year')\n",
    "    ['ape']\n",
    "    .agg(['mean', 'size'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    predictions\n",
    "    .groupby(['country', 'year'])\n",
    "    ['ape']\n",
    "    .agg(['mean', 'size'])\n",
    "    .unstack(-1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looked previously at AVG(residual) by year,\n",
    "# but that allows large errors to wash out -- not the right metric for this contest!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    predictions\n",
    "    .groupby(['country', 'year'])\n",
    "    ['pe']\n",
    "    .agg(['mean', 'size'])\n",
    "    .unstack(-1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    predictions\n",
    "    .groupby(['country', 'year'])\n",
    "    ['pe']\n",
    "    .agg(['median', 'size'])\n",
    "    .unstack(-1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_sample = predictions.query(\"series_id == 'Norway|Premium Sticker Mart|Kaggle'\")\n",
    "\n",
    "predictions_sample.set_index('date')[['num_sold', 'yhat']].plot.line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_sample = predictions.query(\"series_id == 'Kenya|Stickers for Less|Holographic Goose'\")\n",
    "\n",
    "predictions_sample.set_index('date')[['num_sold', 'yhat']].plot.line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    predictions_sample\n",
    "    .groupby('year')\n",
    "    [['residual']]\n",
    "    .agg([np.std, 'mean'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nonparametric form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURES_TREND_CYCLE = [\n",
    "#     'gdp_per_capita_log',\n",
    "#     # 'days_since_start_2year_sin',\n",
    "#     # 'days_since_start_5year_sin',\n",
    "#     # 'days_since_start_2year_cos',\n",
    "#     # 'days_since_start_5year_cos',\n",
    "#     # 'year'\n",
    "#     ]\n",
    "\n",
    "# kfolds = [\n",
    "\n",
    "#     # validation set 2014-16 matches ultimate test set's length, 2017-19\n",
    "#     ( \n",
    "#         ( (XY['date'] >= pd.to_datetime('2010-01-01')) & (XY['date'] < pd.to_datetime(\"2014-01-01\")) ),\n",
    "#         XY['date'] >= pd.to_datetime(\"2014-01-01\")\n",
    "#     ),\n",
    "\n",
    "#     # ( \n",
    "#     #     ( (XY['date'] >= pd.to_datetime('2010-01-01')) & (XY['date'] < pd.to_datetime(\"2016-01-01\")) ),\n",
    "#     #     XY['date'] >= pd.to_datetime(\"2016-01-01\")\n",
    "#     # )\n",
    "\n",
    "#     ]\n",
    "\n",
    "# def objective(trial):\n",
    "\n",
    "#     ROUNDS_COUNT = 1_000\n",
    "\n",
    "#     param = {\n",
    "#         \"objective\": \"reg:squarederror\",\n",
    "#         \"booster\": trial.suggest_categorical(\n",
    "#             \"booster\", [\"gbtree\", \"gblinear\", \"dart\"]\n",
    "#         ),\n",
    "#         \"lambda\": trial.suggest_float(\"lambda\", 1e-4, 1.0, log=True),\n",
    "#         \"alpha\": trial.suggest_float(\"alpha\", 1e-4, 1.0, log=True),\n",
    "#         \"subsample\": trial.suggest_float(\"subsample\", 0.4, 1.0),\n",
    "#         \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n",
    "#     }\n",
    "\n",
    "#     if param[\"booster\"] in [\"gbtree\", \"dart\"]:\n",
    "#         param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 1, 9, step=2)\n",
    "\n",
    "#         param[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 1, 10)\n",
    "#         param[\"eta\"] = trial.suggest_float(\"eta\", 1e-4, 0.01, log=True)\n",
    "\n",
    "#         param[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-4, 1.0, log=True)\n",
    "#         param[\"grow_policy\"] = trial.suggest_categorical(\n",
    "#             \"grow_policy\", [\"depthwise\", \"lossguide\"]\n",
    "#         )\n",
    "\n",
    "#     if param[\"booster\"] == \"dart\":\n",
    "#         param[\"sample_type\"] = trial.suggest_categorical(\n",
    "#             \"sample_type\", [\"uniform\", \"weighted\"]\n",
    "#         )\n",
    "#         param[\"normalize_type\"] = trial.suggest_categorical(\n",
    "#             \"normalize_type\", [\"tree\", \"forest\"]\n",
    "#         )\n",
    "#         param[\"rate_drop\"] = trial.suggest_float(\n",
    "#             \"rate_drop\", 1e-4, 1.0, log=True\n",
    "#         )\n",
    "#         param[\"skip_drop\"] = trial.suggest_float(\n",
    "#             \"skip_drop\", 1e-4, 1.0, log=True\n",
    "#         )\n",
    "\n",
    "#     kfolds_evaluation = []\n",
    "#     for is_training, is_validation in kfolds:\n",
    "\n",
    "#         segments_models = {}\n",
    "#         segments_predictions = []\n",
    "#         for grp, XY_grp in segments_XY.items():\n",
    "\n",
    "#             # feature_transform_pipeline_local_model = ColumnTransformer([\n",
    "#             #     ('transformer_onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), ['year']),\n",
    "#             #     # ('transformer_std', StandardScaler(), FEATURES_UNIVERSE_NUMERIC_CONTINUOUS)\n",
    "#             #     ],\n",
    "#             #     verbose_feature_names_out=False,\n",
    "#             #     # caller expected to provide some features that require no transforms.\n",
    "#             #     # don't drop those just because they received no special operations.\n",
    "#             #     # but do be sure to drop non-features in advance!\n",
    "#             #     remainder='passthrough'\n",
    "#             #     )\n",
    "#             # feature_transform_pipeline_local_model.set_output(transform='pandas')\n",
    "\n",
    "#             # X_train = (\n",
    "#             #     feature_transform_pipeline_local_model\n",
    "#             #     .fit_transform(XY_grp.loc[is_training, FEATURES_TREND_CYCLE])\n",
    "#             #     )\n",
    "#             dtrain = xgb.DMatrix(\n",
    "#                 XY_grp.loc[is_training, FEATURES_TREND_CYCLE], \n",
    "#                 label=XY_grp.loc[is_training, 'num_sold_log']\n",
    "#                 )\n",
    "            \n",
    "#             # X_test = feature_transform_pipeline_local_model.transform(XY_grp)\n",
    "#             X_test = XY_grp[FEATURES_TREND_CYCLE].copy()\n",
    "#             dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "#             model_local = xgb.train(param, dtrain, ROUNDS_COUNT)\n",
    "            \n",
    "#             yhat_log = model_local.predict(dtest)\n",
    "#             predictions = (\n",
    "#                 XY_grp\n",
    "#                 .copy()\n",
    "#                 .assign(yhat = np.exp(yhat_log))\n",
    "#                 )\n",
    "\n",
    "#             segments_predictions.append(predictions)\n",
    "\n",
    "#         predictions = pd.concat(segments_predictions, axis=0)\n",
    "\n",
    "#         scores = {\n",
    "#             'validation': mean_absolute_percentage_error( \n",
    "#                 predictions.loc[is_validation, 'num_sold'],\n",
    "#                 predictions.loc[is_validation, 'yhat']\n",
    "#                 ),\n",
    "#             'train': mean_absolute_percentage_error( \n",
    "#                 predictions.loc[is_training, 'num_sold'],\n",
    "#                 predictions.loc[is_training, 'yhat']\n",
    "#                 )\n",
    "#             }\n",
    "        \n",
    "#         kfolds_evaluation.append(scores)\n",
    "\n",
    "#     score_overall = np.mean([ \n",
    "#         kfolds_evaluation[0]['validation'], \n",
    "#         # kfolds_evaluation[1]['validation'] \n",
    "#         ])\n",
    "    \n",
    "#     return score_overall\n",
    "\n",
    "# study = optuna.create_study()\n",
    "\n",
    "# study.optimize(\n",
    "#     objective,\n",
    "#     n_trials=10,\n",
    "#     catch=(ValueError,),\n",
    "#     n_jobs=-1,\n",
    "#     timeout=60 * 10,\n",
    "# )\n",
    "\n",
    "# study.best_trial, study.best_params, study.best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROUNDS_COUNT = 500\n",
    "# # from optuna study partially completed\n",
    "# param = {\n",
    "#     'booster': 'gbtree', \n",
    "#     'lambda': 0.0028985936219238853, \n",
    "#     'alpha': 0.002145256181191053, \n",
    "#     'subsample': 0.4802539115271221, \n",
    "#     'colsample_bytree': 0.41474812246756865, \n",
    "#     'max_depth': 3, \n",
    "#     'min_child_weight': 10, \n",
    "#     'eta': 0.009889883032905597, \n",
    "#     'gamma': 0.0007618860150049975, \n",
    "#     'grow_policy': 'depthwise'\n",
    "#     }\n",
    "# # from optuna study with strictly gdp feature\n",
    "# # {'booster': 'gbtree',\n",
    "# #   'lambda': 0.15007680125462597,\n",
    "# #   'alpha': 0.0015129996967046535,\n",
    "# #   'subsample': 0.8744649731277182,\n",
    "# #   'colsample_bytree': 0.8420779252737395,\n",
    "# #   'max_depth': 5,\n",
    "# #   'min_child_weight': 6,\n",
    "# #   'eta': 0.0036670930558455332,\n",
    "# #   'gamma': 0.00032938064690042157,\n",
    "# #   'grow_policy': 'depthwise'}\n",
    "# FEATURES_TREND_CYCLE = [\n",
    "#     'gdp_per_capita_log',\n",
    "#     # 'days_since_start_2year_sin',\n",
    "#     # 'days_since_start_5year_sin',\n",
    "#     # 'days_since_start_2year_cos',\n",
    "#     # 'days_since_start_5year_cos'\n",
    "#     ]\n",
    "\n",
    "# kfolds_evaluation = []\n",
    "# for is_training, is_validation in kfolds:\n",
    "\n",
    "#     segments_models = {}\n",
    "#     segments_predictions = []\n",
    "#     i = 0\n",
    "#     segments_total = len(segments_XY)\n",
    "\n",
    "#     for grp, XY_grp in segments_XY.items():\n",
    "\n",
    "#         feature_transform_pipeline_local_model = ColumnTransformer([\n",
    "#             # ('transformer_onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), ['year']),\n",
    "#             ('transformer_std', StandardScaler(), FEATURES_TREND_CYCLE)\n",
    "#             ],\n",
    "#             verbose_feature_names_out=False,\n",
    "#             # caller expected to provide some features that require no transforms.\n",
    "#             # don't drop those just because they received no special operations.\n",
    "#             # but do be sure to drop non-features in advance!\n",
    "#             remainder='passthrough'\n",
    "#             )\n",
    "#         feature_transform_pipeline_local_model.set_output(transform='pandas')\n",
    "\n",
    "#         X_train = (\n",
    "#             feature_transform_pipeline_local_model\n",
    "#             .fit_transform(XY_grp.loc[is_training, FEATURES_TREND_CYCLE])\n",
    "#             )\n",
    "#         dtrain = xgb.DMatrix(X_train, label=XY_grp.loc[is_training, 'num_sold_log'])\n",
    "        \n",
    "#         X_test = feature_transform_pipeline_local_model.transform(XY_grp)\n",
    "#         dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "#         model_local = xgb.train(param, dtrain, ROUNDS_COUNT)\n",
    "        \n",
    "#         yhat_log = model_local.predict(dtest)\n",
    "#         predictions = (\n",
    "#             XY_grp\n",
    "#             .copy()\n",
    "#             .assign(yhat = np.exp(yhat_log))\n",
    "#             )\n",
    "\n",
    "#         segments_predictions.append(predictions)\n",
    "\n",
    "#         i += 1\n",
    "#         print(f\"{i}/{segments_total} models complete.\")\n",
    "\n",
    "#     predictions = pd.concat(segments_predictions, axis=0)\n",
    "\n",
    "#     scores = {\n",
    "#         'validation': mean_absolute_percentage_error( \n",
    "#             predictions.loc[is_validation, 'num_sold'],\n",
    "#             predictions.loc[is_validation, 'yhat']\n",
    "#             ),\n",
    "#         'train': mean_absolute_percentage_error( \n",
    "#             predictions.loc[is_training, 'num_sold'],\n",
    "#             predictions.loc[is_training, 'yhat']\n",
    "#             )\n",
    "#         }\n",
    "    \n",
    "#     kfolds_evaluation.append(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kfolds_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions_sample = predictions.query(\"series_id == 'Norway|Premium Sticker Mart|Kaggle'\")\n",
    "\n",
    "# predictions_sample.set_index('date')[['num_sold', 'yhat']].plot.line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_local.get_score(importance_type='gain')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Seasonal & Remainder Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_SEASONALITY_TO_ONEHOT = [\n",
    "    'month', \n",
    "    'week_of_year',\n",
    "    'day_of_week',\n",
    "    ]\n",
    "\n",
    "FEATURES_SEASONALITY_NUMERIC_CONTINUOUS = [\n",
    "    'year', # allow seasonality to evolve over time\n",
    "    'day_of_month', \n",
    "    'day_of_month_sin',\n",
    "    'day_of_month_cos',\n",
    "    'day_of_year_sin',\n",
    "    'day_of_year_cos',\n",
    "    'day_of_year'\n",
    "    ]\n",
    "\n",
    "FEATURES_SELECT = (\n",
    "    FEATURES_TREND_CYCLE_NUMERIC_CONTINUOUS + \n",
    "    FEATURES_SEASONALITY_TO_ONEHOT + \n",
    "    FEATURES_SEASONALITY_NUMERIC_CONTINUOUS + \n",
    "    FEATURES_UNIVERSE_ALREADY_ONEHOT\n",
    "    )\n",
    "\n",
    "# from preceding optuna run\n",
    "REMAINDER_MODEL_XGBOOST_ROUNDS_COUNT = 1_000\n",
    "REMAINDER_XGBOOST_MODEL_BEST_PARAMS = {\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"booster\": \"gbtree\",\n",
    "    'max_depth': 1,\n",
    "    'subsample': 0.8046714215178343,\n",
    "    'colsample_bytree': 0.5340615124161192\n",
    "    }\n",
    "\n",
    "kfolds_evaluation = []\n",
    "for is_training, is_validation in [KFOLDS_ALTERNATIVES['validate_2016']]:\n",
    "\n",
    "    segments_models = {}\n",
    "    segments_predictions = []\n",
    "\n",
    "    for grp, XY_grp in segments_XY.items():\n",
    "\n",
    "        trend_level_feature_transformer = ColumnTransformer([\n",
    "            ('transformer_std', StandardScaler(), FEATURES_TREND_CYCLE_NUMERIC_CONTINUOUS)\n",
    "            ],\n",
    "            verbose_feature_names_out=False,\n",
    "            # trend-cycle model has simpler (reduced) feature set\n",
    "            remainder='drop'\n",
    "            ).set_output(transform='pandas')\n",
    "\n",
    "        remainder_feature_transformer = ColumnTransformer([\n",
    "            ('transformer_onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), FEATURES_SEASONALITY_TO_ONEHOT),\n",
    "            # no transforms required\n",
    "            (\n",
    "                'select_others', \n",
    "                'passthrough', \n",
    "                FEATURES_SEASONALITY_NUMERIC_CONTINUOUS + FEATURES_UNIVERSE_ALREADY_ONEHOT\n",
    "            )\n",
    "            ],\n",
    "            verbose_feature_names_out=False,\n",
    "            remainder='drop'\n",
    "            ).set_output(transform='pandas')\n",
    "\n",
    "        pipeline_e2e = TrendRemainderModelPipeline(\n",
    "            trend_level_feature_transformer,\n",
    "            RIDGE_ALPHA_TREND_CYCLE_LOCAL_MODELS['alpha'],\n",
    "            remainder_feature_transformer, \n",
    "            'xgboost',\n",
    "            REMAINDER_MODEL_XGBOOST_ROUNDS_COUNT, \n",
    "            REMAINDER_XGBOOST_MODEL_BEST_PARAMS\n",
    "            )\n",
    "\n",
    "        pipeline_e2e.fit(\n",
    "            XY_grp.loc[is_training, FEATURES_SELECT],\n",
    "            XY_grp.loc[is_training, 'num_sold_log']\n",
    "            )\n",
    "        \n",
    "        segments_models[grp] = pipeline_e2e\n",
    "\n",
    "        predictions = (\n",
    "            XY_grp\n",
    "            .copy()\n",
    "            .assign(yhat = lambda df_: np.exp(pipeline_e2e.predict(df_)))\n",
    "            )\n",
    "        segments_predictions.append(predictions)\n",
    "\n",
    "    predictions = pd.concat(segments_predictions, axis=0)\n",
    "\n",
    "    scores = {\n",
    "        'validation': {\n",
    "            'nobs': predictions.loc[is_validation].shape[0],\n",
    "            'score': mean_absolute_percentage_error( \n",
    "                predictions.loc[is_validation, 'num_sold'],\n",
    "                predictions.loc[is_validation, 'yhat']\n",
    "                )\n",
    "            },\n",
    "        'train': {\n",
    "            'nobs': predictions.loc[is_training].shape[0],\n",
    "            'score': mean_absolute_percentage_error( \n",
    "                predictions.loc[is_training, 'num_sold'],\n",
    "                predictions.loc[is_training, 'yhat']\n",
    "                )\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    kfolds_evaluation.append(scores)\n",
    "\n",
    "kfolds_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = (\n",
    "    predictions\n",
    "    .assign(ape = lambda df_: abs(100 * (df_['num_sold'] / df_['yhat'] - 1)))\n",
    "    .assign(pe = lambda df_: (100 * (df_['num_sold'] / df_['yhat'] - 1)))\n",
    "    .assign(residual = lambda df_: df_['num_sold'] - df_['yhat'])\n",
    "    .assign(residual_log = lambda df_: df_['num_sold_log'] - np.log(df_['yhat']))\n",
    "    .assign(residual_abs = lambda df_: np.abs(df_['residual']))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    predictions\n",
    "    .groupby('country')\n",
    "    ['ape']\n",
    "    .agg(['mean', 'size'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    predictions\n",
    "    .groupby('year')\n",
    "    ['ape']\n",
    "    .agg(['mean', 'size'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    predictions\n",
    "    .groupby(['country', 'year'])\n",
    "    ['ape']\n",
    "    .agg(['mean', 'size'])\n",
    "    .unstack(-1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    predictions\n",
    "    .groupby(['country', 'year'])\n",
    "    ['pe']\n",
    "    .agg(['mean', 'size'])\n",
    "    .unstack(-1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    predictions\n",
    "    .groupby(['country', 'year'])\n",
    "    ['residual_log']\n",
    "    .agg(['mean', 'size'])\n",
    "    .unstack(-1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_sample = predictions.query(\"series_id == 'Norway|Premium Sticker Mart|Kaggle'\")\n",
    "\n",
    "predictions_sample.set_index('date')[['num_sold', 'yhat']].plot.line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_sample = (\n",
    "    predictions_sample\n",
    "    .assign(residual = lambda df_: df_['num_sold'] - df_['yhat'])\n",
    "    .sort_values('date')\n",
    "    .assign(\n",
    "        residual_lag1 = lambda df_: df_['residual'].shift(1),\n",
    "        residual_lag2 = lambda df_: df_['residual'].shift(2),\n",
    "        residual_lag3 = lambda df_: df_['residual'].shift(3),\n",
    "        residual_lag5 = lambda df_: df_['residual'].shift(5),\n",
    "        residual_lag7 = lambda df_: df_['residual'].shift(7),\n",
    "        residual_lag14 = lambda df_: df_['residual'].shift(14),\n",
    "        )\n",
    "    )\n",
    "\n",
    "(\n",
    "    p9.ggplot((\n",
    "        predictions_sample\n",
    "        .loc[predictions_sample['date'] <= pd.to_datetime(\"2014-01-01\")]\n",
    "        )) + \n",
    "    p9.theme_bw() + \n",
    "    p9.geom_point(p9.aes('residual_lag1', 'residual')) + \n",
    "    p9.geom_smooth(p9.aes('residual_lag1', 'residual'), method='lm', se=False, color='lightblue')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (\n",
    "#     predictions\n",
    "#     .assign(mape = lambda df_: np.abs(100 * (df_['num_sold']/df_['yhat'] - 1)) )\n",
    "#     .sort_values('mape', ascending=False)\n",
    "#     .head(100)\n",
    "#     .to_csv(\"./data/processed/predictions_errors_local_model.csv\", index=False)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    predictions\n",
    "    .assign(mape = lambda df_: np.abs(100 * (df_['num_sold']/df_['yhat'] - 1)) )\n",
    "    .sort_values('mape', ascending=False)\n",
    "    .head(50)\n",
    "    .to_csv(\"./data/processed/predictions_local_model.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_sample = predictions.query(\"series_id == 'Kenya|Stickers for Less|Holographic Goose'\")\n",
    "\n",
    "predictions_sample.set_index('date')[['num_sold', 'yhat']].plot.line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURES_SEASONALITY_TO_ONEHOT = [\n",
    "#     'month', \n",
    "#     'week_of_year',\n",
    "#     'day_of_week',\n",
    "#     ]\n",
    "\n",
    "# FEATURES_SEASONALITY_NUMERIC_CONTINUOUS = [\n",
    "#     'year', # allow seasonality to evolve over time\n",
    "#     'day_of_month', \n",
    "#     'day_of_month_sin',\n",
    "#     'day_of_month_cos',\n",
    "#     'day_of_year_sin',\n",
    "#     'day_of_year_cos',\n",
    "#     'day_of_year'\n",
    "#     ]\n",
    "\n",
    "# def objective(trial):\n",
    "\n",
    "#     ROUNDS_COUNT = 1_000\n",
    "\n",
    "#     param = {\n",
    "#         \"objective\": \"reg:squarederror\",\n",
    "#         \"booster\": \"gbtree\",\n",
    "#         \"max_depth\": trial.suggest_int(\"max_depth\", 1, 9, step=2),\n",
    "#         \"subsample\": trial.suggest_float(\"subsample\", 0.4, 1.0),\n",
    "#         \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n",
    "#         }\n",
    "\n",
    "#     kfolds_evaluation = []\n",
    "#     for is_training, is_validation in [KFOLDS_ALTERNATIVES['validate_2016']]:\n",
    "\n",
    "#         segments_models = {}\n",
    "#         segments_predictions = []\n",
    "\n",
    "#         for grp, XY_grp in segments_XY.items():\n",
    "\n",
    "#             trend_level_feature_transformer = ColumnTransformer([\n",
    "#                 ('transformer_std', StandardScaler(), FEATURES_TREND_CYCLE_NUMERIC_CONTINUOUS)\n",
    "#                 ],\n",
    "#                 verbose_feature_names_out=False,\n",
    "#                 # trend-cycle model has simpler (reduced) feature set\n",
    "#                 remainder='drop'\n",
    "#                 ).set_output(transform='pandas')\n",
    "            \n",
    "#             pipeline_trend_e2e = Pipeline([\n",
    "#                 ('transform_features', trend_level_feature_transformer), \n",
    "#                 ('model', Ridge(RIDGE_ALPHA_TREND_CYCLE_LOCAL_MODELS['alpha']))\n",
    "#                 ])\n",
    "#             pipeline_trend_e2e.fit(\n",
    "#                 XY_grp.loc[is_training], \n",
    "#                 XY_grp.loc[is_training, 'num_sold_log']\n",
    "#                 )\n",
    "\n",
    "#             predictions = (\n",
    "#                 XY_grp\n",
    "#                 .copy()\n",
    "#                 .assign(yhat_trend_log = lambda df_: pipeline_trend_e2e.predict(df_))\n",
    "#                 .assign(num_sold_log_detrend = lambda df_: df_['num_sold_log'] - df_['yhat_trend_log'])\n",
    "#                 )\n",
    "\n",
    "\n",
    "#             remainder_feature_transformer = ColumnTransformer([\n",
    "#                 ('transformer_onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), FEATURES_SEASONALITY_TO_ONEHOT),\n",
    "#                 # no transforms required\n",
    "#                 (\n",
    "#                     'select_others', \n",
    "#                     'passthrough', \n",
    "#                     FEATURES_SEASONALITY_NUMERIC_CONTINUOUS + FEATURES_UNIVERSE_ALREADY_ONEHOT\n",
    "#                 )\n",
    "#                 ],\n",
    "#                 verbose_feature_names_out=False,\n",
    "#                 remainder='drop'\n",
    "#                 ).set_output(transform='pandas')\n",
    "            \n",
    "#             X_remainder_train = remainder_feature_transformer.fit_transform(predictions.loc[is_training])        \n",
    "#             dtrain = xgb.DMatrix(X_remainder_train, label=predictions.loc[is_training, 'num_sold_log_detrend'])\n",
    "#             model_remainder = xgb.train(param, dtrain, ROUNDS_COUNT)\n",
    "\n",
    "#             dtest = xgb.DMatrix(remainder_feature_transformer.transform(predictions))\n",
    "#             yhat_remainder_log = model_remainder.predict(dtest)\n",
    "#             predictions = (\n",
    "#                 predictions\n",
    "#                 .assign(yhat_remainder_log = yhat_remainder_log)\n",
    "#                 .assign(yhat = lambda df_: np.exp(df_['yhat_trend_log'] + df_['yhat_remainder_log']))\n",
    "#                 )\n",
    "\n",
    "#             segments_predictions.append(predictions)\n",
    "\n",
    "#         predictions = pd.concat(segments_predictions, axis=0)\n",
    "\n",
    "#         scores = {\n",
    "#             'validation': {\n",
    "#                 'nobs': predictions.loc[is_validation].shape[0],\n",
    "#                 'score': mean_absolute_percentage_error( \n",
    "#                     predictions.loc[is_validation, 'num_sold'],\n",
    "#                     predictions.loc[is_validation, 'yhat']\n",
    "#                     )\n",
    "#                 },\n",
    "#             'train': {\n",
    "#                 'nobs': predictions.loc[is_training].shape[0],\n",
    "#                 'score': mean_absolute_percentage_error( \n",
    "#                     predictions.loc[is_training, 'num_sold'],\n",
    "#                     predictions.loc[is_training, 'yhat']\n",
    "#                     )\n",
    "#                 }\n",
    "#             }\n",
    "        \n",
    "#         kfolds_evaluation.append(scores)\n",
    "\n",
    "#     score_overall = kfolds_evaluation[0]['validation']['score']\n",
    "\n",
    "#     return score_overall\n",
    "\n",
    "\n",
    "# study = optuna.create_study()\n",
    "# # if optuna returns nulls in y_pred, don't fail the entire study\n",
    "# study.optimize(\n",
    "#     objective,\n",
    "#     n_trials=10,\n",
    "#     catch=(ValueError,),\n",
    "#     n_jobs=-1,\n",
    "#     timeout=12 * 60 * 60,\n",
    "# )\n",
    "\n",
    "# study.best_params\n",
    "# study.best_trial\n",
    "# study.best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEGMENTS_MODELS = {}\n",
    "\n",
    "for grp, XY_grp in segments_XY.items():\n",
    "\n",
    "    trend_level_feature_transformer = ColumnTransformer([\n",
    "        ('transformer_std', StandardScaler(), FEATURES_TREND_CYCLE_NUMERIC_CONTINUOUS)\n",
    "        ],\n",
    "        verbose_feature_names_out=False,\n",
    "        # trend-cycle model has simpler (reduced) feature set\n",
    "        remainder='drop'\n",
    "        ).set_output(transform='pandas')\n",
    "\n",
    "    remainder_feature_transformer = ColumnTransformer([\n",
    "        ('transformer_onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), FEATURES_SEASONALITY_TO_ONEHOT),\n",
    "        # no transforms required\n",
    "        (\n",
    "            'select_others', \n",
    "            'passthrough', \n",
    "            FEATURES_SEASONALITY_NUMERIC_CONTINUOUS + FEATURES_UNIVERSE_ALREADY_ONEHOT\n",
    "        )\n",
    "        ],\n",
    "        verbose_feature_names_out=False,\n",
    "        remainder='drop'\n",
    "        ).set_output(transform='pandas')\n",
    "\n",
    "    pipeline_e2e = TrendRemainderModelPipeline(\n",
    "        trend_level_feature_transformer,\n",
    "        RIDGE_ALPHA_TREND_CYCLE_LOCAL_MODELS['alpha'],\n",
    "        remainder_feature_transformer, \n",
    "        'xgboost',\n",
    "        REMAINDER_MODEL_XGBOOST_ROUNDS_COUNT, \n",
    "        REMAINDER_XGBOOST_MODEL_BEST_PARAMS\n",
    "        )\n",
    "    pipeline_e2e.fit(XY_grp, XY_grp['num_sold_log'])\n",
    "    \n",
    "    SEGMENTS_MODELS[grp] = pipeline_e2e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonparametric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfolds_evaluation = []\n",
    "for is_training, is_validation in [KFOLDS_ALTERNATIVES['validate_2016']]:\n",
    "\n",
    "    feature_transform_pipeline_global_model = ColumnTransformer([\n",
    "        ('transformer_onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), FEATURES_UNIVERSE_TO_ONEHOT),\n",
    "        ('transformer_std', StandardScaler(), FEATURES_UNIVERSE_NUMERIC_CONTINUOUS)\n",
    "        ],\n",
    "        verbose_feature_names_out=False,\n",
    "        # caller expected to provide some features that require no transforms.\n",
    "        # don't drop those just because they received no special operations.\n",
    "        # but do be sure to drop non-features in advance!\n",
    "        remainder='passthrough'\n",
    "        )\n",
    "    feature_transform_pipeline_global_model.set_output(transform='pandas')\n",
    "    \n",
    "    pipeline_e2e = Pipeline([\n",
    "        ('transform_features', feature_transform_pipeline_global_model), \n",
    "        ('model', RandomForestRegressor(n_estimators=100, n_jobs=-1))\n",
    "        ])\n",
    "\n",
    "    pipeline_e2e.fit(\n",
    "        XY.loc[is_training, FEATURES_GLOBAL_MODEL],\n",
    "        XY.loc[is_training, 'num_sold_log']\n",
    "        )\n",
    "    \n",
    "    predictions = (\n",
    "        XY\n",
    "        .copy()\n",
    "        .assign(yhat = lambda df_: np.exp(pipeline_e2e.predict(df_[FEATURES_GLOBAL_MODEL])))\n",
    "        )\n",
    "    \n",
    "    scores = {\n",
    "        'validation': {\n",
    "            'nobs': predictions.loc[is_validation].shape[0],\n",
    "            'score': mean_absolute_percentage_error( \n",
    "                predictions.loc[is_validation, 'num_sold'],\n",
    "                predictions.loc[is_validation, 'yhat']\n",
    "                )\n",
    "            },\n",
    "        'train': {\n",
    "            'nobs': predictions.loc[is_training].shape[0],\n",
    "            'score': mean_absolute_percentage_error( \n",
    "                predictions.loc[is_training, 'num_sold'],\n",
    "                predictions.loc[is_training, 'yhat']\n",
    "                )\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    kfolds_evaluation.append(scores)\n",
    "\n",
    "kfolds_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_sample = predictions.query(\"series_id == 'Norway|Stickers for Less|Kaggle'\")\n",
    "\n",
    "predictions_sample.set_index('date')[['num_sold', 'yhat']].plot.line(alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfolds_evaluation = []\n",
    "for is_training, is_validation in [KFOLDS_ALTERNATIVES['validate_2016']]:\n",
    "\n",
    "    feature_transform_pipeline_global_model = ColumnTransformer([\n",
    "        ('transformer_onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), FEATURES_UNIVERSE_TO_ONEHOT),\n",
    "        ('transformer_std', StandardScaler(), FEATURES_UNIVERSE_NUMERIC_CONTINUOUS)\n",
    "        ],\n",
    "        verbose_feature_names_out=False,\n",
    "        remainder='passthrough'\n",
    "        )\n",
    "    feature_transform_pipeline_global_model.set_output(transform='pandas')\n",
    "\n",
    "    pipeline_e2e = Pipeline([\n",
    "        ('transform_features', feature_transform_pipeline_global_model), \n",
    "        ('model', Ridge(1e-2))\n",
    "        ])\n",
    "\n",
    "    pipeline_e2e.fit(\n",
    "        XY.loc[is_training, FEATURES_GLOBAL_MODEL],\n",
    "        XY.loc[is_training, 'num_sold_log']\n",
    "        )\n",
    "    \n",
    "    predictions = (\n",
    "        XY\n",
    "        .copy()\n",
    "        .assign(yhat = lambda df_: np.exp(pipeline_e2e.predict(df_[FEATURES_GLOBAL_MODEL])))\n",
    "        )\n",
    "    \n",
    "    scores = {\n",
    "        'validation': {\n",
    "            'nobs': predictions.loc[is_validation].shape[0],\n",
    "            'score': mean_absolute_percentage_error( \n",
    "                predictions.loc[is_validation, 'num_sold'],\n",
    "                predictions.loc[is_validation, 'yhat']\n",
    "                )\n",
    "            },\n",
    "        'train': {\n",
    "            'nobs': predictions.loc[is_training].shape[0],\n",
    "            'score': mean_absolute_percentage_error( \n",
    "                predictions.loc[is_training, 'num_sold'],\n",
    "                predictions.loc[is_training, 'yhat']\n",
    "                )\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    kfolds_evaluation.append(scores)\n",
    "\n",
    "kfolds_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_sample = predictions.query(\"series_id == 'Norway|Stickers for Less|Kaggle'\")\n",
    "# predictions_sample = predictions.query(\"series_id == 'Norway|Premium Sticker Mart|Kaggle'\")\n",
    "\n",
    "predictions_sample.set_index('date')[['num_sold', 'yhat']].plot.line(alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble (Trend, Seasonal & Remainder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_TREND_CYCLE_TO_ONEHOT = [\n",
    "    'country', \n",
    "    'store',\n",
    "    'product',\n",
    "    'country_store',\n",
    "    'country_product',\n",
    "    'store_product',\n",
    "    ]\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    parameters = {'alpha': trial.suggest_float(\"alpha\", 1e-1, 1_000, log=True)}\n",
    "\n",
    "    kfolds_evaluation = []\n",
    "    for is_training, is_validation in [KFOLDS_ALTERNATIVES['validate_2016']]:\n",
    "\n",
    "        feature_transform_pipeline_global_model = ColumnTransformer([\n",
    "            ('transformer_onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), FEATURES_TREND_CYCLE_TO_ONEHOT),\n",
    "            ('transformer_std', StandardScaler(), FEATURES_TREND_CYCLE_NUMERIC_CONTINUOUS)\n",
    "            ],\n",
    "            verbose_feature_names_out=False,\n",
    "            remainder='drop'\n",
    "            )\n",
    "        feature_transform_pipeline_global_model.set_output(transform='pandas')\n",
    "\n",
    "        pipeline_e2e = Pipeline([\n",
    "            ('transform_features', feature_transform_pipeline_global_model), \n",
    "            ('model', Ridge(**parameters))\n",
    "            ])\n",
    "\n",
    "        pipeline_e2e.fit(XY.loc[is_training], XY.loc[is_training, 'num_sold_log'])\n",
    "        \n",
    "        predictions = (\n",
    "            XY\n",
    "            .copy()\n",
    "            .assign(yhat = lambda df_: np.exp(pipeline_e2e.predict(df_)))\n",
    "            )\n",
    "        \n",
    "        scores = {\n",
    "            'validation': {\n",
    "                'nobs': predictions.loc[is_validation].shape[0],\n",
    "                'score': mean_absolute_percentage_error( \n",
    "                    predictions.loc[is_validation, 'num_sold'],\n",
    "                    predictions.loc[is_validation, 'yhat']\n",
    "                    )\n",
    "                },\n",
    "            'train': {\n",
    "                'nobs': predictions.loc[is_training].shape[0],\n",
    "                'score': mean_absolute_percentage_error( \n",
    "                    predictions.loc[is_training, 'num_sold'],\n",
    "                    predictions.loc[is_training, 'yhat']\n",
    "                    )\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        kfolds_evaluation.append(scores)\n",
    "\n",
    "    score_overall = np.mean([ \n",
    "        kfolds_evaluation[0]['validation']['score']\n",
    "        ])\n",
    "    \n",
    "    return score_overall\n",
    "\n",
    "study = optuna.create_study()\n",
    "\n",
    "study.optimize(\n",
    "    objective,\n",
    "    n_trials=50,\n",
    "    catch=(ValueError,),\n",
    "    n_jobs=-1,\n",
    "    timeout=12 * 60 * 60,\n",
    ")\n",
    "\n",
    "RIDGE_ALPHA_TREND_CYCLE_GLOBAL_MODEL = study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_trial, study.best_params, study.best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfolds_evaluation = []\n",
    "for is_training, is_validation in [KFOLDS_ALTERNATIVES['validate_2016']]:\n",
    "\n",
    "    trend_level_feature_transformer = ColumnTransformer([\n",
    "        ('transformer_onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), FEATURES_TREND_CYCLE_TO_ONEHOT),\n",
    "        ('transformer_std', StandardScaler(), FEATURES_TREND_CYCLE_NUMERIC_CONTINUOUS)\n",
    "        ],\n",
    "        verbose_feature_names_out=False,\n",
    "        remainder='drop'\n",
    "        ).set_output(transform='pandas')\n",
    "\n",
    "    remainder_feature_transformer = ColumnTransformer([\n",
    "        ('transformer_onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), FEATURES_UNIVERSE_TO_ONEHOT),\n",
    "        # no transforms required\n",
    "        (\n",
    "            'select_others', \n",
    "            'passthrough', \n",
    "            FEATURES_SEASONALITY_NUMERIC_CONTINUOUS + FEATURES_UNIVERSE_ALREADY_ONEHOT\n",
    "        )\n",
    "        ],\n",
    "        verbose_feature_names_out=False,\n",
    "        remainder='drop'\n",
    "        ).set_output(transform='pandas')\n",
    "\n",
    "    model_global_pipeline = TrendRemainderModelPipeline(\n",
    "        trend_level_feature_transformer,\n",
    "        RIDGE_ALPHA_TREND_CYCLE_GLOBAL_MODEL['alpha'],\n",
    "        remainder_feature_transformer,\n",
    "        'random_forest'\n",
    "        )\n",
    "    model_global_pipeline.fit(XY.loc[is_training], XY.loc[is_training, 'num_sold_log'])\n",
    "    \n",
    "    predictions = XY.copy()\n",
    "    predictions = (\n",
    "        predictions\n",
    "        .assign(yhat_log = lambda df_: model_global_pipeline.predict(df_))\n",
    "        .assign(yhat = lambda df_: np.exp(df_['yhat_log']) )\n",
    "        )\n",
    "    \n",
    "    scores = {\n",
    "        'validation': {\n",
    "            'nobs': predictions.loc[is_validation].shape[0],\n",
    "            'score': mean_absolute_percentage_error( \n",
    "                predictions.loc[is_validation, 'num_sold'],\n",
    "                predictions.loc[is_validation, 'yhat']\n",
    "                )\n",
    "            },\n",
    "        'train': {\n",
    "            'nobs': predictions.loc[is_training].shape[0],\n",
    "            'score': mean_absolute_percentage_error( \n",
    "                predictions.loc[is_training, 'num_sold'],\n",
    "                predictions.loc[is_training, 'yhat']\n",
    "                )\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    kfolds_evaluation.append(scores)\n",
    "\n",
    "kfolds_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    predictions\n",
    "    .assign(ape = lambda df_: abs(100 * (df_['num_sold'] / df_['yhat'] - 1)))\n",
    "    .groupby('country')\n",
    "    ['ape']\n",
    "    .agg(['mean', 'size'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_sample = predictions.query(\"series_id == 'Norway|Stickers for Less|Kaggle'\")\n",
    "# predictions_sample = predictions.query(\"series_id == 'Norway|Premium Sticker Mart|Kaggle'\")\n",
    "\n",
    "predictions_sample.set_index('date')[['num_sold', 'yhat']].plot.line(alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WIP\n",
    "\n",
    "# kfolds = [\n",
    "\n",
    "#     # validation set 2014-16 matches ultimate test set's length, 2017-19\n",
    "#     ( \n",
    "#         ( (XY['date'] >= pd.to_datetime('2010-01-01')) & (XY['date'] < pd.to_datetime(\"2014-01-01\")) ),\n",
    "#         XY['date'] >= pd.to_datetime(\"2014-01-01\")\n",
    "#     ),\n",
    "\n",
    "#     # ( \n",
    "#     #     ( (XY['date'] >= pd.to_datetime('2010-01-01')) & (XY['date'] < pd.to_datetime(\"2016-01-01\")) ),\n",
    "#     #     XY['date'] >= pd.to_datetime(\"2016-01-01\")\n",
    "#     # )\n",
    "\n",
    "#     ]\n",
    "\n",
    "# def objective(trial):\n",
    "\n",
    "#     ROUNDS_COUNT = 1_000\n",
    "\n",
    "#     param = {\n",
    "#         \"objective\": \"reg:squarederror\",\n",
    "#         \"booster\": trial.suggest_categorical(\n",
    "#             \"booster\", [\"gbtree\", \"gblinear\", \"dart\"]\n",
    "#         ),\n",
    "#         \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "#         \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "#         \"subsample\": trial.suggest_float(\"subsample\", 0.4, 1.0),\n",
    "#         \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n",
    "#         'device': 'cuda',\n",
    "#         'tree_method': 'hist'\n",
    "#     }\n",
    "\n",
    "#     if param[\"booster\"] in [\"gbtree\", \"dart\"]:\n",
    "#         param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 1, 9, step=2)\n",
    "\n",
    "#         param[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 1, 10)\n",
    "#         param[\"eta\"] = trial.suggest_float(\"eta\", 1e-5, 0.01, log=True)\n",
    "\n",
    "#         param[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
    "#         param[\"grow_policy\"] = trial.suggest_categorical(\n",
    "#             \"grow_policy\", [\"depthwise\", \"lossguide\"]\n",
    "#         )\n",
    "\n",
    "#     if param[\"booster\"] == \"dart\":\n",
    "#         param[\"sample_type\"] = trial.suggest_categorical(\n",
    "#             \"sample_type\", [\"uniform\", \"weighted\"]\n",
    "#         )\n",
    "#         param[\"normalize_type\"] = trial.suggest_categorical(\n",
    "#             \"normalize_type\", [\"tree\", \"forest\"]\n",
    "#         )\n",
    "#         param[\"rate_drop\"] = trial.suggest_float(\n",
    "#             \"rate_drop\", 1e-8, 1.0, log=True\n",
    "#         )\n",
    "#         param[\"skip_drop\"] = trial.suggest_float(\n",
    "#             \"skip_drop\", 1e-8, 1.0, log=True\n",
    "#         )\n",
    "\n",
    "\n",
    "#     kfolds_evaluation = []\n",
    "#     for is_training, is_validation in kfolds:\n",
    "\n",
    "#         feature_transform_pipeline_global_model = ColumnTransformer([\n",
    "#             ('transformer_onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), FEATURES_UNIVERSE_TO_ONEHOT),\n",
    "#             ('transformer_std', StandardScaler(), FEATURES_UNIVERSE_NUMERIC_CONTINUOUS)\n",
    "#             ],\n",
    "#             verbose_feature_names_out=False,\n",
    "#             # caller expected to provide some features that require no transforms.\n",
    "#             # don't drop those just because they received no special operations.\n",
    "#             # but do be sure to drop non-features in advance!\n",
    "#             remainder='passthrough'\n",
    "#             )\n",
    "#         feature_transform_pipeline_global_model.set_output(transform='pandas')\n",
    "\n",
    "#         X_train = (\n",
    "#             feature_transform_pipeline_global_model\n",
    "#             .fit_transform(XY.loc[is_training, FEATURES_GLOBAL_MODEL])\n",
    "#             )\n",
    "#         dtrain = xgb.DMatrix(\n",
    "#             X_train, \n",
    "#             label=XY.loc[is_training, 'num_sold_log']\n",
    "#             )\n",
    "        \n",
    "#         X_test = feature_transform_pipeline_global_model.transform(XY)\n",
    "#         dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "#         model_global = xgb.train(param, dtrain, ROUNDS_COUNT)\n",
    "        \n",
    "#         yhat_log = model_global.predict(dtest)\n",
    "#         predictions = (\n",
    "#             XY\n",
    "#             .copy()\n",
    "#             .assign(yhat = np.exp(yhat_log))\n",
    "#             )\n",
    "\n",
    "#         scores = {\n",
    "#             'validation': mean_absolute_percentage_error( \n",
    "#                 predictions.loc[is_validation, 'num_sold'],\n",
    "#                 predictions.loc[is_validation, 'yhat']\n",
    "#                 ),\n",
    "#             'train': mean_absolute_percentage_error( \n",
    "#                 predictions.loc[is_training, 'num_sold'],\n",
    "#                 predictions.loc[is_training, 'yhat']\n",
    "#                 )\n",
    "#             }\n",
    "        \n",
    "#         kfolds_evaluation.append(scores)\n",
    "\n",
    "#     score_overall = np.mean([ \n",
    "#         kfolds_evaluation[0]['validation'], \n",
    "#         # kfolds_evaluation[1]['validation'] \n",
    "#         ])\n",
    "    \n",
    "#     return score_overall\n",
    "\n",
    "# study = optuna.create_study()\n",
    "\n",
    "# study.optimize(\n",
    "#     objective,\n",
    "#     n_trials=50,\n",
    "#     catch=(ValueError,),\n",
    "#     n_jobs=-1,\n",
    "#     timeout=12 * 60 * 60,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trend_level_feature_transformer = ColumnTransformer([\n",
    "    ('transformer_onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), FEATURES_TREND_CYCLE_TO_ONEHOT),\n",
    "    ('transformer_std', StandardScaler(), FEATURES_TREND_CYCLE_NUMERIC_CONTINUOUS)\n",
    "    ],\n",
    "    verbose_feature_names_out=False,\n",
    "    remainder='drop'\n",
    "    ).set_output(transform='pandas')\n",
    "\n",
    "remainder_feature_transformer = ColumnTransformer([\n",
    "    ('transformer_onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), FEATURES_UNIVERSE_TO_ONEHOT),\n",
    "    # no transforms required\n",
    "    (\n",
    "        'select_others', \n",
    "        'passthrough', \n",
    "        FEATURES_SEASONALITY_NUMERIC_CONTINUOUS + FEATURES_UNIVERSE_ALREADY_ONEHOT\n",
    "    )\n",
    "    ],\n",
    "    verbose_feature_names_out=False,\n",
    "    remainder='drop'\n",
    "    ).set_output(transform='pandas')\n",
    "\n",
    "MODEL_GLOBAL = TrendRemainderModelPipeline(\n",
    "    trend_level_feature_transformer,\n",
    "    RIDGE_ALPHA_TREND_CYCLE_GLOBAL_MODEL['alpha'],\n",
    "    remainder_feature_transformer,\n",
    "    'random_forest'\n",
    "    )\n",
    "\n",
    "MODEL_GLOBAL.fit(XY, XY['num_sold_log'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_test_daily = pd.read_csv(\"./data/external/test.csv\").assign(\n",
    "\n",
    "    date = lambda df_: pd.to_datetime(df_['date']),\n",
    "\n",
    "    country_store = lambda df_: df_['country'].str.cat(df_['store'], sep='|'),\n",
    "    country_product = lambda df_: df_['country'].str.cat(df_['product'], sep='|'),\n",
    "    store_product = lambda df_: df_['store'].str.cat(df_['product'], sep='|'),\n",
    "    country_store_product = lambda df_: df_['country'].str.cat([df_['store'], df_['product']], sep='|')\n",
    "\n",
    "    ).assign(series_id = lambda df_: df_['country_store_product'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_test_daily.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_test_daily['date'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_test_daily = transform_calendar_features(sales_test_daily)\n",
    "sales_test_daily = integrate_external_features(sales_test_daily)\n",
    "sales_test_daily = sales_test_daily.assign(\n",
    "    num_sold = None,\n",
    "    num_sold_log = None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_X_test = {grp: df for grp, df in sales_test_daily.groupby('series_id')}\n",
    "\n",
    "segments_predictions_test = []\n",
    "for grp, df in segments_X_test.items():\n",
    "\n",
    "    if grp in SEGMENTS_MODELS:\n",
    "        df = df.assign(yhat = lambda df_: np.exp(SEGMENTS_MODELS[grp].predict(df_)))  \n",
    "        \n",
    "    else:\n",
    "        df = df.assign(yhat = lambda df_: np.exp(MODEL_GLOBAL.predict(df_)))\n",
    "\n",
    "    segments_predictions_test.append(df)\n",
    "\n",
    "predictions_test = pd.concat(segments_predictions_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test_submit = (\n",
    "    predictions_test\n",
    "    [['id', 'yhat']]\n",
    "    .rename(columns={'yhat': 'num_sold'})\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert predictions_test_submit.shape[0] == 98_550\n",
    "assert predictions_test_submit.notnull().all().all()\n",
    "predictions_test_submit.to_csv(\"./data/processed/submission_pare_trend_model.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differential Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_compare = pd.read_csv(\"./data/processed/submission_xgb_remainder_local_models.csv\").rename(columns={'num_sold': 'num_sold_prev'})\n",
    "predictions_compare = (\n",
    "    pd.merge(predictions_compare, predictions_test_submit, how='left')\n",
    "    .assign(diff = lambda df_: 100 * (df_['num_sold'] / df_['num_sold_prev'] - 1))\n",
    "    )\n",
    "assert predictions_compare.shape[0] == 98_550\n",
    "\n",
    "predictions_compare = pd.merge(\n",
    "    predictions_compare, \n",
    "    sales_test_daily[['id', 'country', 'product', 'store', 'year', 'series_id']]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_compare['diff'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_compare.groupby('country')['diff'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_compare.query(\"year == 2017\").groupby('country')['diff'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_compare.groupby('year')['diff'].describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forecast_stickers_kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
